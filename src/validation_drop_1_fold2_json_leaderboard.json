[
    {
        "Context": "Geometric and Combinatorial Properties of Well-Centered Triangulations in Three and Higher Dimensions An n-simplex is said to be n-well-centered if its circumcenter lies in its interior. We introduce several other geometric conditions and an algebraic condition that can be used to determine whether a simplex is n-well-centered. These conditions, together with some other observations, are used to describe restrictions on the local combinatorial structure of simplicial meshes in which every simplex is well-centered. In particular, it is shown that in a 3-well-centered (2well-centered) tetrahedral mesh there are at least 7 (9) edges incident to each interior vertex, and these bounds are sharp. Moreover, it is shown that, in stark contrast to the 2-dimensional analog, where there are exactly two vertex links that prevent a well-centered triangle mesh in R 2 , there are infinitely many vertex links that prohibit a well-centered tetrahedral mesh in R 3 .",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "The Smallville Effect: Social Ties Make Mobile Networks More Secure Against the Node Capture Attack Mobile Ad Hoc networks, due to the unattended nature of the network itself and the dispersed location of nodes, are subject to several unique security issues. One of the most vexed security threat is node capture. A few solutions have already been proposed to address this problem; however, those solutions are either centralized or focused on theoretical mobility models alone. In the former case the solution does not fit well the distributed nature of the network while, in the latter case, the quality of the solutions obtained for realistic mobility models severely differs from the results obtained for theoretical models. The rationale of this paper is inspired by the observation that re-encounters of mobile nodes do elicit a form of social ties. Leveraging these ties, it is possible to design efficient and distributed algorithms that, with a moderated degree of node cooperation, enforce the emergent property of node capture detection. In particular, in this paper we provide a proof of concept proposing a set of algorithms that leverage, to different extent, node mobility and node cooperation-that is, identifying social ties-to thwart node capture attack. In particular, we test these algorithms on a realistic mobility scenario. Extensive simulations show the quality of the proposed solutions and, more important, the viability of the proposed approach.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "GeneSyst: a Tool to Reason about Behavioral Aspects of B Event Specifications. Application to Security Properties \u22c6 In this paper, we present a method and a tool to build symbolic labelled transition systems from B specifications. The tool, called GeneSyst, can take into account refinement levels and can visualize the decomposition of abstract states in concrete hierarchical states. The resulting symbolic transition system represents all the behaviors of the initial B event system. So, it can be used to reason about them. We illustrate the use of GeneSyst to check security properties on a model of electronic purse.The events have the form \"e = G =\u21d2 T \" where G is a predicate, T is a generalized substitution such that I \u2227 G \u21d2 fis(T ). Predicate G is called the guard of e and T is its action. They are respectively denoted by Guard (e) and Action(e). If the syntactic definition of an event e = S does not fulfill this form, it can be built by computing e = fis(S) =\u21d2 S. Following the so-called event-based approach [10], the semantics of event-B systems can be chosen to be the set of all the valid sequences of event executions.Definition 1 (Traces of Event-B systems) A finite sequence of event occurrences e 0 .e 1 .e 2 . . . en is a trace of system S if and only if e 0 is the initialisation of S, {e 1 , e 2 , . . . , en } \u2286 Interface(S) and fis(e 0 ; e 1 ; e 2 ; . . . ; en ) \u21d4 true.The set of all the finite traces of a system S is called Traces(S). For the initialisation, one can notice that prd x (Init ) does not depend on the initial values of the variables and that Guard (Init ) \u21d4 true. The following property characterizes traces by the existence of intermediary states xi in which the guard of e i holds and where the pair (x i , x i+1 ) is in the before-after predicate of event e i : Abstract. In this paper, we present a method and a tool to build symbolic labelled transition systems from B specifications. The tool, called GeneSyst, can take into account refinement levels and can visualize the decomposition of abstract states in concrete hierarchical states. The resulting symbolic transition system represents all the behaviors of the initial B event system. So, it can be used to reason about them. We illustrate the use of GeneSyst to check security properties on a model of electronic purse.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "MeTRAbs: Metric-Scale Truncation-Robust Heatmaps for Absolute 3D Human Pose Estimation Heatmap representations have formed the basis of human pose estimation systems for many years, and their extension to 3D has been a fruitful line of recent research. This includes 2.5D volumetric heatmaps, whose X and Y axes correspond to image space and Z to metric depth around the subject. To obtain metric-scale predictions, 2.5D methods need a separate post-processing step to resolve scale ambiguity. Further, they cannot localize body joints outside the image boundaries, leading to incomplete estimates for truncated images. To address these limitations, we propose metric-scale truncation-robust (MeTRo) volumetric heatmaps, whose dimensions are all defined in metric 3D space, instead of being aligned with image space. This reinterpretation of heatmap dimensions allows us to directly estimate complete, metric-scale poses without test-time knowledge of distance or relying on anthropometric heuristics, such as bone lengths. To further demonstrate the utility our representation, we present a differentiable combination of our 3D metric-scale heatmaps with 2D image-space ones to estimate absolute 3D pose (our MeTRAbs architecture). We find that supervision via absolute pose loss is crucial for accurate non-root-relative localization. Using a ResNet-50 backbone without further learned layers, we obtain state-of-the-art results on Human3.6M, MPI-INF-3DHP and MuPoTS-3D. Our code is publicly available. 1 Index Terms-3D human pose estimation, absolute human pose, scale estimation, truncation ! \u2022 I. S\u00e1r\u00e1ndi and B. Leibe are with RWTH 4 is evaluated in a multi-person context by training on MuCo-3DHP (MuCo) and testing on MuPoTS-3D (MuPoTS) Two evaluation protocols are in wide use In Protocol 2, subjects 1, 5, 6, 7, 8, 9 are used in training and 11 in evaluation, with Procrustes alignment between prediction and ground truth Every 64 th frame is evaluated MPII is a 2D-labeled dataset with 25k training images We use this dataset for weak supervision, following the idea of Zhou et al. Only arm and leg joints are used from MPII, as we found these to be the most consistently labeled across datasets In this dataset, the hips are labeled closer to the legs than in MPII We evaluate on both MuCo is a synthetically composited multi-person dataset, derived from 3DHP by pasting persons over each other based on their root joint depth order For single-person datasets, synthetic occlusion is added with 70% probability, Methods using no ground truth scale or depth information at test time 50 . 2\u00b10 . 3 Methods using ground - truth scale or depth information at test time Walk SitD Pur . WD Avg \u2193 Sm . Pht . Dis . Dir . Gre . Pose Eat Phn . Wait WT Sit Comparison of MPJPE with prior work on H3 . 6M under Protocol 2 ( test subject 11 with Procrustes alignment to the ground truth ) . Universal , height - normalized skeletons ( simplified scale recovery task ) Metric - scale skeletons ( full scale recovery task ) - door Exer - reach screen 89 . 9\u00b10 . 2 Sport cise floor Sit on 88 . 7\u00b10 . 6 90 . 6\u00b10 . 4 No Stand / chair Cro . / gr . sc . Out - Total Misc . MPJPE\u2193 PCK\u2191 AUC\u2191 walk Green On PCK\u2191 AUC\u2191",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"3D Poses in the Wild Challenge\", \"Metric\": \"MPJPE\", \"Score\": \"68.83\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"Human3.6M\", \"Metric\": \"Average MPJPE (mm)\", \"Score\": \"38.9\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"Human3.6M\", \"Metric\": \"Multi-View or Monocular\", \"Score\": \"Multi-View\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"Human3.6M\", \"Metric\": \"Using 2D ground-truth joints\", \"Score\": \"No\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"Human3.6M\", \"Metric\": \"Average MPJPE (mm)\", \"Score\": \"48.0\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"Human3.6M\", \"Metric\": \"Frames Needed\", \"Score\": \"1\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"Human3.6M\", \"Metric\": \"Need Ground Truth 2D Pose\", \"Score\": \"No\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Human Pose Estimation\","
    },
    {
        "Context": "Grafit: Learning fine-grained image representations with coarse labels  We consider evaluation scenarios where it is beneficial to learn at a finer granularity than that provided by the training labels The third protocol is vanilla transfer learning, where we transfer from Imagenet to a fine-grained dataset We carryout our evaluations on public benchmarks, which statistics are detailed in shows that Grafit not only provides a better on-the-fly classification (as evaluated by the kNN metric), but that the ranked list is more relevant to the query (results for mAP) In SSL, the standard way to evaluate the quality of a feature extractor f is to measure the accuracy of the network after learning a linear classifier l for the target classes on top off The Rademacher complexity measures how a class of: Separability experiment on CIFAR-100 Ina second stage, we train a linear classifer l on the Resnet-18 trunk with fine class supervision, and evaluate its accuracy on the test set Table 1: Datasets used for our different tasks. The four top datasets offer two or more levels of granularity, we use them for all coarse-to-fine tasks. The bottom three are fine-grained datasets employed to evaluate transfer learning. 20 / 100 127 / 1000 6 / . . . / 1 , 010 #classes 6 / . . . / 8 , 142 Table 2: Coarse-to-fine: comparison with the state of the art for category-level retrieval (mAP, %) and kNN classification (top-1, %), with the ResNet50 architec- ture. We compare Grafit with the state of the art Method ImageNet - 1k kNN CIFAR - 100 mAP for details . Table 3: kNN evaluation on iNaturalist-2018 with dif- ferent semantic levels. The symbol \u2205 refers to the un- supervised case (a unique class). We compare with the best competing method according to Table 2. low Goyal et al . ' s [ 24",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"79.6%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"Flowers-102\", \"Metric\": \"Accuracy\", \"Score\": \"99.1%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"iNaturalist 2018\", \"Metric\": \"Top-1 Accuracy\", \"Score\": \"81.2%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"iNaturalist 2018\", \"Metric\": \"Top-1 Accuracy\", \"Score\": \"69.8%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-100\", \"Metric\": \"Percentage correct\", \"Score\": \"83.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"iNaturalist 2019\", \"Metric\": \"Top-1 Accuracy\", \"Score\": \"84.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"Oxford 102 Flowers\", \"Metric\": \"Accuracy\", \"Score\": \"99.1%\"}}, {\"LEADERBOARD\": {\"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"Food-101\", \"Metric\": \"Accuracy\", \"Score\": \"93.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"Stanford Cars\", \"Metric\": \"Accuracy\", \"Score\": \"94.7%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"iNaturalist 2018\", \"Metric\": \"Top-1 Accuracy\", \"Score\": \"69.05%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"iNaturalist 2018\", \"Metric\": \"Top-1 Accuracy\", \"Score\": \"67.98%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"iNaturalist 2018\", \"Metric\": \"Top-1 Accuracy\", \"Score\": \"64.16%\"}} ]"
    },
    {
        "Context": "Sliced Recurrent Neural Networks Recurrent neural networks have achieved great success in many NLP tasks. However, they have difficulty in parallelization because of the recurrent structure, so it takes much time to train RNNs. In this paper, we introduce sliced recurrent neural networks (SRNNs), which could be parallelized by slicing the sequences into many subsequences. SRNNs have the ability to obtain high-level information through multiple layers with few extra parameters. We prove that the standard RNN is a special case of the SRNN when we use linear activation functions. Without changing the recurrent units, SRNNs are 136 times as fast as standard RNNs and could be even faster when we train longer sequences. Experiments on six largescale sentiment analysis datasets show that SRNNs achieve better performance than standard RNNs. We evaluate SRNNs on six large-scale sentiment analysis datasets shows the information of the datasets Yelp reviews: The Yelp reviews datasets are obtained from the Yelp Dataset Challenge, which has 5 sentiment labels (the higher, the better) This dataset consists of 4,736,892 documents, and we extract three subsets Yelp 2013, 2014, 2015 containing 468,608, 670,440 and 897,835 documents separately created the polarity dataset including 598,000 documents with two sentiment labels, and we obtain the polarity dataset from them Amazon reviews: The Amazon reviews dataset is a commentary dataset containing 34,686,770 reviews on 2,441,053 products from 6,643,669 users The dataset is also constructed into a full dataset with 3,650,000 documents and a polarity dataset with 4,000,000 documents, which is also obtained from When we focus on the results of SRNN (2,8) on Yelp datasets and SRNN (2,7) on Amazon datasets, we could find that even if they did not achieve the best Table 1: Dataset information. Max words denotes the max sequence length, and Average words denotes the average length of the sentences in each dataset. 308 , 028 228 , 715 210 , 353 202 , 058 1 , 274 , 916 Max words Dataset Average words Table 2. We choose different n and k values and get different SRNNs. For example, SRNN (16,1) means n=16 and k=1, which could get a 32-length minimum subsequence when T is 512 or a 16-length minimum subsequence when T is 256. We compare four SRNNs with the standard RNN. For each dataset, we use bold words to label the highest-performing model and the fastest model. 67s 284s 164s 145s 201s 388s 204s 4142s 238s 3172s 270s Validation Parameters Test Table 2: The accuracy and training time on validation and test sets of the models on each dataset. Four different structures of SRNNs are constructed. DCCNN",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Sentiment Analysis\", \"Dataset\": \"Yelp Binary classification\", \"Metric\": \"Error\", \"Score\": \"3.96\"}}, {\"LEADERBOARD\": {\"Task\": \"Sentiment Analysis\", \"Dataset\": \"Amazon Review Full\", \"Metric\": \"Accuracy\", \"Score\": \"61.65\"}}, {\"LEADERBOARD\": {\"Task\": \"Sentiment Analysis\", \"Dataset\": \"Amazon Review Polarity\", \"Metric\": \"Accuracy\", \"Score\": \"95.26\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"User and product information\", \"Metric\": \"IMDB (Acc)\", \"Score\": \"56.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"User and product information\", \"Metric\": \"Yelp 2013 (Acc)\", \"Score\": \"67.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"User and product information\", \"Metric\": \"Yelp 2014 (Acc)\", \"Score\": \"69.2\"}} ]"
    },
    {
        "Context": "Distilling Visual Priors from Self-Supervised Learning Convolutional Neural Networks (CNNs) are prone to overfit small training datasets. We present a novel two-phase pipeline that leverages self-supervised learning and knowledge distillation to improve the generalization ability of CNN models for image classification under the data-deficient setting. The first phase is to learn a teacher model which possesses rich and generalizable visual representations via self-supervised learning, and the second phase is to distill the representations into a student model in a self-distillation manner, and meanwhile fine-tune the student model for the image classification task. We also propose a novel margin loss for the self-supervised contrastive learning proxy task to better learn the representation under the data-deficient scenario. Together with other tricks, we achieve competitive performance in the VIPriors image classification challenge. The self-supervised trained checkpoint from phase-1 is then used to initialize the teacher and student for fine-tuning on the whole dataset with labels The distillation process can be seen as a regulation to prevent the student from overfitting the small train dataset and give the student a more diversed representation for classification Along with a cross-entropy loss for classification: the final loss function for the student model is: The student model is then used for evaluation Dataset Only the subset of the ImageNet dataset given by the VIPrior challenge is used for our experiments, no external data or pre-trained checkpoint is used The VIPrior challenge dataset contains 1,000 classes which is the same with the original ImageNet, and is split into train, val and test splits, each of the splits has 50 images for each class, resulting in a total of 150,000 images Table 1: Training and Pre-training the model on the train split and evaluate the performance on the validation split on the given dataset. 'finetune fc' stands for train a linear classifier on top of the pretrained representation, 'finetune' stands for train the weight of the whole model. Our proposed pipeline (Phase-1 + Phase-2) can have 16.7 performance gain in top-1 validation accuracy. #Neg Margin Val Acc #Pretrain Epoch #Finetune Epoch Val Acc Table 2: The Val Acc means the linear classification accuracy obtained by fine- tune a linear classifier on top of the learned representation. The original MoCo v2 is sensitive to the number of negative, the performance drops drastically when number negatives is small. Our modified margin loss is less sensitive to the number negatives, as shown in the table, even has 16x less negatives the performance only drops 0.9. #Neg Margin Val Acc Table 3: The tricks used in",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Object Classification\", \"Dataset\": \"ImageNet VIPriors subset\", \"Metric\": \"Top-1\", \"Score\": \"68.8\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 1% labeled data\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"64.03%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 1% labeled data\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"84.88%\"}}, { \"LEADERBOARD\": { \"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of Params\", \"Score\": \"53.3M\"}}, { \"LEADERBOARD\": { \"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"73.8%\"}}, { \"LEADERBOARD\": { \"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"92.2%\"}} ]"
    },
    {
        "Context": "Graph Structured Network for Image-Text Matching Image-text matching has received growing interest since it bridges vision and language. The key challenge lies in how to learn correspondence between image and text. Existing works learn coarse correspondence based on object co-occurrence statistics, while failing to learn finegrained phrase correspondence. In this paper, we present a novel Graph Structured Matching Network (GSMN) to learn fine-grained correspondence. The GSMN explicitly models object, relation and attribute as a structured phrase, which not only allows to learn correspondence of object, relation and attribute separately, but also benefits to learn fine-grained correspondence of structured phrase. This is achieved by node-level matching and structure-level matching. The node-level matching associates each node with its relevant nodes from another modality, where the node can be object, relation or attribute. The associated nodes then jointly infer fine-grained correspondence by fusing neighborhood associations at structure-level matching. Comprehensive experiments show that GSMN outperforms state-of-the-art methods on benchmarks, with relative Re-call@1 improvements of nearly 7% and 2% on Flickr30K and MSCOCO, respectively. Code will be released at: https://github.com/CrossmodalGroup/GSMN . To validate the effectiveness of our proposed method, we evaluate it on two most widely used benchmarks, Flickr30K and MSCOCO The evaluation result is calculated on 5-folds of testing images The commonly used evaluation metrics for image-text matching are Recall@K (K=1,5,10), denoted as R@1, R@5, and R@10, which depict the percentage of ground truth being retrieved at top 1, 5, 10 results, respectively Table 1: Image-text matching results on Flickr30K, \u2032 f t \u2032 and \u2032 f ixed \u2032 are fine-tuning and no fine-tuning. The bests are in bold. R@1 R@5 R@10 rSum Image - to - Text Text - to - Image Table 2: Image-text matching results on MSCOCO, \u2032 f t \u2032 and \u2032 f ixed \u2032 are fine-tuning and no fine-tuning. The bests are in bold. R@1 R@5 R@10 rSum Image - to - Text Text - to - Image Table 3: The ablation study on Flickr30K to investigate the effect of different network structures. R@1 R@10 Image - to - Text Text - to - Image",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Cross-Modal Retrieval\", \"Dataset\": \"Flickr30k\", \"Metric\": \"Image-to-text R@1\", \"Score\": \"76.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-Modal Retrieval\", \"Dataset\": \"Flickr30k\", \"Metric\": \"Image-to-text R@10\", \"Score\": \"97.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-Modal Retrieval\", \"Dataset\": \"Flickr30k\", \"Metric\": \"Image-to-text R@5\", \"Score\": \"94.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-Modal Retrieval\", \"Dataset\": \"Flickr30k\", \"Metric\": \"Text-to-image R@1\", \"Score\": \"57.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-Modal Retrieval\", \"Dataset\": \"Flickr30k\", \"Metric\": \"Text-to-image R@10\", \"Score\": \"89.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-Modal Retrieval\", \"Dataset\": \"Flickr30k\", \"Metric\": \"Text-to-image R@5\", \"Score\": \"82.3\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Retrieval\", \"Dataset\": \"Flickr30K 1K test\", \"Metric\": \"R@1\", \"Score\": \"29.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Retrieval\", \"Dataset\": \"Flickr30K 1K test\", \"Metric\": \"R@10\", \"Score\": \"72.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Retrieval\", \"Dataset\": \"Flickr30K 1K test\", \"Metric\": \"R@5\", \"Score\": \"60.1\"}} ]"
    },
    {
        "Context": "Nystr\u00f6mformer: A Nystr\u00f6m-based Algorithm for Approximating Self-Attention Transformers have emerged as a powerful tool fora broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences -a topic being actively studied in the community. To address this limitation, we propose Nystr\u00f6mformer -a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nystr\u00f6m method to approximate standard self-attention with O(n) complexity. The scalability of Nystr\u00f6mformer enables application to longer sequences with thousands of tokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard sequence length, and find that our Nystr\u00f6mformer performs comparably, or in a few cases, even slightly better, than standard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nystr\u00f6mformer performs favorably relative to other efficient self-attention methods. Our code is available at https://github.com/mlpen/Nystromformer. In the first stage, we train Nystr\u00f6mformer on a largescale text corpus, and report the language modeling performance of our model on a hold-out validation set In the second stage, we fine-tune the pre-trained Nystr\u00f6mformer across several different NLP tasks in GLUE benchmarks) and IMDB reviews, and report the performance on individual dataset for each task Table 1: Memory consumption and running time results on various input sequence length. We report the average memory consump- input sequence length n 4096 time ( ms ) 2048 512 1024 8192 memory ( MB ) Table 2: Results on natural language understanding tasks. We re- IMDB QQP SST - 2 MRPC QNLI Table 3: Results on Long Range Arena (LRA) benchmark using our PyTorch implementation. We report classification accuracy for each Pathfinder ( 1K ) Retrieval ( 4K ) Avg Text ( 4K ) ListOps ( 2K ) Image ( 1K )",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Natural Language Inference\", \"Dataset\": \"QNLI\", \"Metric\": \"Accuracy\", \"Score\": \"88.7%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Textual Similarity\", \"Dataset\": \"MRPC\", \"Metric\": \"F1\", \"Score\": \"88.1%\"}}, {\"LEADERBOARD\": {\"Task\": \"Sentiment Analysis\", \"Dataset\": \"IMDb\", \"Metric\": \"Accuracy\", \"Score\": \"93.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Sentiment Analysis\", \"Dataset\": \"SST-2 Binary classification\", \"Metric\": \"Accuracy\", \"Score\": \"91.4\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Natural Language Inference\", \"Dataset\": \"MultiNLI\", \"Metric\": \"Matched\", \"Score\": \"91.0\"}}, { \"LEADERBOARD\": { \"Task\": \"Natural Language Inference\", \"Dataset\": \"MultiNLI\", \"Metric\": \"Mismatched\", \"Score\": \"90.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Natural Language Inference\", \"Dataset\": \"SNLI\", \"Metric\": \"% Test Accuracy\", \"Score\": \"93.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Natural Language Inference\", \"Dataset\": \"SciTail\", \"Metric\": \"Accuracy\", \"Score\": \"91.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Natural Language Inference\", \"Dataset\": \"WNLI\", \"Metric\": \"Accuracy\", \"Score\": \"91.89%\"}}, { \"LEADERBOARD\": { \"Task\": \"Natural Language Inference\", \"Dataset\": \"QNLI\", \"Metric\": \"Accuracy\", \"Score\": \"95.4%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Textual Similarity\", \"Dataset\": \"STS Benchmark\", \"Metric\": \"Pearson Correlation\", \"Score\": \"0.9"
    },
    {
        "Context": "NP Datalog: a Logic Language for Expressing NPSearch and Optimization Problems This paper presents a logic language for expressing NP search and optimization problems. Specifically, first a language obtained by extending (positive) DATALOG with intuitive and efficient constructs (namely, stratified negation, constraints and exclusive disjunction) is introduced. Next, a further restricted language only using a restricted form of disjunction to define (non-deterministically) subsets (or partitions) of relations is investigated. This language, called NP Datalog , captures the power of DATALOG \u00ac in expressing search and optimization problems. A system prototype implementing NP Datalog is presented. The system translates NP Datalog queries into OPL programs which are executed by the ILOG OPL Development Studio. Our proposal combines easy formulation of problems, expressed by means of a declarative logic language, with the efficiency of the ILOG System. Several experiments show the effectiveness of this approach. The performances of the systems have been evaluated by measuring the time necessary to find one solution of the following problems: 3-Coloring, Hamiltonian Cycle, Transitive Closure, Min Coloring, N-Queens and Latin Squares The Hamiltonian Cycle problem has been evaluated over benchmark graphs used to test other systems (HC Instances) and random graphs generated by means of Culberson\"s graph generator (HC Program Archive)",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "SEE: Towards Semi-Supervised End-to-End Scene Text Recognition Detecting and recognizing text in natural scene images is a challenging, yet not completely solved task. In recent years several new systems that try to solve at least one of the two sub-tasks (text detection and text recognition) have been proposed. In this paper we present SEE, a step towards semi-supervised neural networks for scene text detection and recognition, that can be optimized end-to-end. Most existing works consist of multiple deep neural networks and several pre-processing steps. In contrast to this, we propose to use a single deep neural network, that learns to detect and recognize text from natural images, in a semi-supervised way. SEE is a network that integrates and jointly learns a spatial transformer network, which can learn to detect text regions in an image, and a text recognition network that takes the identified text regions and recognizes their textual content. We introduce the idea behind our novel approach and show its feasibility, by performing a range of experiments on standard benchmark datasets, where we achieve competitive results. In this section we evaluate our presented network architecture on standard scene text detection/recognition benchmark datasets While performing our experiments we tried to answer the following questions: (1) Is the concept of letting the network automatically learn to detect text feasible? (2) Can we apply the method on areal world dataset? (3) Can we get any insights on what kind of features the network is trying: Operation method of grid generator and image sampler to extract? In order to answer these questions, we used different datasets On the one hand we used standard benchmark datasets for scene text recognition On the other hand we generated some datasets on our own First, we performed experiments on the SVHN dataset), that we used to prove that our concept as such is feasible Second, we generated more complex datasets based on SVHN images, to see how our system performs on images that contain several",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Optical Character Recognition\", \"Dataset\": \"FSNS - Test\", \"Metric\": \"Sequence error\", \"Score\": \"22\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Scene Text Detection\", \"Dataset\": \"ICDAR 2013\", \"Metric\": \"F-Measure\", \"Score\": \"90.1%\"}} ]"
    },
    {
        "Context": "stdchk: A Checkpoint Storage System for Desktop Grid Computing Checkpointing is an indispensable technique to provide fault tolerance for long-running high-throughput applications like those running on desktop grids. This paper argues that a dedicated checkpoint storage system, optimized to operate in these environments, can offer multiple benefits: reduce the load on a traditional file system, offer high-performance through specialization, and, finally, optimize data management by taking into account checkpoint application semantics. Such a storage system can present a unifying abstraction to checkpoint operations, while hiding the fact that there are no dedicated resources to store the checkpoint data.We prototype stdchk, a checkpoint storage system that uses scavenged disk space from participating desktops to build a low-cost storage system, offering a traditional file system interface for easy integration with applications. This paper presents the stdchk architecture, key performance optimizations, support for incremental checkpointing, and increased data availability. Our evaluation confirms that the stdchk approach is viable in a desktop grid setting and offers a low-cost storage system with desirable performance characteristics: high write throughput and reduced storage space and network effort to save checkpoint images. We evaluate our prototype under a range of micro-and macro-benchmarks Table 1 Time to write a 1 GB file. Local I/O FUSE to local I/O /stdchk/null Average Time (s) 11.80 12.00 1.04 Standard deviation 0.16 0.24 0.03 Local I / O FUSE to local I / O / stdchk / null Table 1 Time to write a 1 GB file . Table 2: Characteristics of the collected checkpoints. Applica tion ( BLCR ) checkpoint s # of Average size ( MB ) Table 2 : Characteristics of the collected checkpoints . Table 4: The effect of m and k on CbCH no-overlap performance. The table presents the ratio of detected similarity (in percentage), the heuristic's throughput in MB/s, the average resulted checkpoint size in KB, and the average minimum and maximum chunk sizes (Values for m in bytes and for k in bits) k m 20 32 64 128 256 1544 and for k in bits ) checkpoint KB , heuristic",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Accurate 3D Object Detection using Energy-Based Models Accurate 3D object detection (3DOD) is crucial for safe navigation of complex environments by autonomous robots. Regressing accurate 3D bounding boxes in cluttered environments based on sparse LiDAR data is however a highly challenging problem. We address this task by exploring recent advances in conditional energy-based models (EBMs) for probabilistic regression. While methods employing EBMs for regression have demonstrated impressive performance on 2D object detection in images, these techniques are not directly applicable to 3D bounding boxes. In this work, we therefore design a differentiable pooling operator for 3D bounding boxes, serving as the core module of our EBM network. We further integrate this general approach into the state-of-the-art 3D object detector SA-SSD. On the KITTI dataset, our proposed approach consistently outperforms the SA-SSD baseline across all 3DOD metrics, demonstrating the potential of EBM-based regression for highly accurate 3DOD. Code is available at https://github.com/fregu856/ebms_3dod. We evaluate our EBM-based 3DOD approach on the KITTI 3DOD dataset and compare it with the SA-SSD baseline and other state-of-the-art methods KITTI is the most commonly used dataset for automotive 3DOD On the KITTI benchmark server, models are evaluated in terms of average precision (AP) in both 3D and BEV",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Hard\", \"Metric\": \"AP\", \"Score\": \"72.78%\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Moderate val\", \"Metric\": \"AP\", \"Score\": \"86.83\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Easy val\", \"Metric\": \"AP\", \"Score\": \"95.45\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Easy\", \"Metric\": \"AP\", \"Score\": \"91.05%\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Hard val\", \"Metric\": \"AP\", \"Score\": \"82.23\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Moderate\", \"Metric\": \"AP\", \"Score\": \"80.12%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Moderate val\", \"Metric\": \"AP\", \"Score\": \"75.8\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Moderate val\", \"Metric\": \"AP\", \"Score\": \"59.3\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Easy val\", \"Metric\": \"AP\", \"Score\": \"54.3\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Easy\", \"Metric\": \"AP\", \"Score\": \"53.5\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Moderate val\", \"Metric\": \"AP\", \"Score\": \"56.1\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Moderate val\", \"Metric\": \"AP\", \"Score\": \"67.9%\"}} ]"
    },
    {
        "Context": "Fi-GNN: Modeling Feature Interactions via Graph Neural Networks for CTR Prediction Click-through rate (CTR) prediction is an essential task in web applications such as online advertising and recommender systems, whose features are usually in multi-field form. The key of this task is to model feature interactions among different feature fields. Recently proposed deep learning based models follow a general paradigm: raw sparse input multi-filed features are first mapped into dense field embedding vectors, and then simply concatenated together to feed into deep neural networks (DNN) or other specifically designed networks to learn high-order feature interactions. However, the simple unstructured combination of feature fields will inevitably limit the capability to model sophisticated interactions among different fields in a sufficiently flexible and explicit fashion.In this work, we propose to represent the multi-field features in a graph structure intuitively, where each node corresponds to a feature field and different fields can interact through edges. The task of modeling feature interactions can be thus converted to modeling node interactions on the corresponding graph. To this end, we design a novel model Feature Interaction Graph Neural Networks (Fi-GNN). Taking advantage of the strong representative power of graphs, our proposed model cannot only model sophisticated feature interactions in a flexible and explicit fashion, but also provide good model explanations for CTR prediction. Experimental results on two real-world datasets show its superiority over the state-of-the-arts. 4.1.1 Datasets We evaluate our proposed models on the following two datasets, whose statistics are summarized in This is a famous industry benchmark dataset for CTR prediction, which has 45 million users' click records in 39 anonymous feature fields on displayed ads This dataset contains users' click behaviors on displayed mobile ads For the two datasets, we remove the infrequent features appearing in less than 10, 5 times respectively and treat them as a single feature \"<unknown>\" We use the following two metrics for model evaluation: AUC (Area Under the ROC curve) and Logloss (cross entropy) Table 1: Statistics of evaluation datasets. 998 , 960 #Fields Table 2: Performance Comparison of Different methods. The best performance on each dataset and metric are highlighted. Further analysis is provided in Section 4.2. Second - order RI - Logloss Logloss Criteo Model Avazu AUC RI - AUC",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Click-Through Rate Prediction\", \"Dataset\": \"Avazu\", \"Metric\": \"AUC\", \"Score\": \"0.812\"}}, {\"LEADERBOARD\": {\"Task\": \"Click-Through Rate Prediction\", \"Dataset\": \"Avazu\", \"Metric\": \"LogLoss\", \"Score\": \"0.3817\"}}, {\"LEADERBOARD\": {\"Task\": \"Click-Through Rate Prediction\", \"Dataset\": \"Criteo\", \"Metric\": \"AUC\", \"Score\": \"0.8062\"}}, {\"LEADERBOARD\": {\"Task\": \"Click-Through Rate Prediction\", \"Dataset\": \"Criteo\", \"Metric\": \"Log Loss\", \"Score\": \"0.4453\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Click-Through Rate Prediction\", \"Dataset\": \"34.4\", \"Metric\": \"AUC\", \"Score\": \"0.8104\"}}, { \"LEADERBOARD\": { \"Task\": \"Click-Through Rate Prediction\", \"Dataset\": \"34.4\", \"Metric\": \"Log Loss\", \"Score\": \"0.4416\"}} ]"
    },
    {
        "Context": "BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation Recent leading approaches to semantic segmentation rely on deep convolutional networks trained with humanannotated, pixel-level segmentation masks. Such pixelaccurate supervision demands expensive labeling effort and limits the performance of deep networks that usually benefit from more training data. In this paper, we propose a method that achieves competitive accuracy but only requires easily obtained bounding box annotations. The basic idea is to iterate between automatically generating region proposals and training convolutional networks. These two steps gradually recover segmentation masks for improving the networks, and vise versa. Our method, called \"BoxSup\", produces competitive results (e.g., 62.0% mAP for validation) supervised by boxes only, on par with strong baselines (e.g., 63.8% mAP) fully supervised by masks under the same setting. By leveraging a large amount of bounding boxes, BoxSup further unleashes the power of deep convolutional networks and yields state-of-the-art results on PAS-CAL VOC 2012 and PASCAL-CONTEXT [24]. The VGG model is also used by all competitors We first evaluate our method on the PASCAL VOC 2012 semantic segmentation benchmark This dataset involves 20 semantic categories of objects We use the \"comp6\" evaluation protocol The accuracy is evaluated by mean IoU scores We further perform experiments on the recently labeled PASCAL-CONTEXT dataset This dataset provides ground-truth semantic labels for the whole scene, including object and stuff (e.g., grass, sky, water) The training and evaluation are performed on the training and validation sets that have 4,998 and 5,105 images respectively To train a BoxSup model for this dataset, we first use the box annotations from all 80 object categories in the COCO dataset to train the FCN (using VGG-16) Table 1: Comparisons of supervision in PASCAL VOC 2012 validation. 10 , 582 VOC train + COCO 133 , 869 9 , 118 123 , 287 1 , 464 box semi VOC train - mask Table 2: Comparisons of estimated masks for supervision in PASCAL VOC 2012 validation. All methods only use 10,582 bounding boxes as annotations, with no ground- truth segmentation mask used. mean IoU Table 3: Comparisons of the effects of region proposal methods on our method in PASCAL VOC 2012 validation. All methods only use 10,582 bounding boxes as annota- tions, with no ground-truth segmentation mask used. SS MCG GOP",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL Context\", \"Metric\": \"mIoU\", \"Score\": \"40.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL VOC 2012 test\", \"Metric\": \"Mean IoU\", \"Score\": \"64.6%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL Context\", \"Metric\":\"mIoU\", \"Score\": \"60.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL VOC 2012 test\", \"Metric\": \"Mean IoU\", \"Score\": \"69.8%\"}} ]"
    },
    {
        "Context": "ESPRESSO: A FAST END-TO-END NEURAL SPEECH RECOGNITION TOOLKIT We present ESPRESSO, an open-source, modular, extensible endto-end neural automatic speech recognition (ASR) toolkit based on the deep learning library PyTorch and the popular neural machine translation toolkit FAIRSEQ. ESPRESSO supports distributed training across GPUs and computing nodes, and features various decoding approaches commonly employed in ASR, including look-ahead word-based language model fusion, for which a fast, parallelized decoder is implemented. ESPRESSO achieves state-of-the-art ASR performance on the WSJ, LibriSpeech, and Switchboard data sets among other end-to-end systems without data augmentation, and is 4-11\u00d7 faster for decoding than similar systems (e.g. ESPNET).  Table 2. Hyper-parameters for the three recipes. Hyper - parameter SWBD LM 48 18M 65k 113M 1 , 733 70M 80M 5k 435 1k 50 52 ASR 35 1 , 200 1 , 024 174M 1 , 783 25M WSJ 320 1 , 800 640 800 LibriSpeech 60 Table 3. WERs (%) on the WSJ dev93 and eval92 set. eval92 dev93 Table 4. Breakdown of the WERs (%) on WSJ. Sub eval92 Del dev93 Ins Table 5. WERs (%) on the LibriSpeech dev and test sets. other dev test clean Table 6. WERs (%) on the SWBD Hub5'00 evaluation set. CallHome Switchboard Table 7. Training (per epoch) and decoding wall time on WSJ. ASR LM 1min 27s 46min 36min 29min 16s ASR Decoding ( eval92 ) Training w / o LM w / look - ahead LM 31min 56min 5min 21s 2min 44s",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Speech Recognition\", \"Dataset\": \"Hub5'00 SwitchBoard\", \"Metric\": \"Eval2000\", \"Score\": \"9.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Speech Recognition\", \"Dataset\": \"WSJ eval92\", \"Metric\": \"Word Error Rate (WER)\", \"Score\": \"3.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Speech Recognition\", \"Dataset\": \"LibriSpeech test-clean\", \"Metric\": \"Word Error Rate (WER)\", \"Score\": \"2.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Speech Recognition\", \"Dataset\": \"LibriSpeech test-other\", \"Metric\": \"Word Error Rate (WER)\", \"Score\": \"8.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Speech Recognition\", \"Dataset\": \"Hub5'00 CallHome\", \"Metric\": \"Word Error Rate (WER)\", \"Score\": \"19.1\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Speech Recognition\", \"Dataset\": \"LibriSpeech test-clean\", \"Metric\": \"Word Error Rate (WER)\", \"Score\": \"2.10\"}}, { \"LEADERBOARD\": { \"Task\": \"Speech Recognition\", \"Dataset\": \"LibriSpeech test-other\", \"Metric\": \"Word Error Rate (WER)\", \"Score\": \"3.83\"}} ]"
    },
    {
        "Context": "Classification of Shoulder X-Ray Images with Deep Learning Ensemble Models Fractures occur in the shoulder area, which has a wider range of motion than other joints in the body, for various reasons. To diagnose these fractures, data gathered from X-radiation (Xray), magnetic resonance imaging (MRI), or computed tomography (CT) are used. This study aims to help physicians by classifying shoulder images taken from X-ray devices as fracture / non-fracture with artificial intelligence. For this purpose, the performances of 26 deep learning-based pre-trained models in the detection of shoulder fractures were evaluated on the musculoskeletal radiographs (MURA) dataset, and two ensemble learning models (EL1 and EL2) were developed. The pretrained models used are ResNet, ResNeXt, DenseNet, VGG, Inception, MobileNet, and their spinal fully connected (Spinal FC) versions. In the EL1 and EL2 models developed using pre-trained models with the best performance, test accuracy was 0.8455,0.8472, Cohen's kappa was 0.6907, 0.6942 and the area that was related with fracture class under the receiver operating characteristic (ROC) curve (AUC) was 0.8862,0.8695. As a result of 28 different classifications in total, the highest test accuracy and Cohen's kappa values were obtained in the EL2 model, and the highest AUC value was obtained in the EL1 model. The MURA dataset was first introduced to the literature in a paper published in the OpenReview platform, announced in the conference on \"Medical Imaging with Deep Learning\" held in Amsterdam in 2018 Following this publication, this dataset was made publicly available for academic studies in a competition called \"Bone X-Ray Deep Learning Competition\" by the Machine Learning group of the Stanford University Being one of the largest public radiographic image datasets, MURA contains a total of 40,561 X-ray images in png format for the following parts of the body, labeled as either normal or abnormal (fracture): elbow, finger, forearm, hand, humerus, shoulder, and wrist using DenseNet-169 on this dataset, the AUC score representing the area under the overall Receiver Operator Characteristics (ROC) curve was 0.929, and the overall Cohen's kappa score was 0.705 Following this first study in which this dataset was introduced to the literature, there have been various studies Table 1. Layer with values of Spinal FCs used in classification models. Width Spinal FC Layer Width Spinal FC Layer Models 20 ResNeXt50 Table 2. Details of the shoulder bone X-ray images used in the study png , Bone X - ray Images New Image Test Dataset Size Table 7. Training accuracy results of classification models. Spinal DenseNet - 169 Standart Models VGG - 19 ResNet - 101 Net Standart FC FC ResNext - 101 MobileNet - v2 Spinal FC Table 8. Test accuracy results of classification models. Spinal Standart Models Net FC Table 9. Precision results of classification models. 75% Spinal 85% Standart Models VGG - 19 Net Standart FC FC ResNext - 101 MobileNet - v2 Spinal FC Table 10. Recall results of classification models. Spinal DenseNet - 169 Standart Models VGG - 19 ResNet - 101 Net Standart FC FC ResNext - 101 MobileNet - v2 Spinal FC",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"Fracture/Normal Shoulder Bone X-ray Images on MURA\", \"Metric\": \"Cohen\u2019s Kappa score\", \"Score\": \"0.6942\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"Fracture/Normal Shoulder Bone X-ray Images on MURA\", \"Metric\": \"Test Accuracy\", \"Score\": \"84.72%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"Fracture/Normal Shoulder Bone X-ray Images on MURA\", \"Metric\": \"AUC score\", \"Score\": \"0.8862\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Sarcasm Detection\", \"Dataset\": \"Automatic Misogynistic Identification\", \"Metric\": \"Accuracy\", \"Score\": \"0.83\"}} ]"
    },
    {
        "Context": "batchboost: REGULARIZATION FOR STABILIZING TRAINING WITH RESISTANCE TO UNDERFITTING & OVERFITTING DRAFT Overfitting & underfitting and stable training are an important challenges in machine learning. Current approaches for these issues are mixup[1], SamplePairing[2] and BC learning [3]. In our work, we state the hypothesis that mixing many images together can be more effective than just two. batchboost pipeline has three stages: (a) pairing: method of selecting two samples. (b) mixing: how to create anew one from two samples. (c) feeding: combining mixed samples with new ones from dataset into batch (with ratio \u03b3). Note that sample that appears in our batch propagates with subsequent iterations with less and less importance until the end of training. Pairing stage calculates the error per sample, sorts the samples and pairs with strategy: hardest with easiest one, than mixing stage merges two samples using mixup, x 1 + (1 \u2212 \u03bb)x 2 . Finally, feeding stage combines new samples with mixed by ratio 1:1. batchboost has 0.5-3% better accuracy than the current state-of-the-art mixup regularization on CIFAR-10[4] & Fashion-MNIST[5]. Our method is slightly better than SamplePairing technique on small datasets (up to 5%). batchboost provides stable training on not tuned parameters (like weight decay), thus its a good method to test performance of different architectures. Source code is at: https://github.com/maciejczyzewski/batchboost",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Percentage correct\", \"Score\": \"97.54\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"Kuzushiji-MNIST\", \"Metric\": \"Accuracy\", \"Score\": \"98.13\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"Kuzushiji-MNIST\", \"Metric\": \"Error\", \"Score\": \"8.25\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"Kuzushiji-MNIST\", \"Metric\": \"Error\", \"Score\": \"8.59\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"Kuzushiji-MNIST\", \"Metric\": \"Error\", \"Score\": \"2.77\"}} ]"
    },
    {
        "Context": "TransReID: Transformer-based Object Re-Identification Extracting robust feature representation is one of the key challenges in object re-identification (ReID). Although convolution neural network (CNN)-based methods have achieved great success, they only process one local neighborhood at a time and suffer from information loss on details caused by convolution and downsampling operators (e.g. pooling and strided convolution). To overcome these limitations, we propose a pure transformer-based object ReID framework named TransReID. Specifically, we first encode an image as a sequence of patches and build a transformer-based strong baseline with a few critical improvements, which achieves competitive results on several ReID benchmarks with CNN-based methods. To further enhance the robust feature learning in the context of transformers, two novel modules are carefully designed. (i) The jigsaw patch module (JPM) is proposed to rearrange the patch embeddings via shift and patch shuffle operations which generates robust features with improved discrimination ability and more diversified coverage. (ii) The side information embeddings (SIE) is introduced to mitigate feature bias towards camera/view variations by plugging in learnable embeddings to incorporate these non-visual clues. To the best of our knowledge, this is the first work to adopt a pure transformer for ReID research. Experimental results of TransReID are superior promising, which achieve stateof-the-art performance on both person and vehicle ReID benchmarks. Code is available at https://github. com/heshuting555/TransReID. We evaluate our proposed method on four person ReID datasets, Market-1501, DukeMTMC-reID, MSMT17, Occluded-Duke, and two vehicle ReID datasets, VeRi-776 and VehicleID It is noted that, unlike other datasets, images in Occluded-Duke are selected from DukeMTMC-reID and the training/query/gallery set contains 9%/ 100%/ 10% occluded images respectively All datasets except VehicleID provide camera ID for each image, while only VeRi-776 and VehicleID dataset provide viewpoint labels for each image The details of these datasets are summarized in Table 1: Statistics of datasets used in the paper. - 15 #view #cam - . Table 2: Comparison of different backbones. Inference time is represented by comparing each model to ResNet50 as only relative comparison is necessary. All the experiments were carried out on the same machine for fair comparison. ViT-B/16 is regarded as the baseline model and abbreviated as Baseline in the rest of this paper. VeRi - 776 MSMT17 Inference Time mAP R1 Table 4: Ablation study of SIE. Since the person ReID datasets do not provide viewpoint annotations, viewpoint information can only be encoded in VeRi-776. Pairwise Distance ( w / o SIE ) intra_viewpoint \u221a inter_camera VeRi - 776 Viewpoint \u221a Method Camera inter_viewpoint mAP R1 Table 5. For the Baseline, JPM and SIE improve the performance by +2.6%/+1.0% mAP and +1.4%/+1.4% mAP on MSMT17/VeRi-776, respectively. With these two modules used together, TransReID achieves 64.9% (+3.9%) mAP",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Person Re-Identification\", \"Dataset\": \"MSMT17\", \"Metric\": \"Rank-1\", \"Score\": \"86.20\"}}, {\"LEADERBOARD\": {\"Task\": \"Person Re-Identification\", \"Dataset\": \"MSMT17\", \"Metric\": \"mAP\", \"Score\": \"69.40\"}}, {\"LEADERBOARD\": {\"Task\": \"Person Re-Identification\", \"Dataset\": \"Market-1501\", \"Metric\": \"MAP\", \"Score\": \"89.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Person Re-Identification\", \"Dataset\": \"Market-1501\", \"Metric\": \"Rank-1\", \"Score\": \"95.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Person Re-Identification\", \"Dataset\": \"DukeMTMC-reID\", \"Metric\": \"MAP\", \"Score\": \"82.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Person Re-Identification\", \"Dataset\": \"DukeMTMC-reID\", \"Metric\": \"Rank-1\", \"Score\": \"91.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Vehicle Re-Identification\", \"Dataset\": \"VeRi-776\", \"Metric\": \"Rank-1\", \"Score\": \"97.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Vehicle Re-Identification\", \"Dataset\": \"VeRi-776\", \"Metric\": \"mAP\", \"Score\": \"82.3\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Person Re-Identification\", \"Dataset\": \"MSMT17\", \"Metric\": \"Rank-1\", \"Score\": \"78.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Person Re-Identification\", \"Dataset\": \"MSMT17\", \"Metric\":\"mAP\", \"Score\": \"52.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Person Re-Identification\", \"Dataset\": \"Market-1501\", \"Metric\": \"MAP\", \"Score\": \"67.07\"}}, { \"LEADERBOARD\": { \"Task\": \"Person Re-Identification\", \"Dataset\": \"Market-1501\", \"Metric\": \"Rank-1\", \"Score\": \"74.16\"}}, { \"LEADERBOARD\": { \"Task\": \"Person Re-Identification\", \"Dataset\": \"DukeMTMC-reID\", \"Metric\": \"MAP\", \"Score\": \"78.59\"}}, { \"LEADERBOARD\": { \"Task\": \"Person Re-Identification\", \"Dataset\": \"DukeMTMC-reID\", \"Metric\": \"Rank-1\", \"Score\": \"88.2\"}} ]"
    },
    {
        "Context": "A BERT-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies In this paper, we investigate the following two limitations for the existing distractor generation (DG) methods. First, the quality of the existing DG methods are still far from practical use. There are still room for DG quality improvement. Second, the existing DG designs are mainly for single distractor generation. However, for practical MCQ preparation, multiple distractors are desired. Aiming at these goals, in this paper, we present anew distractor generation scheme with multi-tasking and negative answer training strategies for effectively generating multiple distractors. The experimental results show that (1) our model advances the state-of-the-art result from 28.65 to 39.81 (BLEU 1 score) and (2) the generated multiple distractors are diverse and shows strong distracting power for multiple choice question. Datasets We follow the setting to evaluate our framework with the RACE dataset Table 2: A Running Example for the BDG scheme Iter . [ C ] Table 3: Answer Copying Problem on P.M. Gold 12 57 P . M . Table 4: Training Data Statistics 12284 96501 Table 5: Performance Comparison on Token Scores BLEU 4 BLEU 3 BLEU 2 BLEU 1 ROUGE L Table 6: The Effect on Mitigating Answer Copying Problem 0 121 0 Gold AN+PM BDG 115 109 GPT Table 5 : Performance Comparison on Token Scores Table 7: Comparison by MCQ Accuracy Accuracy Table 6 : The Effect on Mitigating Answer Copying Problem Table 9: Performance Comparison on Token Scores with Different \u03b3 Settings BLEU 4 BLEU 3 BLEU 2 BLEU 1 ROUGE L",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Distractor Generation\", \"Dataset\": \"RACE\", \"Metric\": \"BLEU-1\", \"Score\": \"39.81\"}}, {\"LEADERBOARD\": {\"Task\": \"Distractor Generation\", \"Dataset\": \"RACE\", \"Metric\": \"BLEU-2\", \"Score\": \"24.81\"}}, {\"LEADERBOARD\": {\"Task\": \"Distractor Generation\", \"Dataset\": \"RACE\", \"Metric\": \"BLEU-3\", \"Score\": \"17.66\"}}, {\"LEADERBOARD\": {\"Task\": \"Distractor Generation\", \"Dataset\": \"RACE\", \"Metric\": \"BLEU-4\", \"Score\": \"13.56\"}}, {\"LEADERBOARD\": {\"Task\": \"Distractor Generation\", \"Dataset\": \"RACE\", \"Metric\": \"ROUGE-L\", \"Score\": \"34.01\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Reading Comprehension\", \"Dataset\": \"RACE\", \"Metric\": \"Accuracy\", \"Score\": \"89.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Reading Comprehension\", \"Dataset\": \"RACE\", \"Metric\": \"Accuracy (High)\", \"Score\": \"92.6\"}}, { \"LEADERBOARD\": { \"Task\": \"Reading Comprehension\", \"Dataset\": \"RACE\", \"Metric\": \"Accuracy (Middle)\", \"Score\": \"88.7\"}} ]"
    },
    {
        "Context": "A Style-Based Generator Architecture for Generative Adversarial Networks We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce anew, highly varied and high-quality dataset of human faces. We have collected anew dataset of human faces, Flickr-Faces-HQ (FFHQ), consisting of 70,000 high-quality images at 1024 2 resolution The dataset includes vastly more variation than CELEBA-HQ in terms of age, ethnicity and image background, and also has much better coverage of accessories such as eyeglasses, sunglasses, hats, etc We have made the dataset publicly available at https://github.com/NVlabs/ffhq-dataset BEDROOM the coarse styles basically control the viewpoint of the camera, middle styles select the particular furniture, and fine styles deal with colors and smaller details of materials These datasets were trained using the same setup as FFHQ for the duration of 70M images for BEDROOM and CATS, and 46M for CARS CARS has much higher quality training data that also allows higher spatial resolution (512 \u00d7 384 instead of 256 2 ), and CATS continues to be a difficult dataset due to the high intrinsic variation in poses, zoom levels, and backgrounds Table 1. Fr\u00e9chet inception distance (FID) for various generator de- signs (lower is better). In this paper we calculate the FIDs using 50,000 images drawn randomly from the training set, and report the lowest distance encountered over the course of training. FFHQ CelebA - HQ Table 2. FIDs in FFHQ for networks trained by enabling the mix- ing regularization for different percentage of training examples. Here we stress test the trained networks by randomizing 1 . . . 4 latents and the crossover points between them. Mixing regular- ization improves the tolerance to these adverse operations signifi- cantly. Labels E and F refer to the configurations in Table 1. 1 2 3 4 Number of latents during testing Table 3. Perceptual path lengths and separability scores for various generator architectures in FFHQ (lower is better). We perform the measurements in Z for the traditional network, and in W for style- based",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Generation\", \"Dataset\": \"LSUN Bedroom 256 x 256\", \"Metric\": \"FID\", \"Score\": \"2.65\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Generation\", \"Dataset\": \"CelebA-HQ 1024x1024\", \"Metric\": \"FID\", \"Score\": \"5.06\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Generation\", \"Dataset\": \"FFHQ\", \"Metric\": \"FID\", \"Score\": \"4.43\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Generation\", \"Dataset\": \"LSUN Bedroom\", \"Metric\": \"FID-50k\", \"Score\": \"2.65\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"CelebA-HQ 128x128\", \"Metric\": \"FID\", \"Score\": \"5.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"LSUN Bedroom 256 x 256\", \"Metric\": \"FID\", \"Score\": \"6.95\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"ImageNet 128x128\", \"Metric\": \"FID\", \"Score\": \"4.0\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"FFHQ 256 x 256\", \"Metric\": \"FID\", \"Score\": \"41.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"FFHQ 256 x 256\", \"Metric\": \"FID\", \"Score\": \"41.9\"}} ]"
    },
    {
        "Context": "Scale-aware Fast R-CNN for Pedestrian Detection In this work, we consider the problem of pedestrian detection in natural scenes. Intuitively, instances of pedestrians with different spatial scales may exhibit dramatically different features. Thus, large variance in instance scales, which results in undesirable large intra-category variance in features, may severely hurt the performance of modern object instance detection methods. We argue that this issue can be substantially alleviated by the divide-and-conquer philosophy. Taking pedestrian detection as an example, we illustrate how we can leverage this philosophy to develop a Scale-Aware Fast R-CNN (SAF R-CNN) framework. The model introduces multiple built-in subnetworks which detect pedestrians with scales from disjoint ranges. Outputs from all the sub-networks are then adaptively combined to generate the final detection results that are shown to be robust to large variance in instance scales, via agate function defined over the sizes of object proposals. Extensive evaluations on several challenging pedestrian detection datasets well demonstrate the effectiveness of the proposed SAF R-CNN. Particularly, our method achieves state-of-the-art performance on Caltech [8], INRIA [5], and ETH [9], and obtains competitive results on KITTI [11]. We evaluate the effectiveness of the proposed SAF R-CNN on several popular pedestrian detection datasets including Caltech, INRIA, ETH, and KITTI More experimental analyses on the effectiveness of each component in our network are further given on the challenging Caltech dataset Datasets 1) Caltech: The Caltech dataset and its associated benchmark are among the most popular pedestrian detection datasets Every frame in the raw Caltech dataset has been densely annotated with the bounding boxes of pedestrian instances In the reasonable evaluation setting, the performance is evaluated on pedestrians over 50 pixels tall with no or partial occlusion 2) INRIA and ETH: The INRIA pedestrian dataset is split into a training and a testing set Following the training setting commonly adopted by the best performing approaches [10], we train our SAF R-CNN model using the INRIA training set and test it on both the INRIA and the ETH testing sets, in order",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Pedestrian Detection\", \"Dataset\": \"Caltech\", \"Metric\": \"Reasonable Miss Rate\", \"Score\": \"9.68\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Pedestrian Detection\", \"Dataset\": \"Caltech\", \"Metric\": \"Reasonable Miss Rate\", \"Score\": \"12.4\"}} ]"
    },
    {
        "Context": "Learning One Class Representations for Face Presentation Attack Detection using Multi-channel Convolutional Neural Networks Face recognition has evolved as a widely used biometric modality. However, its vulnerability against presentation attacks poses a significant security threat. Though presentation attack detection (PAD) methods try to address this issue, they often fail in generalizing to unseen attacks. In this work, we propose anew framework for PAD using a one-class classifier, where the representation used is learned with a Multi-Channel Convolutional Neural Network (MCCNN). A novel loss function is introduced, which forces the network to learn a compact embedding for bonafide class while being far from the representation of attacks. A one-class Gaussian Mixture Model is used on top of these embeddings for the PAD task. The proposed framework introduces a novel approach to learn a robust PAD system from bonafide and available (known) attack classes. This is particularly important as collecting bonafide data and simpler attacks are much easier than collecting a wide variety of expensive attacks. The proposed system is evaluated on the publicly available WMCA multi-channel face PAD database, which contains a wide variety of 2D and 3D attacks. Further, we have performed experiments with MLFP and SiW-M datasets using RGB channels only. Superior performance in unseen attack protocols shows the effectiveness of the proposed approach. Software, data, and protocols to reproduce the results are made available publicly. In order to evaluate the effectiveness of the proposed approach, we have performed experiments in three publicly available databases, namely WMCA, MLFP, and SiW-M datasets We have made challenging protocols in the WMCA dataset to perform an extensive set of evaluations emulating real-world unseen attack scenarios 1) Protocols in SiW-M: To emulate unseen attack scenarios, we use the leave-one-out (LOO) testing protocols available with the SiW-M dataset The evaluation set consists of 20% of bonafide data and the attack which was left out in the training phase The subjects in bonafide sets are disjoint in train and evaluation sets For the MLFP dataset, we report only EER in the evaluation set since only two sets are available \u2022 MCCNN(BCE+OCCL)-GMM: Here, the bonafide embeddings from the MCCNN trained using both the losses are used to train a GMM, and in the evaluation stage, the score from the one class GMM is used",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Face Anti-Spoofing\", \"Dataset\": \"MLFP\", \"Metric\": \"HTER\", \"Score\": \"3.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Presentation Attack Detection\", \"Dataset\": \"WMCA\", \"Metric\": \"ACER\", \"Score\": \"0.097\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Face Presentation Attack Detection\", \"Dataset\": \"WMCA\", \"Metric\": \"ACER\", \"Score\": \"0.3\"}} ]"
    },
    {
        "Context": "Temporal Dynamic Graph LSTM for Action-driven Video Object Detection In this paper, we investigate a weakly-supervised object detection framework. Most existing frameworks focus on using static images to learn object detectors. However, these detectors often fail to generalize to videos because of the existing domain shift. Therefore, we investigate learning these detectors directly from boring videos of daily activities. Instead of using bounding boxes, we explore the use of action descriptions as supervision since they are relatively easy to gather. A common issue, however, is that objects of interest that are not involved inhuman actions are often absent in global action descriptions known as \"missing label\". To tackle this problem, we propose a novel temporal dynamic graph Long Short-Term Memory network (TD-Graph LSTM). TD-Graph LSTM enables global temporal reasoning by constructing a dynamic graph that is based on temporal correlations of object proposals and spans the entire video. The missing label issue for each individual frame can thus be significantly alleviated by transferring knowledge across correlated objects proposals in the whole video. Extensive evaluations on a large-scale daily-life action dataset (i.e., Charades) demonstrates the superiority of our proposed method. We also release object bounding-box annotations for more than 5,000 frames in Charades. We believe this annotated data can also benefit other research on video-based object recognition in the future. Dataset Analysis We evaluate the action-drive weaklysupervised object detection performance on the Charades dataset The Charades video dataset is composed of daily indoor activities collected through Amazon Mechanical Turk In order to evaluate the video object detection performance over 17 daily object classes, we collect the bounding box annotations for 5,000 test frames from 200 videos in the Charades test set This poses more challenges for the object detection model compared to an image-based object detection dataset, such as the most popular PASCAL VOC that is widely used in existing weakly-based object detection methods further shows example frames with action labels on the Charades dataset Moreover, the video frames often appear with a very cluttered background, blurry objects and diverse viewpoints, which are more challenging and realistic compared to existing image datasets (e.g., MS COCO and ImageNet) and video datasets (e.g., UCF101) Evaluation Measures We evaluate the performance of both object Table 1. Per-class performance comparison of our proposed models with two state-of-the-art weakly-supervised learning methods when evaluating on the Charades dataset[32], test classification average precision (%). bounding box numbers in each image of the test set . bed tv towel vacuum window mAP 2500 table 10 12 sofa Percentage 18 ( 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 2000 ) 0 door laptop mirror pillow refri shelf 1 1500 3 ( % ) dish 5 6 broom chair Objects 8 cup Table 2. Per-class performance comparison of our proposed models with two state-of-the-art weakly-supervised learning methods when evaluating on the Charades dataset[32], test detection average precision (%). bed door laptop mirror pillow refri shelf sofa tv bed broom chair cup dish door laptop mirror pillow refri shelf sofa table dish broom chair towel vacuum window mAP table cup Table 3. Performance comparison of",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Weakly Supervised Object Detection\", \"Dataset\": \"Charades\", \"Metric\": \"MAP\", \"Score\": \"1.98\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Action Detection\", \"Dataset\": \"Charades\", \"Metric\":\"mAP\", \"Score\": \"45.1\"}} ]"
    },
    {
        "Context": "Correspondence Networks with Adaptive Neighbourhood Consensus In this paper, we tackle the task of establishing dense visual correspondences between images containing objects of the same category. This is a challenging task due to large intra-class variations and alack of dense pixel level annotations. We propose a convolutional neural network architecture, called adaptive neighbourhood consensus network (ANC-Net), that can be trained end-to-end with sparse keypoint annotations, to handle this challenge. At the core of ANC-Net is our proposed non-isotropic 4D convolution kernel, which forms the building block for the adaptive neighbourhood consensus module for robust matching. We also introduce a simple and efficient multi-scale self-similarity module in ANC-Net to make the learned feature robust to intra-class variations. Furthermore, we propose a novel orthogonal loss that can enforce the one-to-one matching constraint. We thoroughly evaluate the effectiveness of our method on various benchmarks, where it substantially outperforms state-of-the-art methods. Datasets We evaluate our method on four public datasets, namely, PF-PASCAL, Spair-71k, and CUB Spair-71k dataset is much more challenging than the others as it contains both large viewpoint differences and scale differences Spair-71k is only used to evaluate the transferrability of the models trained on the PF-PASCAL training split The CUB dataset contains 11,788 images of various species of birds with large variation of appearance, shape and pose Evaluation metric Following common practice, we use the percentage of correct key-points (PCK@\u03b1) as our evaluation metric Table 1: Comparison with state-of-the-art methods. Spair - 71k CUB PF - PASCAL - Table 2. Our method consistently outperforms NC-Net and DCCNet. w / o 302 w / o 95 Original Table 2: Unbiased evaluation on PF-PASCAL. w / o 302 w / o 95 Original Table 3: Ablation study experimental results. 82 . 6 / 83 . 7 PCK@0 . 1 78 . 9 / 81 . 9",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semantic correspondence\", \"Dataset\": \"PF-PASCAL\", \"Metric\": \"PCK\", \"Score\": \"88.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic correspondence\", \"Dataset\": \"SPair-71k\", \"Metric\": \"PCK\", \"Score\": \"30.1\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic correspondence\", \"Dataset\": \"N3-weighted\", \"Metric\": \"PCK@0.5\", \"Score\": \"88.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic correspondence\", \"Dataset\": \"N3-weighted\", \"Metric\": \"PCK@0.5\", \"Score\": \"43.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic correspondence\", \"Dataset\": \"N3-test\", \"Metric\": \"PCK@0.2\", \"Score\": \"42.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic correspondence\", \"Dataset\": \"N3-test\", \"Metric\": \"PCK@0.3\", \"Score\": \"42.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic correspondence\", \"Dataset\": \"N3-test\", \"Metric\": \"PCK@0.4\", \"Score\": \"32.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic correspondence\", \"Dataset\": \"N3-test\", \"Metric\": \"PCK@0.5\", \"Score\": \"41.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic correspondence\", \"Dataset\": \"N3-test\", \"Metric\": \"PCK@0.2\", \"Score\": \"37.3\""
    },
    {
        "Context": "Invariant Information Clustering for Unsupervised Image Classification and Segmentation We present a novel clustering objective that learns a neural network classifier from scratch, given only unlabelled data samples. The model discovers clusters that accurately match semantic classes, achieving state-of-the-art results in eight unsupervised clustering benchmarks spanning image classification and segmentation. These include STL10, an unsupervised variant of ImageNet, and CIFAR10, where we significantly beat the accuracy of our closest competitors by 6.6 and 9.5 absolute percentage points respectively. The method is not specialised to computer vision and operates on any paired dataset samples; in our experiments we use random transforms to obtain a pair from each image. The trained network directly outputs semantic labels, rather than high dimensional representations that need external processing to be usable for semantic clustering. The objective is simply to maximise mutual information between the class assignments of each pair. It is easy to implement and rigorously grounded in information theory, meaning we effortlessly avoid degenerate solutions that other clustering methods are susceptible to. In addition to the fully unsupervised mode, we also test two semi-supervised settings. The first achieves 88.8% accuracy on STL10 classification, setting anew global state-of-the-art overall existing methods (whether supervised, semi-supervised or unsupervised). The second shows robustness to 90% reductions in label coverage, of relevance to applications that wish to make use of small amounts of labels. github.com/xu-ji/IIC arXiv:1807.06653v4 [cs.CV]  Table 1: Unsupervised image clustering. Legend: \u2020Method based on k-means. \u2021Method that does not directly learn a clustering function and requires further application of k-means to be used for image clustering. Results obtained using our experiments with authors' original code. STL10 CIFAR10 CFR100 - 20 MNIST Table 2: Ablations of IIC (unsupervised setting). Each row shows a single change from the full setting. The full setting has auxiliary overclus- tering, 5 initialisation heads, 5 sample repeats, and uses the unlabelled data subset of STL10. STL10 Table 3: Fully and semi-supervised clas- sification. Legend: *Fully supervised method. Our experiments with authors' code. \u2020Multi-fold evaluation. STL10 Figure 5 : Unsupervised image clustering ( IIC ) results on STL10 . Dog Table 4: Unsupervised segmentation. IIC experiments use a single sub- head. Legend: \u2020Method based on k-means. \u2021Method that does not directly learn a clustering function and requires further application of k-means to be",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Unsupervised Semantic Segmentation\", \"Dataset\": \"Potsdam-3\", \"Metric\": \"Accuracy\", \"Score\": \"45.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Unsupervised Semantic Segmentation\", \"Dataset\": \"Potsdam\", \"Metric\": \"Accuracy\", \"Score\": \"65.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Unsupervised Semantic Segmentation\", \"Dataset\": \"COCO-Stuff-3\", \"Metric\": \"Accuracy\", \"Score\": \"72.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Unsupervised Semantic Segmentation\", \"Dataset\": \"COCO-Stuff-15\", \"Metric\": \"Accuracy\", \"Score\": \"27.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Clustering\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"ARI\", \"Score\": \"0.411\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Clustering\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Accuracy\", \"Score\": \"0.617\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Clustering\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Backbone\", \"Score\": \"ResNet-34\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Clustering\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"NMI\", \"Score\": \"0.511\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Clustering\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Train set\", \"Score\": \"Train+Test\"}}, {\"LEADERBOARD\": {\"Task\": \"Unsupervised MNIST\", \"Dataset\": \"MNIST\", \"Metric\": \"Accuracy\", \"Score\": \"99.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"STL-10\", \"Metric\": \"Percentage correct\", \"Score\": \"88.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"STL-10\", \"Metric\": \"Accuracy\", \"Score\": \"88.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Unsupervised Image Classification\", \"Dataset\": \"CIFAR-20\", \"Metric\": \"Accuracy\", \"Score\": \"25.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Unsupervised Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Accuracy\", \"Score\": \"61.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Unsupervised Image Classification\", \"Dataset\": \"STL-10\", \"Metric\": \"Accuracy\", \"Score\": \"61.00\"}}, {\"LEADERBOARD\": {\"Task\": \"Unsupervised Image Classification\", \"Dataset\": \"MNIST\", \"Metric\": \"Accuracy\", \"Score\": \"99.3\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"STL-10\", \"Metric\": \"Accuracy\", \"Score\": \"88.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"STL-10\", \"Metric\": \"Percentage correct\", \"Score\": \"88.8\"}} ]"
    },
    {
        "Context": "Interpretable 3D Human Action Analysis with Temporal Convolutional Networks The discriminative power of modern deep learning models for 3D human action recognition is growing ever so potent. In conjunction with the recent resurgence of 3D human action representation with 3D skeletons, the quality and the pace of recent progress have been significant. However, the inner workings of state-of-the-art learning based methods in 3D human action recognition still remain mostly black-box. In this work, we propose to use anew class of models known as Temporal Convolutional Neural Networks (TCN) for 3D human action recognition. Compared to popular LSTM-based Recurrent Neural Network models, given interpretable input such as 3D skeletons, TCN provides us away to explicitly learn readily interpretable spatio-temporal representations for 3D human action recognition. We provide our strategy in re-designing the TCN with interpretability in mind and how such characteristics of the model is leveraged to construct a powerful 3D activity recognition method. Through this work, we wish to take a step towards a spatio-temporal model that is easier to understand, explain and interpret. The resulting model, Res-TCN, achieves state-of-the-art results on the largest 3D human action recognition dataset, NTU-RGBD. We evaluate Res-TCN on 3D skeleton based human activity recognition dataset of NTU NTU RGB+D dataset is currently the largest human activity recognition dataset with full 3D skeleton annotations The dataset provides two train/test split paradigms: Cross-Subject (CS) and Cross-View (CV) settings The dataset covers 40 distinct subjects with varying physical traits Table 1. Comparison to other learning based methods on NTURGB+D skeleton dataset with Cross-Subject (CS) and Cross- View (CV) settings in accuracy (%). CS CV",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"Varying-view RGB-D Action-Skeleton\", \"Metric\": \"Accuracy (AV I)\", \"Score\": \"48%\"}}, {\"LEADERBOARD\": {\"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"Varying-view RGB-D Action-Skeleton\", \"Metric\": \"Accuracy (AV II)\", \"Score\": \"68%\"}}, {\"LEADERBOARD\": {\"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"Varying-view RGB-D Action-Skeleton\", \"Metric\": \"Accuracy (CS)\", \"Score\": \"63%\"}}, {\"LEADERBOARD\": {\"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"Varying-view RGB-D Action-Skeleton\", \"Metric\": \"Accuracy (CV I)\", \"Score\": \"14%\"}}, {\"LEADERBOARD\": {\"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"Varying-view RGB-D Action-Skeleton\", \"Metric\": \"Accuracy (CV II)\", \"Score\": \"48%\"}}, {\"LEADERBOARD\": {\"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"Kinetics-Skeleton dataset\", \"Metric\": \"Accuracy\", \"Score\": \"20.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"Accuracy (CS)\", \"Score\": \"74.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"Accuracy (CV)\", \"Score\": \"83.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Multimodal Activity Recognition\", \"Dataset\": \"EV-Action\", \"Metric\": \"Accuracy\", \"Score\": \"80.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Multimodal Activity Recognition\", \"Dataset\": \"EV-Action\", \"Metric\": \"Accuracy\", \"Score\": \"64.1\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"Accuracy (CS)\", \"Score\": \"71.8\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"Accuracy (CV)\", \"Score\": \"84.9\"}} ]"
    },
    {
        "Context": "DP-LSTM: Differential Privacy-inspired LSTM for Stock Prediction Using Financial News Stock price prediction is important for value investments in the stock market. In particular, short-term prediction that exploits financial news articles is promising in recent years. In this paper, we propose a novel deep neural network DP-LSTM for stock price prediction, which incorporates the news articles as hidden information and integrates difference news sources through the differential privacy mechanism. First, based on the autoregressive moving average model (ARMA), a sentiment-ARMA is formulated by taking into consideration the information of financial news articles in the model. Then, an LSTM-based deep neural network is designed, which consists of three components: LSTM, VADER model and differential privacy (DP) mechanism. The proposed DP-LSTM scheme can reduce prediction errors and increase the robustness. Extensive experiments on S&P 500 stocks show that (i) the proposed DP-LSTM achieves 0.32% improvement in mean MPA of prediction result, and (ii) for the prediction of the market index S&P 500, we achieve up to 65.79% improvement in MSE. We calculate the mean prediction accuracy (MPA) to evaluate the proposed methods, which is defined as where X t, is the real stock price of the -th stock on the t-th day, L is the number of stocks andX t, is the corresponding prediction result Table 1: Predicted Mean MPA results. Mean MPA Table 2: S&P 500 predicted results. DP - LSTM LSTM without news LSTM with news",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Stock Price Prediction\", \"Dataset\": \"2019_test set\", \"Metric\": \"10 fold Cross validation\", \"Score\": \"22\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"SYelp Binary classification\", \"Metric\": \"Error\", \"Score\": \"4.16\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"SYelp Binary classification\", \"Metric\": \"Error\", \"Score\": \"11.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"SYelp Binary classification\", \"Metric\": \"Error\", \"Score\": \"41.87\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"SYelp Binary classification\", \"Metric\": \"Error\", \"Score\": \"71.47\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"SYelp Fine-grained classification\", \"Metric\": \"Error\", \"Score\": \"29.62\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"SYelp Fine-grained classification\", \"Metric\": \"Error\", \"Score\": \"29.42\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"SYelp Fine-grained classification\", "
    },
    {
        "Context": "Multi-Fiber Networks for Video Recognition In this paper, we aim to reduce the computational cost of spatio-temporal deep neural networks, making them run as fast as their 2D counterparts while preserving state-of-the-art accuracy on video recognition benchmarks. To this end, we present the novel Multi-Fiber architecture that slices a complex neural network into an ensemble of lightweight networks or fibers that run through the network. To facilitate information flow between fibers we further incorporate multiplexer modules and end up with an architecture that reduces the computational cost of 3D networks by an order of magnitude, while increasing recognition performance at the same time. Extensive experimental results show that our multi-fiber architecture significantly boosts the efficiency of existing convolution networks for both image and video recognition tasks, achieving state-of-the-art performance on UCF-101, HMDB-51 and Kinetics datasets. Our proposed model requires over 9\u00d7 and 13\u00d7 less computations than the I3D [1] and R(2+1)D [2] models, respectively, yet providing higher accuracy. We evaluate the proposed multi-fiber network on three benchmark datasets, Kinetics, UCF-101 and HMDB51, and compare the results with other state-of-the-art models Note, the complexity is evaluated with FLOPs, i.e Table 2. Multi-fiber Network architecture. The \"2D MF-Net\" takes images as input, while the \"3D MF-Net\" takes frames, i.e. video clips, as input. Note, the complexity is evaluated with FLOPs, i.e. floating-point multiplication-adds. The stride of \"3D MF- Net\" is denoted by \"(temporal stride, height stride, width stride)\", and the stride of \"2D MF-Net\" is denoted by \"(height stride, width stride)\". ( 1 , 2 , 2 ) 3D MF - Net ( 1 , 1 ) Stride ( 2 , 2 ) ( 1 , 2 , 2 ) 1000 ( 2 , 1 , 1 ) #Channel ( 1 , 1 , 1 ) 400 1 \u00d7 1 \u00d7 1 8 \u00d7 7 \u00d7 7 7 \u00d7 7 1 \u00d7 1 Output Size 2D MF - Net Table 3. Comparison on action recognition accuracy with state-of-the-arts on Kinetics. The complexity is measured using FLOPs, i.e. floating-point multiplication-adds. All results",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@1\", \"Score\": \"72.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@5\", \"Score\": \"90.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"UCF101\", \"Metric\": \"3-fold Accuracy\", \"Score\": \"96.0\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@1\", \"Score\": \"73.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@1\", \"Score\": \"71.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Recognition\", \"Dataset\": \"UCF101\", \"Metric\": \"3-fold Accuracy\", \"Score\": \"96.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Recognition\", \"Dataset\": \"HMDB-51\", \"Metric\": \"Average accuracy of 3 splits\", \"Score\": \"74.9\"}} ]"
    },
    {
        "Context": "Time-aware Large Kernel Convolutions To date, most state-of-the-art sequence modeling architectures use attention to build generative models for language based tasks. Some of these models use all the available sequence tokens to generate an attention distribution which results in time complexity of O(n 2 ). Alternatively, they utilize depthwise convolutions with softmax normalized kernels of size k acting as a limited-window self-attention, resulting in time complexity of O(k\u00b7n). In this paper, we introduce Time-aware Large Kernel (TaLK) Convolutions, a novel adaptive convolution operation that learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix. This method yields a time complexity of O(n), effectively making the sequence encoding process linear to the number of tokens. We evaluate the proposed method on large-scale standard machine translation, abstractive summarization and language modeling datasets and show that TaLK Convolutions constitute an efficient improvement over other attention/convolution based approaches. We evaluated our proposed encoding technique on machine translation, abstractive summarization and language mod- eling Machine Translation On the machine translation task, we report results on three mainstream benchmark datasets: WMT English to German (En-De), WMT English to French (En-Fr) and IWSLT German to English (De-En) For all datasets, we replicated the pre-processing steps mentioned in We validated on newstest2012+2013 and tested on newstest2014 evaluation datasets For all datasets, we measured case-sensitive tokenized BLEU scores using multi-bleu 1 For all datasets, we used beam search with beam width 5 Abstractive Summarization For the abstractive summarization task, we decided to experiment with the CNN-DailyMail dataset The dataset is composed by approximately 280K news articles with associated multi-sentence summaries Language Modeling We experimented on the WikiText-103 benchmark dataset Specifically, we follow for WMT En-De and WMT En-Fr datasets the model hidden size d was set to 1024, the feed-forward hidden size d ff Table 2. Machine translation accuracy in terms of BLEU for WMT En-De and WMT En-Fr on newstest2014. - WMT En - Fr WMT En - De Table 3. Machine translation accuracy in terms of BLEU on IWSLT De-En. - WMT En - Fr WMT En - De Table 4. Results on CNN-DailyMail abstractive summarization. Rouge - L Rouge - 1 Rouge - 2 Table 5. Test perplexity on WikiText-103. We used adaptive inputs similar to Baevski & Auli (2019) and show that our method yields better perplexity than dynamic convolutions and comparative per- formance with self-attention. Rouge - L Rouge - 1 Rouge - 2 Table 6. Throughput and memory consumption decrease measured for different sequence lengths (n) on a batch of size 10 with each token being represented with d = 1024 and H = 16. Throughput is calculated across 100K iterations of a single input encoding execution for each",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-French\", \"Metric\": \"BLEU score\", \"Score\": \"43.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"IWSLT2014 German-English\", \"Metric\": \"BLEU score\", \"Score\": \"35.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-German\", \"Metric\": \"BLEU score\", \"Score\": \"29.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-103\", \"Metric\": \"Number of params\", \"Score\": \"240M\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-103\", \"Metric\": \"Test perplexity\", \"Score\": \"23.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Document Summarization\", \"Dataset\": \"CNN / Daily Mail\", \"Metric\": \"ROUGE-1\", \"Score\": \"40.59\"}}, {\"LEADERBOARD\": {\"Task\": \"Document Summarization\", \"Dataset\": \"CNN / Daily Mail\", \"Metric\": \"ROUGE-2\", \"Score\": \"18.97\"}}, {\"LEADERBOARD\": {\"Task\": \"Document Summarization\", \"Dataset\": \"CNN / Daily Mail\", \"Metric\": \"ROUGE-L\", \"Score\": \"36.81\"}}, {\"LEADERBOARD\": {\"Task\": \"Document Summarization\", \"Dataset\": \"CNN / Daily Mail\", \"Metric\": \"ROUGE-1\", \"Score\": \"40.03\"}}, {\"LEADERBOARD\": {\"Task\": \"Document Summarization\", \"Dataset\": \"CNN / Daily Mail\", \"Metric\": \"ROUGE-2\", \"Score\": \"18.45\"}}, {\"LEADERBOARD\": {\"Task\": \"Document Summarization\", \"Dataset\": \"CNN / Daily Mail\", \"Metric\": \"ROUGE-L\", \"Score\": \"36.13\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-French\", \"Metric\": \"BLEU score\", \"Score\": \"43.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-French\", \"Metric\": \"BLEU score\", \"Score\": \"44.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"WMT2016 English-Romanian\", \"Metric\": \"BLEU score\", \"Score\": \"27.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-German\", \"Metric\": \"BLEU score\", \"Score\": \"21.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-German\", \"Metric\": \"BLEU score\", \"Score\": \"14.54\"}}, { \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"IWSLT2015 English-German\", \"Metric\": \"BLEU score\", \"Score\": \"37.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"IWSLT2015 German-English\", \"Metric\": \"BLEU score\", 'Score'"
    },
    {
        "Context": "A light-based device for solving the Hamiltonian path problem In this paper we suggest the use of light for performing useful computations. Namely, we propose a special device which uses light rays for solving the Hamiltonian path problem on a directed graph. The device has a graph-like representation and the light is traversing it following the routes given by the connections between nodes. In each node the rays are uniquely marked so that they can be easily identified. At the destination node we will search only for particular rays that have passed only once through each node. We show that the proposed device can solve small and medium instances of the problem in reasonable time.  Table 1. The labeling system generated by our backtracking procedure. First column contains the number of nodes of the graph. The second column represents the labels applied to nodes. 8 , 12 , 14 , 15 16 , 24 , 28 , 30 , 31 Labels ( delays ) n",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "ON THE EXPECTED MAXIMUM DEGREE OF GABRIEL AND YAO GRAPHS Motivated by applications of Gabriel graphs and Yao graphs in wireless ad-hoc networks, we show that the maximal degree of a random Gabriel graph or Yao graph defined on n points drawn uniformly at random from a unit square grows as \u0398(log n/ log log n) in probability.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "ProtTrans: Towards Cracking the Language of Life's Code Through Self-Supervised Learning Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans. Language model corpora Here, we show more details on the differences between the different corpora used for protein LM pre-training Such an analysis could allow fora cheap and fast analysis of single proteins without a) needing large labeled datasets for supervised training and b) being less influenced by the experimental bias in today's labeled databases which focus mostly on model organisms with applications to biotechnology 2 , 122 BFD UniRef100 Fp32 Master Weight 15% 4096 25% 420M 32768 / 6144 15360 / 2560 40K / 5K 44928 Summit 18 / 7 10K 20K Lamb 343K 224M 32 / 6 TPU Pod 2048 140K / 20K 40K / 40K 40K / 0K 11B 9216 / 3584 AdaFactor 409M 22464 - 920K 3B Adam 2 21 / 2 4 5 1024 8 10752 / 1024 30 / 5 None Table 9), the other because it is larger and less redundant (dubbed NEW364 introduced here). Standard errors were computed using bootstrapping: CASP12=\u00b11.6%, NEW364=\u00b10.5%. Highest values in each column marked in bold-face. 78 , 1 CASP12 73 , 9 NEW364 0 100 t Q10 : Localization Q2 : Membrane / other Table 8. TS115 CASP12 CB513 NEW364 TS115 CASP12 CB513 NEW364 SeqVec Time ProtAlbert ProtT5 - XL Mean 89 59 Max ProtT5 - XXL ProtTXL ProtBert / ProtElectra Concat Min",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Protein Secondary Structure Prediction\", \"Dataset\": \"TS115\", \"Metric\": \"Q3\", \"Score\": \"0.87\"}}, {\"LEADERBOARD\": {\"Task\": \"Protein Secondary Structure Prediction\", \"Dataset\": \"TS115\", \"Metric\": \"Q8\", \"Score\": \"0.77\"}}, {\"LEADERBOARD\": {\"Task\": \"Protein Secondary Structure Prediction\", \"Dataset\": \"TS115\", \"Metric\": \"Q3\", \"Score\": \"0.85\"}}, {\"LEADERBOARD\": {\"Task\": \"Protein Secondary Structure Prediction\", \"Dataset\": \"TS115\", \"Metric\": \"Q8\", \"Score\": \"0.74\"}}, {\"LEADERBOARD\": {\"Task\": \"Protein Secondary Structure Prediction\", \"Dataset\": \"TS115\", \"Metric\": \"Q3\", \"Score\": \"0.84\"}}, {\"LEADERBOARD\": {\"Task\": \"Protein Secondary Structure Prediction\", \"Dataset\": \"TS115\", \"Metric\": \"Q8\", \"Score\": \"0.73\"}}, {\"LEADERBOARD\": {\"Task\": \"Protein Secondary Structure Prediction\", \"Dataset\": \"CB513\", \"Metric\": \"Q3\", \"Score\": \"0.86\"}}, {\"LEADERBOARD\": {\"Task\": \"Protein Secondary Structure Prediction\", \"Dataset\": \"CB513\", \"Metric\": \"Q8\", \"Score\": \"0.74\"}}, {\"LEADERBOARD\": {\"Task\": \"Protein Secondary Structure Prediction\", \"Dataset\": \"CB513\", \"Metric\": \"Q3\", \"Score\": \"0.84\"}}, {\"LEADERBOARD\": {\"Task\": \"Protein Secondary Structure Prediction\", \"Dataset\": \"CB513\", \"Metric\": \"Q8\", \"Score\": \"0.71\"}}, {\"LEADERBOARD\": {\"Task\": \"Protein Secondary Structure Prediction\", \"Dataset\": \"CB513\", \"Metric\": \"Q3\", \"Score\": \"0.83\"}}, {\"LEADERBOARD\": {\"Task\": \"Protein Secondary Structure Prediction\", \"Dataset\": \"CB513\", \"Metric\": \"Q8\", \"Score\": \"0.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Protein Secondary Structure Prediction\", \"Dataset\": \"CASP12\", \"Metric\": \"Q3\", \"Score\": \"0.81\"}}, {\"LEADERBOARD\": {\"Task\": \"Protein Secondary Structure Prediction\", \"Dataset\": \"CASP12\", \"Metric\": \"Q8\", \"Score\": \"0.70\"}}, {\"LEADERBOARD\": {\"Task\": \"Protein Secondary Structure Prediction\", \"Dataset\": \"CASP12\", \"Metric\": \"Q3\", \"Score\": \"0.77\"}}, {\"LEADERBOARD\": {\"Task\": \"Protein Secondary Structure Prediction\", \"Dataset\": \"CASP12\", \"Metric\": \"Q8\", \"Score\": \"0.66\"}}, {\"LEADERBOARD\": {\"Task\": \"Protein Secondary Structure Prediction\", \"Dataset\": \"CASP12\", \"Metric\": \"Q3\", \"Score\": \"0.76\"}}, {\"LEADERBOARD\": {\"Task\": \"Protein Secondary Structure Prediction\", \"Dataset\": \"CASP12\", \"Metric\": \"Q8\", \"Score\": \"0.65\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"78.0%\"}}, { \"LEADERBOARD\": { \"Task\": \"Drug Discovery\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"65.3%\"}}, { \"LEADERBOARD\": { \"Task\": \"Drug Discovery\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"62.5%\"}}, { \"LEADERBOARD\": { \"Task\": \"Drug Discovery\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"38.14\"}}, { \"LEADERBOARD\": { \"Task\": \"Drug Discovery\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"73.21\"}}, { \"LEADERBOARD\": { \"Task\": \"Drug Discovery\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"75.9%\"}}, { \"LEADERBOARD\": { \"Task\": \"Drug Discovery\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"75.3%\"}}, { \"LEADERBOARD\": { "
    },
    {
        "Context": "Unifying Graph Convolutional Neural Networks and Label Propagation Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. However, while conceptually similar, theoretical relation between LPA and GCN has not yet been investigated. Here we study the relationship between LPA and GCN in terms of two aspects: (1) feature/label smoothing where we analyze how the feature/label of one node is spread over its neighbors; And, (2) feature/label influence of how much the initial feature/label of one node influences the final feature/label of another node. Based on our theoretical analysis, we propose an end-to-end model that unifies GCN and LPA for node classification. In our unified model, edge weights are learnable, and the LPA serves as regularization to assist the GCN in learning proper edge weights that lead to improved classification performance. Our model can also be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models. Ina number of experiments on real-world graphs, our model shows superiority over state-of-the-art GCN-based methods in terms of node classification accuracy. We evaluate our model and present its performance on five datasets including citation networks and coauthor networks We use the following five datasets in our experiments: datasets: Cora, Citeseer, and Pubmed In these datasets, nodes correspond to documents, edges correspond to citation links, and each node has a sparse bag-of-words feature vector as well as a class label Statistics of the five datasets are shown in Table 1: Dataset statistics after removing self-loops and duplicate edges. 19 , 717 8 , 415 81 , 894 2 , 708 1 , 433 Coauthor - CS Cora Citeseer 3 , 327 15 4 , 552 18 , 333 3 , 703 Pubmed 34 , 493 3 5 , 278 500 5 6 44 , 324 Coauthor - Phy 7 6 , 805 247 , 962 Table 2: Mean and the 95% confidence intervals of test set accuracy for all methods and datasets. Citeseer Coauthor - CS Coauthor - Phy Cora Pubmed Table 3: Result of GCN-LPA on Citeseer dataset with differet ratio of labeled nodes in LPA. 100% 0% 20% 40% 60% 80% Table 4: Hyper-parameter settings for all datasets. Citeseer 16 Coauthor - CS Coauthor - Phy Cora Pubmed 32",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"Citeseer\", \"Metric\": \"Accuracy\", \"Score\": \"78.7%\"}}, {\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"Pubmed\", \"Metric\": \"Accuracy\", \"Score\": \"87.8%\"}}, {\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"Cora\", \"Metric\": \"Accuracy\", \"Score\": \"88.5%\"}}, {\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"Coauthor Phy\", \"Metric\": \"Accuracy\", \"Score\": \"96.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"Coauthor CS\", \"Metric\": \"Accuracy\", \"Score\": \"94.8%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Citeseer\", \"Metric\": \"Accuracy\", \"Score\": \"71.8%\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Pubmed\", \"Metric\": \"Accuracy\", \"Score\": \"76.80%\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Cora\", \"Metric\": \"Accuracy\", \"Score\": \"84.8%\"}} ]"
    },
    {
        "Context": "Proposition of a full deterministic medium access method for wireless network in a robotic application Today, many network applications require shorter react time. Robotic field is an excellent example of these needs: robot react time has a direct effect on its task's complexity. Here, we propose a full deterministic medium access method fora wireless robotic application. This contribution is based on some low-power wireless personal area networks, like ZigBee standard. Indeed, ZigBee has identified limits with Quality of Service due to non-determinist medium access and probable collisions during medium reservation requests. In this paper, two major improvements are proposed: an efficient polling of the star nodes and a temporal deterministic distribution of peer-to-peer messages. This new MAC protocol with no collision offers some QoS faculties.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Effect of different substrates on Compact stacked square Microstrip Antenna Selection of the most suitable substrate fora Microstrip antenna is a matter of prime importance. This is because many limitations of the microstrip antenna such as high return loss, low gain and low efficiency can be overcome by selecting an appropriate substrate for fabrication of the antenna, without shifting the resonant frequency significantly. The substate properties such as its dielectric constant, loss tangent have a pronounced effect on the antenna characteristics. Some of the critical properties that are to betaken care of while selecting a dielectric are homogeneity, moisture absorption and adhesion of metal-foil cladding. In this paper a comprehensive study of the effect of variation of substrate material on the antenna properties has been presented.  buten 4350 SIMULATION RESULTS TABLE 1 Return Loss ( S11 ) Loss ( dBi ) Tangent Resonant ( \u03b5r ) frequency Direc - ( % ) ( GHz ) Efficiency Electrical tivity Gain Antenna Table 1. buten 4350 SIMULATION RESULTS TABLE 1 Return Loss ( S11 ) Loss ( dBi ) Tangent Resonant ( \u03b5r ) frequency Direc - ( % ) ( GHz ) Efficiency Electrical tivity Gain Antenna",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Dependent Randomized Rounding for Matroid Polytopes and Applications Motivated by several applications, we consider the problem of randomly rounding a fractional solution in a matroid (base) polytope to an integral one. We consider the pipage rounding technique [5,6,36] and also present anew technique, randomized swap rounding. Our main technical results are concentration bounds for functions of random variables arising from these rounding techniques. We prove Chernofftype concentration bounds for linear functions of random variables arising from both techniques, and also a lower-tail exponential bound for monotone submodular functions of variables arising from randomized swap rounding.The following are examples of our applications.\u2022 We give a (1 \u2212 1/e \u2212 \u03b5)-approximation algorithm for the problem of maximizing a monotone submodular function subject to 1 matroid and k linear constraints, for any constant k \u2265 1 and \u03b5 > 0. We also give the same result fora super-constant number k of \"loose\" linear constraints, where the right-hand side dominates the matrix entries by an \u2126(\u03b5 \u22122 log k) factor.\u2022 We present a result on minimax packing problems that involve a matroid base constraint. We give an O(log m/ log log m)-approximation for the general problem min{\u03bb : \u2203x \u2208 {0, 1} N , x \u2208 B(M), Ax \u2264 \u03bbb} where m is the number of packing constraints. Examples include the low-congestion multi-path routing problem [34] and spanning-tree problems with capacity constraints on cuts [4,16].\u2022 We generalize the continuous greedy algorithm [35,6] to problems involving multiple submodular functions, and use it to find a (1 \u2212 1/e \u2212 \u03b5)-approximate pareto set for the problem of maximizing a constant number of monotone submodular functions subject to a matroid constraint. An example is the Submodular Welfare Problem where we are looking for an approximate pareto set with respect to individual players' utilities.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "\u00c3 \u00dd \u00db\u00d3\u00d6 \u00d7 \u00d0 \u00d2 \u00d3\u00d2\u00da\u00d3\u00d0\u00d9\u00d8 \u00d3\u00d2\u00b8 \u00d6\u00d2\u00d3\u00d9\u00d0\u00d0 \u00b9 \u00d9\u00d7\u00d7 \u00d2 \u00d1\u00d3 \u00d0\u00b8\u00c5 \u00d6 \u00d3\u00da \u00d2 \u00c5\u00d3\u00d2\u00d8 \u00d6\u00d0\u00d3 \u00d1 \u00d8 \u00d3 \u00d7 * \u00d3\u00d6\u00d6 \u00d7\u00d4\u00d3\u00d2 \u00d2 \u00d9\u00d8 \u00d3\u00d6\u00ba \u00cc \u00d0\u00ba \u00b7\u00bf\u00bf \u00be \u00bc \u00bf \u00be \u00dc \u00b7\u00bf\u00bf \u00be \u00bc \u00bf \u00bf\u00bc \u00d1 \u00d0 \u00d6 \u00d7\u00d7 \u00d7 \u00ba \u00d6\u00dd\u00d2\u00ba \u00b9\u00d2 \u00d2\u00d8 \u00d7\u00ba \u00d6\u00b4 \u00ba \u00b5\u00b8\u00c2 \u00d6\u00d3\u00d1 \u00ba\u00c1 \u00d6 \u00d6\u00dd\u00d2\u00ba \u00b9\u00d2 \u00d2\u00d8 \u00d7\u00ba \u00d6\u00b4\u00c2\u00ba \u00c1 \u00d6\u00b5\u00b8 \u00d6 \u00ba\u00c4 \u00b9 \u00d6\u00d4 \u00d2\u00d8 \u00d6 \u00d6\u00dd\u00d2\u00ba \u00b9\u00d2 \u00d2\u00d8 \u00d7\u00ba \u00d6\u00b4 \u00ba \u00c4 \u00d6\u00d4 \u00d2\u00d8 \u00d6\u00b5\u00ba \u00c8\u00d6 \u00d4\u00d6 \u00d2\u00d8 \u00d7\u00d9 \u00d1 \u00d8\u00d8 \u00d8\u00d3 \u00d0\u00d7 \u00da \u00d6 \u00be\u00bc AE\u00d3\u00da \u00d1 \u00d6 \u00be\u00bc\u00bd \u00bd\u00ba \u00c1\u00d2\u00d8\u00d6\u00d3 \u00d9\u00d8 \u00d3\u00d2",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and to integrate it with data about agents' possible future objectives. Our proposal is general enough to be applied to different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Trajectory Prediction\", \"Dataset\": \"Stanford Drone\", \"Metric\": \"ADE (in world coordinates)\", \"Score\": \"0.54\"}}, {\"LEADERBOARD\": {\"Task\": \"Trajectory Prediction\", \"Dataset\": \"Stanford Drone\", \"Metric\": \"FDE (in world coordinates)\", \"Score\": \"1.05\"}}, {\"LEADERBOARD\": {\"Task\": \"Trajectory Prediction\", \"Dataset\": \"STATS SportVu NBA [ATK]\", \"Metric\": \"ADE\", \"Score\": \"9.18\"}}, {\"LEADERBOARD\": {\"Task\": \"Trajectory Prediction\", \"Dataset\": \"STATS SportVu NBA [ATK]\", \"Metric\": \"FDE\", \"Score\": \"13.54\"}}, {\"LEADERBOARD\": {\"Task\": \"Trajectory Prediction\", \"Dataset\": \"STATS SportVu NBA [DEF]\", \"Metric\": \"ADE\", \"Score\": \"7.01\"}}, {\"LEADERBOARD\": {\"Task\": \"Trajectory Prediction\", \"Dataset\": \"STATS SportVu NBA [DEF]\", \"Metric\": \"FDE\", \"Score\": \"9.76\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Trajectory Prediction\", \"Dataset\": \"Drug-auto-auto-text\", \"Metric\": \"E\", \"Score\": \"0.67\"}}, { \"LEADERBOARD\": { \"Task\": \"Trajectory Prediction\", \"Dataset\": \"Drug-to-text\", \"Metric\": \"E\", \"Score\": \"0.54\"}} ]"
    },
    {
        "Context": "Low Power Oriented CMOS Circuit Optimization Protocol Low power oriented circuit optimization consists in selecting the best alternative between gate sizing, buffer insertion and logic structure transformation, for satisfying a delay constraint at minimum area cost. In this paper we used a closed form model of delay in CMOS structures to define metrics fora deterministic selection of the optimization alternative. The target is delay constraint satisfaction with minimum area cost. We validate the design space exploration method, defining maximum and minimum delay bounds on logical paths. Then we adapt this method to a \"constant sensitivity method\" allowing to size a circuit at minimum area under a delay constraint. An optimisation protocol is finally defined to manage the trade-off performance constraintcircuit structure. These methods are implemented in an optimization tool (POPS) and validated by comparing on a 0.25\u00b5m process, the optimization efficiency obtained on various benchmarks (ISCAS'85) to that resulting from an industrial tool.  Table 1. CPU time comparison in satisfying path delay constraint. 6120 9850 15890 9950 19400 9050 11760 different benchmarks . 11400 POPS ( ms ) W ( \u00b5m ) in methods POPS 2000 c1355 210 Comparison 159 116 of Gate nb c1908 under on",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "A BRANCHING AND MERGING CONVOLUTIONAL NETWORK WITH HOMOGENEOUS FILTER CAPSULES We present a convolutional neural network design that, rather than using a single stem of convolutional layers, uses multiple stems, each having branched off of a prior stem. This leads to a different effective receptive field and level of abstraction for each branch moving onto a unique classification stage. The classification stages are then merged together, each effectively voting their level of confidence in the classification. A further novelty to our design is that we do not use any fully connected layers at all, but rather each of the final filters in each branch is transformed into a pair of homogeneous vector capsules. As the capsules are formed from entire filters, we refer to them as filter capsules. This design, in combination with a domain-specific set of randomly applied augmentation techniques, establishes anew state of the art for the MNIST dataset with an accuracy of 99.84% for an ensemble of these models, as well as establishing anew state of the art fora single model (99.79% accurate). These accuracies were achieved with a 75% reduction in both the number of parameters and the number of epochs of training relative to the previously best performing capsule network on MNIST. All training was performed using the Adam optimizer and experienced no overfitting. Additionally, since CIFAR-10 and CIFAR-100 images are 32 \u00d7 32 pixels, are full color, and are comprised of more complex features, we ran an additional pair of experiments for each of these datasets that used additional convolutions For all four datasets, a model that included the branching and merging and HFCs achieved the highest mean accuracy with statistical significance Given that both of these datasets are monochromatic images with a size of 28 \u00d7 28 and our network was designed with those properties in mind, this is not especially surprising The fact that the accuracy for Fashion-MNIST was not competitive with current state of the art for that dataset is also not surprising as our network design was optimized for accuracy on classification of the Arabic numerals in the MNIST dataset Table 2. Mean SD Table 2 : Individual Models Min Max Table 2: Individual Models Experiment Min Max Mean SD Mean SD Table 2 : Individual Models Min Max Table 3. Shown here are the number of ensembles that were generated that either 0 1 Table 3 : Ensembles Table 3: Ensembles Accuracy: 99.84% 99.83% 99.82% Shown here are the number of ensembles that were generated that either 0 1 Table 3 : Ensembles Table 4: Current and Previous MNIST State of the Art Results Paper Year Accuracy architectures [ 24 ] Single Models Ensembles Table 4 : Current and Previous MNIST State of the Art Results Accuracy Table 5. architectures [ 24 ] Single Models Ensembles Table 4 : Current and Previous MNIST State of the Art Results Accuracy Table 7: Experiments With and Without Branching and Merging and Homogeneous Filter Capsules Accuracy Min Max Mean SD MNIST Fashion -",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"MNIST\", \"Metric\": \"Accuracy\", \"Score\": \"99.84\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"MNIST\", \"Metric\": \"Percentage error\", \"Score\": \"0.16\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"MNIST\", \"Metric\": \"Trainable Parameters\", \"Score\": \"1,514,187\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"Fashion-MNIST\", \"Metric\": \"Accuracy\", \"Score\": \"99.13\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"MNIST\", \"Metric\": \"Percentage error\", \"Score\": \"0.17\"}}, { \"LEADERBOARD\": { \"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"Comp3D\", \"Metric\": \"Accuracy\", \"Score\": \"95.94\"}} ]"
    },
    {
        "Context": "Multiple Instance-Based Video Anomaly Detection using Deep Temporal Encoding-Decoding In this paper, we propose a weakly supervised deep temporal encoding-decoding solution for anomaly detection in surveillance videos using multiple instance learning. The proposed approach uses both abnormal and normal video clips during the training phase which is developed in the multiple instance framework where we treat the video as a bag and video clips as instances in the bag. Our main contribution lies in the proposed novel approach to consider temporal relations between video instances. We deal with video instances (clips) as sequential visual data rather than a set of independent instances. We employ a deep temporal encoding-decoding network that is designed to capture spatio-temporal evolution of video instances overtime. We also propose anew loss function that maximizes the mean distance between normal and abnormal instance predictions. The new loss function ensures a low false alarm rate which is very crucial in practical surveillance application.The proposed temporal encoding-decoding approach with modified loss is benchmarked against the state of the art in simulation studies. The results show that the proposed method performs similar to or better than the state-of-the-art solutions for anomaly detection in video surveillance applications and achieve state of the art false alarm rate on UCF-crime dataset. , where he has worked, since 2010, and is currently a Professor, and a Research Development Lead, as well as the Discipline Leader (Manufacturing and Mechatronics) with the School of Engineering. His main research interests include statistical information fusion, random Finite sets, multi-object tracking, deep learning, and robust multi-structure data ftting in computer vision. In this section, we test our proposed temporal encodingdecoding network with the proposed loss function using two public datasets which are UCF-cirme dataset and Shang-haiTech In this paper, we have conducted experiment on two public datasets which are the UCF-crime dataset and the Shang-haiTech 1) UCF-Crime: is a large scale dataset of long videos with different scenes that represent real-life situations The dataset consists of 1900 videos divided into training sets and testing sets The total dataset duration is 128 hours In this dataset, no temporal (frame-level) annotation is available except for the testing videos The UCF-crime dataset is the biggest video anomaly dataset and the only one that has multiple scenes with real surveillance videos 2) ShanghaiTech: is a medium-scale dataset that contains 437 different videos captured at a university campus ShanghaiTech dataset is commonly used for unsupervised anomaly detection, thus, there is no abnormal videos for training To accommodate",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Anomaly Detection In Surveillance Videos\", \"Dataset\": \"UCF-Crime\", \"Metric\": \"Decidability\", \"Score\": \"-\"}}, {\"LEADERBOARD\": {\"Task\": \"Anomaly Detection In Surveillance Videos\", \"Dataset\": \"UCF-Crime\", \"Metric\": \"EER\", \"Score\": \"-\"}}, {\"LEADERBOARD\": {\"Task\": \"Anomaly Detection In Surveillance Videos\", \"Dataset\": \"UCF-Crime\", \"Metric\": \"ROC AUC\", \"Score\": \"80.10\"}}, {\"LEADERBOARD\": {\"Task\": \"Anomaly Detection In Surveillance Videos\", \"Dataset\": \"ShanghaiTech Weakly Supervised\", \"Metric\": \"AUC-ROC\", \"Score\": \"89.14\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Anomaly Detection In Surveillance Videos\", \"Dataset\": \"UCF-Crime\", \"Metric\": \"Decidability\", \"Score\": \"87.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Anomaly Detection In Surveillance Videos\", \"Dataset\": \"UCF-Crime\", \"Metric\": \"EER\", \"Score\": \"11. Sur\"}}, { \"LEADERBOARD\": { \"Task\": \"Anomaly Detection In Surveillance Videos\", \"Dataset\": \"UCF-Crime\", \"Metric\": \"ROC AUC\", \"Score\": \"75.41\"}}, { \"LEADERBOARD\": { \"Task\": \"Anomaly Detection In Surveillance Videos\", \"Dataset\": \"UCF-Crime\", \"Metric\": \"Decidability\", \"Score\": \"0.861\"}}, { \"LEADERBOARD\": { \"Task\": \"Anomaly Detection In Surveillance Videos\", \"Dataset\": \"UCF-Crime\", \"Metric\": \"EER\", \"Score\": \"0.087\"}} ]"
    },
    {
        "Context": "From source to target and back: Symmetric Bi-Directional Adaptive GAN The effectiveness of GANs in producing images according to a specific visual domain has shown potential in unsupervised domain adaptation. Source labeled images have been modified to mimic target samples for training classifiers in the target domain, and inverse mappings from the target to the source domain have also been evaluated, without new image generation.In this paper we aim at getting the best of both worlds by introducing asymmetric mapping among domains. We jointly optimize bi-directional image transformations combining them with target self-labeling. We define anew class consistency loss that aligns the generators in the two directions, imposing to preserve the class identity of an image passing through both domain mappings. A detailed analysis of the reconstructed images, a thorough ablation study and extensive experiments on six different settings confirm the power of our approach. We evaluate SBADA-GAN on several unsupervised adaptation scenarios 1 , considering the following widely used digits datasets and settings: MNIST \u2192 MNIST-M: MNIST contains centered, 28 \u00d7 28 pixel, grayscale images of single digit numbers on a black background, while MNIST-M is a variant where the background is substituted by a randomly extracted patch obtained from color photos of BSDS500 We follow the evaluation protocol of MNIST \u2194 USPS: USPS [9] is a digit dataset automatically scanned from envelopes by the U.S We follow the evaluation protocol of SVHN \u2194 MNIST: SVHN is the challenging realworld Street View House Number dataset, much larger in scale than the other considered datasets Besides presenting a great variety of styles (in shape and texture), images from this dataset often contain extraneous numbers in addition to the labeled, centered one Table 1: Comparison against previous work. SBADA-GAN C t reports the accuracies produced by the classifier trained in the target domain space. Similarly, SBADA-GAN C s reports the results produced by the classifier trained in the source domain space and tested on the target images mapped to this space. SBADA-GAN reports the results obtained by a weighted combination of the softmax outputs of these two classifiers. Note that all competitors convert SVHN to grayscale, while we deal with the more complex original RGB version. The last three rows report results from online available pre-print papers. - Synth Signs\u2192GTSRB not conv . MNIST\u2192SVHN MNIST\u2192MNIST - M SVHN\u2192MNIST MNIST\u2192 USPS - USPS\u2192MNIST Table 2: Dataset mean SSIM: this measure of data vari- ability suggests that our method successfully generates images with not only the same style of a chosen domain, but also similar perceptual variability. S map to T S T T map",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Domain Adaptation\", \"Dataset\": \"SVHN-to-MNIST\", \"Metric\": \"Accuracy\", \"Score\": \"76.1\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Domain Adaptation\", \"Dataset\": \"SYNSIG-to-GTSRB\", \"Metric\": \"Accuracy\", \"Score\": \"76.3\"}} ]"
    },
    {
        "Context": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagE Representation (KEPLER), which cannot only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KE-PLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pretraining and evaluating KEPLER, we construct Wikidata5M 1 , a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as anew KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from https://github.com/ THU-KEG/KEPLER. Recent pre-trained language representation models (PLMs) such as BERT (Devlin et al., 2019) and * Correspondence to: Z. Liu and J. Tang 1 https://deepgraphlearning.github. io/project/wikidata5m As we cannot afford the full RoBERTa corpora (126 GB, and we only use 13 GB) in KEPLER pre-training, we implement Our RoBERTa for direct comparisons to KEPLER It is initialized by RoBERTa BASE and is further trained with the MLM objective on the same corpora as KEPLER We also evaluate recent knowledge-enhanced PLMs, including ERNIE BERT and KnowBert BERT The evaluation method is described in Section 3.3 The \"m/mm\" stands for matched/mismatched evaluation sets for MNLI Hence we also evaluate the models on LAMA-UHN, which filters out the questionable templates from the Google-RE and T-REx corpora of LAMA The evaluation results are shown in, from which we have the following observations: (1) KEPLER consistently outperforms the vanilla PLM baseline Our RoBERTa in almost all the settings except ConceptNet, which focuses on commonsense knowledge rather than factual knowledge Table 1: Statistics of Wikidata5M (transductive setting) compared with existing KE benchmarks. #relation #entity Percentage #training #test #validation Occurrence Table 2: Top-5 entity categories in Wikidata5M. Percentage Occurrence Table 3: Statistics of Wikidata5M inductive setting. #relation #entity #triplet Table 4: Performances of different KE models on Wikidata5M (% except MR). HITS@10 HITS@1 HITS@3 MRR Table 5: Precision, recall and F-1 on TACRED (%). KnowBert results are different from the original paper since different task settings are used. RoBERTa BERT P R F - 1 Table 6: Accuracies (%) on the FewRel dataset. N -K indicates the N -way K-shot setting. MTB uses the LARGE size and all the other models use the BASE size. \u2020 indicates oracle models which may have seen facts in the FewRel 1.0 test set during pre-training. \u2212 Table 7: Entity typing results on OpenEntity (%). RoBERTa BERT P R F - 1 Table 8: GLUE results",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Relation Extraction\", \"Dataset\": \"TACRED\", \"Metric\": \"F1\", \"Score\": \"71.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Inductive knowledge graph completion\", \"Dataset\": \"Wikidata5m-ind\", \"Metric\": \"Hits@1\", \"Score\": \"0.222\"}}, {\"LEADERBOARD\": {\"Task\": \"Inductive knowledge graph completion\", \"Dataset\": \"Wikidata5m-ind\", \"Metric\": \"Hits@10\", \"Score\": \"0.73\"}}, {\"LEADERBOARD\": {\"Task\": \"Inductive knowledge graph completion\", \"Dataset\": \"Wikidata5m-ind\", \"Metric\": \"Hits@3\", \"Score\": \"0.514\"}}, {\"LEADERBOARD\": {\"Task\": \"Inductive knowledge graph completion\", \"Dataset\": \"Wikidata5m-ind\", \"Metric\": \"MRR\", \"Score\": \"0.402\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"Wikidata5M\", \"Metric\": \"Hits@1\", \"Score\": \"0.6954\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"Wikidata5M\", \"Metric\": \"Hits@10\", \"Score\": \"0.895\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"Wikidata5M\", \"Metric\": \"Hits@3\", \"Score\": \"0.6954\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"Wikidata5M\", \"Metric\": \"MRR\", \"Score\": \"0.5893\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"Wikidata5M\", \"Metric\": \"Hits@1\", \"Score\": \"0.412\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"Wikidata5M\", \"Metric\": \"Hits@10\", \"Score\": \"0.8883\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"Wikidata5M\", \"Metric\": \"Hits@3\", \"Score\": \"0.6908\"}}, { '"
    },
    {
        "Context": "A Balanced and Uncertainty-aware Approach for Partial Domain Adaptation 0000\u22120003\u22123890\u22121894] , Yunbo Wang 2[0000\u22120002\u22126215\u22128888] , Dapeng Hu 1 , Ran He 3[0000\u22120002\u22123807\u2212991X] , and Jiashi Feng 1[0000\u22120001\u22126843\u22120064]Abstract. This work addresses the unsupervised domain adaptation problem, especially in the case of class labels in the target domain being only a subset of those in the source domain. Such a partial transfer setting is realistic but challenging and existing methods always suffer from two key problems, negative transfer and uncertainty propagation. In this paper, we build on domain adversarial learning and propose a novel domain adaptation method BA 3 US with two new techniques termed Balanced Adversarial Alignment (BAA) and Adaptive Uncertainty Suppression (AUS), respectively. On one hand, negative transfer results in misclassification of target samples to the classes only present in the source domain. To address this issue, BAA pursues the balance between label distributions across domains in a fairly simple manner. Specifically, it randomly leverages a few source samples to augment the smaller target domain during domain alignment so that classes in different domains are symmetric. On the other hand, a source sample would be denoted as uncertain if there is an incorrect class that has a relatively high prediction score, and such uncertainty easily propagates to unlabeled target data around it during alignment, which severely deteriorates adaptation performance. Thus we present AUS that emphasizes uncertain samples and exploits an adaptive weighted complement entropy objective to encourage incorrect classes to have uniform and low prediction scores. Experimental results on multiple benchmarks demonstrate our BA 3 US surpasses state-of-the-arts for partial domain adaptation tasks. Code is available at https://github.com/tim-learn/BA3US.  Table 1. Accuracy (%) on Office-Home dataset for partial domain adaptation via ResNet-50 E - DANN Ours ( w / BAA ) SAFN [ 46 ] Table 2. Accuracy (%) on Office31 and ImageNet-Caltech for partial domain adap- tation via ResNet-50 - Table 3. Accuracy (%) on Office31 for partial domain adaptation via VGG-16 Avg . Table 4. Accuracy (%) on Office-Home dataset for vanilla unsupervised domain adaptation via ResNet-50 3 Table 5. Sensitivity of parameter \u03be. weights . Yellow bins denote ground - truth classes . Best viewed in color . 0 ( a ) Accuracy of Ar\u2192Cl weight number of classes in the target domain ( b ) estimated weight of Ar\u2192Cl class index Table 6. Sensitivity of parameter \u03b2. weights . Yellow bins denote ground - truth classes . Best viewed in color . 0 weight ( c ) estimated weight of A\u2192D ( b ) estimated weight",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Partial Domain Adaptation\", \"Dataset\": \"Office-31\", \"Metric\": \"Accuracy (%)\", \"Score\": \"97.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Partial Domain Adaptation\", \"Dataset\": \"Office-Home\", \"Metric\": \"Accuracy (%)\", \"Score\": \"76.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Partial Domain Adaptation\", \"Dataset\": \"ImageNet-Caltech\", \"Metric\": \"Accuracy (%)\", \"Score\": \"83.7\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Partial Domain Adaptation\", \"Dataset\": \"Office-31\", \"Metric\": \"Accuracy (%)\", \"Score\": \"86.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Partial Domain Adaptation\", \"Dataset\": \"Office-Home\", \"Metric\": \"Accuracy (%)\", \"Score\": \"70.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Partial Domain Adaptation\", \"Dataset\": \"ImageNet-Caltech\", \"Metric\": \"Accuracy (%)\", \"Score\": \"87.0\"}} ]"
    },
    {
        "Context": "Polynomial kernels for 3-leaf power graph modification problems * A graph G = (V, E) is a 3-leaf power iff there exists a tree T whose leaves are V and such that (u, v) \u2208 E iff u and v are at distance at most 3 in T . The 3-leaf power graph edge modification problems, i.e. edition (also known as the closest 3-leaf power), completion and edge-deletion, are FTP when parameterized by the size of the edge set modification. However polynomial kernel was known for none of these three problems. For each of them, we provide cubic kernels that can be computed in linear time for each of these problems. We thereby answer an open problem first mentioned by Dom, Guo, H\u00fcffner and Niedermeier [6].",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Escaping the Big Data Paradigm with Compact Transformers With the rise of Transformers as the standard for language processing, and their advancements in computer vision, along with their unprecedented size and amounts of training data, many have come to believe that they are not suitable for small sets of data. This trend leads to great concerns, including but not limited to: limited availability of data in certain scientific domains and the exclusion of those with limited resource from research in the field. In this paper, we dispel the myth that transformers are \"data hungry\" and therefore can only be applied to large sets of data. We show for the first time that with the rightsize and tokenization, transformers can perform head-to-head with state-of-the-art CNNs on small datasets. Our model eliminates the requirement for class token and positional embeddings through a novel sequence pooling strategy and the use of convolutions. We show that compared to CNNs, our compact transformers have fewer parameters and MACs, while obtaining similar accuracies. Our method is flexible in terms of model size, and can have as little as 0.28 M parameters and achieve reasonable results. It can reach an accuracy of 94.72% when training from scratch on CIFAR-10, which is comparable with modern CNN based approaches, and a significant improvement over previous Transformer based models. Our simple and compact design democratizes transformers by making them accessible to those equipped with basic computing resources and/or dealing with important small datasets. We conducted image classification experiments using our method on the following datasets: CIFAR-10, CIFAR-100, MNIST, and Fashion-MNIST These four datasets not only have a small number of training samples, but they are also small in resolution In this experiment we reduce the size of CIFAR-10 to determine the relationship between our model's performance and the number of samples within a dataset This experiment disassociates the dimensionality of the data from the size of the dataset, measuring which metric is the \"data hungry\" aspect of transformers This allows us to conclude that our model is not data hungry and can still effectively learn on very small datasets, obtaining over 84% accuracy with a dataset that has only 10k samples and a total of 10 classes Table 1. MobileNetV2 CCT ViT - Lite ResNet18 ResNet1001 ResNet164 Table 1: Top-1 validation accuracy comparisons. The numbers reported are best out of 4 runs. Hyperparamters are mentioned in Appendix A. Variants with \u2020 used a batch size of 64 instead of the default 128. Compact Convolutional Transformers Compact Vision Transformers Convolutional Networks ( Designed for CIFAR ) Vision Transformers Convolutional Networks ( Designed for ImageNet ) # Params MACs MNIST CIFAR - 10 Fashion - MNIST CIFAR - 100 Table 2: Top-1 validation accuracy of CIFAR-10 and CIFAR-100 when transforming ViT into CCT step by step. The numbers reported are best out of 4 runs. Hyperparamters are mentioned in Appendix A. # Params MACs CIFAR - 10 CIFAR - 100 Table 5: ViT and CCT hyperparamters. 0 Tuned Not Tuned Table 6: Difference between tuned and not tuned runs. 0 Tuned Not Tuned",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-100\", \"Metric\": \"Percentage correct\", \"Score\": \"76.67\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Percentage correct\", \"Score\": \"94.72\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of params\", \"Score\": \"73.8M\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"70.91%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-100\", \"Metric\": \"Percentage correct\", \"Score\": \"74.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Percentage correct\", \"Score\": \"93.7\"}} ]"
    },
    {
        "Context": "Matryoshka Networks: Predicting 3D Geometry via Nested Shape Layers . Exemplary shape reconstructions from a single image by our Matryoshka network based on nested shape layers.In this paper, we develop novel, efficient 2D encodings for 3D geometry, which enable reconstructing full 3D shapes from a single image at high resolution. The key idea is to pose 3D shape reconstruction as a 2D prediction problem. To that end, we first develop a simple baseline network that predicts entire voxel tubes at each pixel of a reference view. By leveraging well-proven architectures for 2D pixelprediction tasks, we attain state-of-the-art results, clearly outperforming purely voxel-based approaches. We scale this baseline to higher resolutions by proposing a memoryefficient shape encoding, which recursively decomposes a 3D shape into nested shape layers, similar to the pieces of a Matryoshka doll. This allows reconstructing highly detailed shapes with complex topology, as demonstrated in extensive experiments; we clearly outperform previous octreebased approaches despite having a much simpler architecture using standard network components. Our Matryoshka networks further enable reconstructing shapes from IDs or shape similarity, as well as shape sampling.  Table 2. Single image 3D shape reconstruction on ShapeNet-core at 32 3 resolution. We report the mean IoU (%) per category, and the average IoU over all categories. Our networks outperform all voxel decoder baselines and are competitive with the more complex PSGN. Figure 5 . Shapes reconstructed from a single image by our Matryoshka network at different resolutions . all bench chair couch watercraft lamp monitor Ground truth 3 256 car airplane 128 speaker firearm cellphone cabinet 64 table Table 3. Single image 3D shape reconstruction for high resolu- tions. We report IoU (in %) between predictions at several res- olutions and ground truth shapes at 256 3 . Predictions at lower resolution are up-sampled to 256 3 . 3 256 128 64 32 Table 4. Evaluation of base architectures. Across all categories, the ResNet-inspired architecture outperforms all other networks with a significant margin. with a significant margin . car",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"3D Object Reconstruction\", \"Dataset\": \"Data3D\u2212R2N2\", \"Metric\": \"3DIoU\", \"Score\": \"0.640\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Object Reconstruction\", \"Dataset\": \"Data3DR2N2\", \"Metric\": \"3DIoU\", \"Score\": \"0.57\"}} ]"
    },
    {
        "Context": "Edge-Informed Single Image Super-Resolution The recent increase in the extensive use of digital imaging technologies has brought with it a simultaneous demand for higher-resolution images. We develop a novel \"edge-informed\" approach to single image superresolution (SISR). The SISR problem is reformulated as an image inpainting task. We use a two-stage inpainting model as a baseline for super-resolution and show its effectiveness for different scale factors (\u00d72, \u00d74, \u00d78) compared to basic interpolation schemes. This model is trained using a joint optimization of image contents (texture and color) and structures (edges). Quantitative and qualitative comparisons are included and the proposed model is compared with current state-of-the-art techniques. We show that our method of decoupling structure and texture reconstruction improves the quality of the final reconstructed high-resolution image. Our proposed models are evaluated on the following publicly available datasets High-quality version of the CelebA dataset with 30K images We evaluate our model using PSNR and SSIM for \u00d72, \u00d74 and \u00d78 SISR scale factors Table 1: Comparison of PSNR and SSIM for \u00d72, \u00d74, and \u00d78 factor SISR over Set5, Set14, BSD100, and Celeb-HQ datasets with bicubic interpolation, ENet \u00d78 0752 \u00d72 \u00d74 Bicubic Ours ENet Baseline EDSR Table 2: Quantitative performance of edge enhancer for Single Image Super-Resolution trained on Canny edges with \u03c3 = 2 for 512 \u00d7 512 images. Statistics are calculated over the standard test sets of each dataset. interpolation , \u00d74 SISR , \u00d74 predicted edge - map SISR . Precision LR Recall Ground Truth",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"BSD100 - 4x upscaling\", \"Metric\": \"PSNR\", \"Score\": \"24.25\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"BSD100 - 4x upscaling\", \"Metric\": \"SSIM\", \"Score\": \"0.851\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"Celeb-HQ 4x upscaling\", \"Metric\": \"PSNR\", \"Score\": \"28.23\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"Celeb-HQ 4x upscaling\", \"Metric\": \"SSIM\", \"Score\": \"0.912\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"Set5 - 4x upscaling\", \"Metric\": \"PSNR\", \"Score\": \"28.59\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"Set5 - 4x upscaling\", \"Metric\": \"SSIM\", \"Score\": \"0.965\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"Set14 - 4x upscaling\", \"Metric\": \"PSNR\", \"Score\": \"25.19\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"Set14 - 4x upscaling\", \"Metric\": \"SSIM\", \"Score\": \"0.894\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Super-Resolution\", \"Dataset\": \"CelebA 64x64\", \"Metric\": \"FID\", \"Score\":\"4x upscaling\", \"Dataset\": \"CelebA 128x128\", \"Metric\": \"FID\", \"Score\":\"4x upscaling\", \"Metric\": \"MS-SSIM\", \"Score\": \"0.18\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Super-Resolution\", \"Dataset\": \"CelebA 128x128\", \"Metric\": \"PSNR\", \"Score\": \"38.46\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Super-Resolution\", \"Dataset\": \"CelebA 128x128\", \"Metric\": \"SSIM\", \"Score\": \"0.96\"}} ]"
    },
    {
        "Context": "SIGN: Scalable Inception Graph Neural Networks Graph representation learning has recently been applied to abroad spectrum of problems ranging from computer graphics and chemistry to high energy physics and social media. The popularity of graph neural networks has sparked interest, both in academia and in industry, in developing methods that scale to very large graphs such as Facebook or Twitter social networks. In most of these approaches, the computational cost is alleviated by a sampling strategy retaining a subset of node neighbors or subgraphs at training time. In this paper we propose anew, efficient and scalable graph deep learning architecture which sidesteps the need for graph sampling by using graph convolutional filters of different size that are amenable to efficient precomputation, allowing extremely fast training and inference. Our architecture allows using different local graph operators (e.g. motifinduced adjacency matrices or Personalized Page Rank diffusion matrix) to best suit the task at hand. We conduct extensive experimental evaluation on various open benchmarks and show that our approach is competitive with other state-ofthe-art architectures, while requiring a fraction of the training and inference time. Moreover, we obtain state-of-the-art results on ogbn-papers100M, the largest public graph dataset, with over 110 million nodes and 1.5 billion edges. * Equal contribution Preprint. Under review. Datasets We evaluated the proposed method on node-wise classification tasks, both in transductive and inductive settings Inductive experiments are performed using four datasets: Reddit, Flickr, Yelp, and PPI Transductive experiments were performed on the new ogbn-products and ogbn-papers100M datasets Overall, this dataset is orders-of-magnitude larger than any existing node classification dataset and is therefore the most important testbed for the scalability of SIGN and related methods Statistics for all the datasets are reported in PPR-based operators are computed from a symmetrically normalized adjacency transition matrix in an approximated form, with a restart probability of \u03b1 = 0.01 for inductive datasets and \u03b1 = 0.05 in the transductive case Architectural and optimization hyperparameters were estimated using Bayesian optimization with a tree Parzen estimator surrogate function overall inductive datasets Given that this dataset represents a directed network, we experimented with operators built via asymmetric normalization of the original directed adjacency matrix and its Table 3: Summary of (s)ingle and (m)ulti-label dataset statistics. Wikipedia is used, with random features, for timing purposes only. 100% / - / 100% Train / Val / Test Table 4: Mean and standard deviation of preprocessing, training (one epoch) and inference times, in seconds, on ogbn-products and Wikipedia datasets, computed over 10 runs. SIGN-r denotes architecture with r precomputed operators. Preprocessing and training times for ClusterGCN on Wikipedia are not reported due to the clustering algorithm failing to complete. Training Preprocessing ogbn - products Inference Wikipedia Table 5: Micro-averaged F1 scores. For SIGN, we show the best performing configurations. The top three performance scores are highlighted as: First, Second, Third. Table 6: Performance on ogbn-products. SIGN(p,s,t) refers to a configuration using p, s, and t powers of simple, PPR-based, and triangle-based adjacency matrices. The top three performance scores are highlighted as: First, Second, Third. Training Validation Test MLP 84.03\u00b10.93 75.54\u00b10.14",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"AMZ Photo\", \"Metric\": \"Accuracy\", \"Score\": \"91.72%\"}}, {\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"Reddit\", \"Metric\": \"Accuracy\", \"Score\": \"96.60%\"}}, {\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"Coauthor CS\", \"Metric\": \"Accuracy\", \"Score\": \"91.98\"}}, {\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"PPI\", \"Metric\": \"F1\", \"Score\": \"96.50\"}}, {\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"AMZ Comp\", \"Metric\": \"Accuracy\", \"Score\": \"85.93%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"NCI1\", \"Metric\": \"Accuracy\", \"Score\": \"80.7%\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Pubmed\", \"Metric\": \"Accuracy\", \"Score\": \"76.6%\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Cora\", \"Metric\": \"Accuracy\", \"Score\": \"82.3%\"}} ]"
    },
    {
        "Context": "Attention-based Dropout Layer for Weakly Supervised Object Localization Weakly Supervised Object Localization (WSOL) techniques learn the object location only using image-level labels, without location annotations. A common limitation for these techniques is that they cover only the most discriminative part of the object, not the entire object. To address this problem, we propose an Attention-based Dropout Layer (ADL), which utilizes the self-attention mechanism to process the feature maps of the model. The proposed method is composed of two key components: 1) hiding the most discriminative part from the model for capturing the integral extent of object, and 2) highlighting the informative region for improving the recognition power of the model. Based on extensive experiments, we demonstrate that the proposed method is effective to improve the accuracy of WSOL, achieving anew state-of-the-art localization accuracy in CUB-200-2011 dataset. We also show that the proposed method is much more efficient in terms of both parameter and computation overheads than existing techniques. Dataset We evaluate the performance of the proposed method in CUB-200-2011 and ImageNet-1k, respectively The ImageNet-1k is a large-scale dataset with 1,000 different classes, consisting of approximately 1.3 million training images and 50,000 validation images For this dataset, we train the model with the training set and evaluate the performance with the validation set For this dataset, we train the model with the training set and evaluate the performance with the testing set The intraclass variation of CUB-200-2011 is smaller than that of ImageNet-1k, because all classes of this dataset belong to birds Consequently, although CUB-200-2011 is not a large-scale dataset such as ImageNet-1k, this is a particularly challenging dataset to conduct WSOL We use a pre-trained model which is trained with ImageNet-1k dataset, and then fine-tune the network We use three evaluation metrics as: Top-1 classification accuracy (Top-1 Clas), Localization accuracy with known ground-truth class (GT-known Loc), and Top-1 localization Table 1. Upper: Accuracy according to drop rate. Middle: Base- line accuracy. Lower: Accuracy when each component has been deactivated. Bold text refers the best localization accuracy, while italic text refers the best classification accuracy. N/A indicates that ADL outputs the raw input feature map instead of applying drop mask or importance map. Clas ( % ) Acc ( % ) Loc ( % ) Top - 1 GT - known Table 2. Effects in accuracy upon the choice of the feature maps to employ ADL. Bold text refers the best localization accuracy, while italic text refers the best classification accuracy. Clas ( % ) Acc ( % ) Loc ( % ) GT - Known Top - 1 Table 3. Quantitative evaluation results on CUB-200-2011 and ImageNet-1k. Bold text refers the best localization accuracy for each backbone network. We also underline the best score in each dataset. Overheads are computed",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Weakly-Supervised Object Localization\", \"Dataset\": \"CUB-200-2011\", \"Metric\": \"Top-1 Error Rate\", \"Score\": \"37.71\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Weakly-Supervised Object Localization\", \"Dataset\": \"ILSVHN\", \"Metric\": \"Top-1 Localization Accuracy\", \"Score\": \"60.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Weakly-Supervised Object Localization\", \"Dataset\": \"CUB-200-2011\", \"Metric\": \"Top-1 Error Rate\", \"Score\": \"64.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Weakly-Supervised Object Localization\", \"Dataset\": \"CUB-200-2011\", \"Metric\": \"Top-1 Localization Accuracy\", \"Score\": \"69.3\"}} ]"
    },
    {
        "Context": "NVAE: A Deep Hierarchical Variational Autoencoder Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ as shown in Fig. 1. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256\u00d7256 pixels. The source code is available at https://github.com/NVlabs/NVAE. In this section, we examine NVAE on several image datasets The performance is measured in bits/dimension (bpd) for all the datasets but MNIST in which negative log-likelihood in nats is reported (lower is better in all cases) NVAE outperforms previous non-autoregressive models on most datasets and reduces the gap with autoregressive models For all the datasets but FFHQ, we follow Glow for the train and test splits Hyperparameters: Given a large number of datasets and the heavy compute requirements, we do not exhaustively optimize the hyperparameters Table 1: Comparison against the state-of-the-art likelihood-based generative models. The performance is measured in bits/dimension (bpd) for all the datasets but MNIST in which negative log-likelihood in nats is reported (lower is better in all cases). NVAE outperforms previous non-autoregressive models on most datasets and reduces the gap with autoregressive models. VAE Models with an Unconditional Decoder Flow Models without any Autoregressive Components in the Generative Model VAE and Flow Models with Autoregressive Components in the Generative Model Autoregressive Models - CelebA HQ 64\u00d764 MNIST CIFAR - 10 FFHQ 28\u00d728 CelebA 256\u00d7256 - 32\u00d732 ImageNet Table 2: Normalization & activation Table 3 : Residual cells in NVAE Bottom - up Top - down Test L = 10 L = 20 L = 40 Table 2 : Normalization & activation ( bpd ) time ( h ) ( GB ) Train Mem . Table 3: Residual cells in NVAE Table 3 :",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Generation\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"bits/dimension\", \"Score\": \"2.91\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Generation\", \"Dataset\": \"CelebA 256x256\", \"Metric\": \"bpd\", \"Score\": \"0.70\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Generation\", \"Dataset\": \"ImageNet 32x32\", \"Metric\": \"bpd\", \"Score\": \"3.92\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Generation\", \"Dataset\": \"FFHQ 256 x 256\", \"Metric\": \"bits/dimension\", \"Score\": \"0.69\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"bits/dimension\", \"Score\": \"2.38\"}} ]"
    },
    {
        "Context": "The privacy implications of Bluetooth A substantial amount of research, as well as media hype, has surrounded RFID technology and its privacy implications. Currently, researchers and the media focus on the privacy threats posed by RFID, while consumer groups choose to boycott products bearing RFID tags. At the same, however, a very similar technology has quietly become part of our everyday lives: Bluetooth. In this paper we highlight the fact that Bluetooth is a widespread technology that has real privacy implications. Furthermore, we explore the applicability of RFID-based solutions to address these privacy implications.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Sequence Level Semantics Aggregation for Video Object Detection Video objection detection (VID) has been a rising research direction in recent years. A central issue of VID is the appearance degradation of video frames caused by fast motion. This problem is essentially ill-posed fora single frame. Therefore, aggregating features from other frames becomes a natural choice. Existing methods rely heavily on optical flow or recurrent neural networks for feature aggregation. However, these methods emphasize more on the temporally nearby frames. In this work, we argue that aggregating features in the full-sequence level will lead to more discriminative and robust features for video object detection. To achieve this goal, we devise a novel Sequence Level Semantics Aggregation (SELSA) module. We further demonstrate the close relationship between the proposed method and the classic spectral clustering method, providing a novel view for understanding the VID problem. We test the proposed method on the ImageNet VID and the EPIC KITCHENS dataset and achieve new state-of-theart results. Our method does not need complicated postprocessing methods such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and clean. In this section, we first introduce the datasets and evaluation metrics used for VID in Sec EPIC KITCHENS is a large scale egocentric dataset, capturing daily activities happened in the kitchens In EPIC KITCHENS dataset, each frame contains avg/max 1.7/9 ob- jects, which is far more complex and challenging 106 sequences collected in the same 28 kitchens (S1) and 54 sequences collected in other 4 unseen kitchens (S2) are used for evaluation ImageNet VID dataset falls short in the density and diversity of objects Here we evaluate SELSA on the EPIC KITCHENS dataset Table 1. Detection results on the ImageNet VID validation set. For sequence-level methods, 21 frames are used when testing. No post-processing techniques are used. The absolute gains compared with the baseline are shown in the subscript. Sequence - level Info ( b ) ( a ) ( c ) Table 2. The effects of post-processing on our method. The abso- lute gains compared with the method without Seq-NMS are shown in the subscript. mAP ) which are both built on flow - based feature aggrega - tively . It also outperforms D ( & T loss ) [ 6 ] by a large margin With no video - level post - processing techniques , com - ResNet - 101 mAP ( % ) Methods Table 4. Performance comparison on EPIC KITCHENS test set. S1 and S2 indicate Seen and Unseen splits. S2 mAP@ . 05 mAP@ . 5 mAP@ . 75",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Video Object Detection\", \"Dataset\": \"ImageNet VID\", \"Metric\": \"MAP\", \"Score\": \"84.3%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Video Object Detection\", \"Dataset\": \"ImageNet VID\", \"Metric\": \"MAP\", \"Score\": \"85.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Video Object Detection\", \"Dataset\": \"ImageNet VID\", \"Metric\":\"mAP\", \"Score\": \"48.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Video Object Detection\", \"Dataset\": \"ImageNet VID\", \"Metric\": \"MAP\", \"Score\": \"85.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Video Object Detection\", \"Dataset\": \"ImageNet VID\", \"Metric\":\"mAP\", \"Score\": \"40.8\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Multi-Object Tracking\", \"Dataset\": \"PASCAL VOC 2007\", \"Metric\": \"MAP\", \"Score\": \"82.6%\"}} ]"
    },
    {
        "Context": "FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation Modern approaches for semantic segmentation usually employ dilated convolutions in the backbone to extract highresolution feature maps, which brings heavy computation complexity and memory footprint. To replace the time and memory consuming dilated convolutions, we propose a novel joint upsampling module named Joint Pyramid Upsampling (JPU) by formulating the task of extracting highresolution feature maps into a joint upsampling problem. With the proposed JPU, our method reduces the computation complexity by more than three times without performance loss. Experiments show that JPU is superior to other upsampling modules, which can be plugged into many existing approaches to reduce computation complexity and improve performance. By replacing dilated convolutions with the proposed JPU module, our method achieves the state-of-the-art performance in Pascal Context dataset (mIoU of 53.13%) and ADE20K dataset (final score of 0.5584) while running 3 times faster. Code is available in https://github.com/wuhuikai/FastFCN . In this section, we first introduce the datasets used in our experiments as well as the implementation details Finally, to compare with the stateof-the-art methods, we report the performance on two segmentation datasets, Pascal Context and ADE20K, which are widely used as the segmentation benchmarks Dataset Pascal Context dataset is based on the PAS-CAL VOC 2010 detection challenge, which provides additional pixel-wise semantic annotations Table 1: Performance on the val set of Pascal Context dataset with the ResNet-50 as the backbone. mIoU% pixAcc% Table 2: Comparison of Computation Complexity. The FPS is measured on a Titan-Xp GPU with a 512\u00d7512 image as input, which is averaged among 100 runs. FPS Table 3: The state-of-the-art methods on the val set of the Pascal Context dataset. PSPNet [ 38 ] EncNet [ 36 ] Backbone mIoU% pixAcc% Table 4: Results on the val set of ADE20K dataset. PSPNet [ 38 ] EncNet [ 36 ] mIoU% pixAcc% Table 5: Results on ADE20K test set. The first two entries ranked 1st and 2nd place in COCO-Place challenge 2017. Final Score",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"ADE20K\", \"Metric\": \"Test Score\", \"Score\": \"55.84\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"ADE20K\", \"Metric\": \"Validation mIoU\", \"Score\": \"44.34\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL Context\", \"Metric\": \"mIoU\", \"Score\": \"53.1\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"ADE20K val\", \"Metric\":\"mIoU\", \"Score\": \"42.99\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"ADE20K\", \"Metric\": \"Test Score\", \"Score\": \"56.32\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"ADE20K\", \"Metric\": \"Validation mIoU\", \"Score\": \"43.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"ADE20K\", \"Metric\": \"Test Score\", \"Score\": \"52.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"ADE20K\", \"Metric\": \"Validation mIoU\", \"Score\": \"41.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL Context\", \"Metric\":\"mIoU\", \"Score\": \"53.9\"}} ]"
    },
    {
        "Context": "Web-enabling Cache Daemon for Complex Data One of the most common basic techniques for improving the performance of web applications is caching frequently accessed data in fast data stores, colloquially known as cache daemons. In this paper we present a cache daemon suitable for storing complex data while maintaining fine-grained control over data storage, retrieval and expiry. Data manipulation in this cache daemon is performed via standard SQL statements so we call it SQLcached. It is a practical, usable solution already implemented in several large web sites.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "LADDERNET: MULTI-PATH NETWORKS BASED ON U-NET FOR MEDICAL IMAGE SEGMENTATION U-Net has been providing state-of-the-art performance in many medical image segmentation problems. Many modifications have been proposed for U-Net, such as attention U-Net, recurrent residual convolutional U-Net (R2-UNet), and U-Net with residual blocks or blocks with dense connections. However, all these modifications have an encoderdecoder structure with skip connections, and the number of paths for information flow is limited. We propose Lad-derNet in this paper, which can be viewed as a chain of multiple U-Nets. Instead of only one pair of encoder branch and decoder branch in U-Net, a LadderNet has multiple pairs of encoder-decoder branches, and has skip connections between every pair of adjacent decoder and decoder branches in each level. Inspired by the success of ResNet and R2-UNet, we use modified residual blocks where two convolutional layers in one block share the same weights. A LadderNet has more paths for information flow because of skip connections and residual blocks, and can be viewed as an ensemble of Fully Convolutional Networks (FCN). The equivalence to an ensemble of FCNs improves segmentation accuracy, while the shared weights within each residual block reduce parameter number. Semantic segmentation is essential for retinal disease detection. We tested LadderNet on two benchmark datasets for blood vessel segmentation in retinal images, and achieved superior performance over methods in the literature. The implementation is provided https: //github.com/juntang-zhuang/LadderNet We evaluated the proposed LadderNet on two popular datasets for retina blood vessel segmentation: the DRIVE dataset and the CHASE DB1 dataset The DRIVE dataset consists of 40 color images of the retina, 20 of which were used for training and the remaining 20 images for testing The CHASE DB1 dataset was collected from both left and right eyes of 14 schoolchildren Field of view (FOV) is provided for the DRIVE dataset but not the CHASE DB1 dataset We used several metrics to evaluate the performance of Lad-derNet, including accuracy (AC), sensitivity (SE), specificity (SP) and F1-score Different metrics are calculated as follows: The F1-score is calculated as follows: To further evaluate the performance of different neural networks, we calculated the receiver operating characteristics (ROC) curve and the are under curve (AUC) SE and SP focus more on one category than the other, while other metrics such as AC, AUC and",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Retinal Vessel Segmentation\", \"Dataset\": \"DRIVE\", \"Metric\": \"AUC\", \"Score\": \"0.9793\"}}, {\"LEADERBOARD\": {\"Task\": \"Retinal Vessel Segmentation\", \"Dataset\": \"DRIVE\", \"Metric\": \"F1 score\", \"Score\": \"0.8202\"}}, {\"LEADERBOARD\": {\"Task\": \"Retinal Vessel Segmentation\", \"Dataset\": \"CHASE_DB1\", \"Metric\": \"AUC\", \"Score\": \"0.9839\"}}, {\"LEADERBOARD\": {\"Task\": \"Retinal Vessel Segmentation\", \"Dataset\": \"CHASE_DB1\", \"Metric\": \"F1 score\", \"Score\": \"0.8031\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Retinal Vessel Segmentation\", \"Dataset\": \"DRIVE\", \"Metric\": \"AUC\", \"Score\": \"0.9864\"}}, { \"LEADERBOARD\": { \"Task\": \"Retinal Vessel Segmentation\", \"Dataset\": \"DRIVE\", \"Metric\": \"Accuracy\", \"Score\": \"0.9698\"}}, { \"LEADERBOARD\": { \"Task\": \"Retinal Vessel Segmentation\", \"Dataset\": \"DRIVE\", \"Metric\": \"F1 score\", \"Score\": \"0.8237\"}}, { \"LEADERBOARD\": { \"Task\": \"Retinal Vessel Segmentation\", \"Dataset\": \"CHASE_DB1\", \"Metric\": \"AUC\", \"Score\": \"0.9905\"}}, { \"LEADERBOARD\": { \"Task\": \"Retinal Vessel Segmentation\", \"Dataset\": \"CHASE_DB1\", \"Metric\": \"F1 score\", \"Score\": \"0.8153\"}} ]"
    },
    {
        "Context": "Public-key cryptography in functional programming context Cryptography is the science of information and communication security. Up to now, for efficiency reasons cryptographic algorithm has been written in an imperative language. But to get acquaintance with a functional programming language a question arises: functional programming offers some new for secure communication or not? This article investigates this question giving an overview on some cryptography algorithms and presents how the RSA encryption in the functional language Clean can be implemented and how can be measured the efficiency of a certain application.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "A Flexible and Secure Remote Systems Authentication Scheme Using Smart Cards The paper presents an authentication scheme for remote systems using smart card. The scheme prevents the scenario of many logged in users with the same login identity, and does not require password/verifier table to validate the users' login request. The scheme provides a user-friendly password change option, and withstands the replay, impersonation, stolen-verifier, guessing, and denial-of-service attacks 1 .",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "PolyLaneNet: Lane Estimation via Deep Polynomial Regression One of the main factors that contributed to the large advances in autonomous driving is the advent of deep learning. For safer self-driving vehicles, one of the problems that has yet to be solved completely is lane detection. Since methods for this task have to work in real-time (+30 FPS), they not only have to be effective (i.e., have high accuracy) but they also have to be efficient (i.e., fast). In this work, we present a novel method for lane detection that uses as input an image from a forwardlooking camera mounted in the vehicle and outputs polynomials representing each lane marking in the image, via deep polynomial regression. The proposed method is shown to be competitive with existing state-of-the-art methods in the TuSimple dataset while maintaining its efficiency (115 FPS). Additionally, extensive qualitative results on two additional public datasets are presented, alongside with limitations in the evaluation metrics used by recent works for lane detection. Finally, we provide source code and trained models that allow others to replicate all the results shown in this paper, which is surprisingly rare in state-of-the-art lane detection methods. The full source code and pretrained models are available at https://github.com/lucastabelini/PolyLaneNet. PolyLaneNet was evaluated on publicly available which are introduced in this section Three datasets were used to evaluate PolyLaneNet: TuSimple, LLAMAS and ELAS The dataset has a total of 6,408 annotated images with a resolution of 1280\u00d7720 pixels, and it is originally split in 3,268 for training, 358 for validation, and 2,782 for testing For qualitative results, two other datasets were used: LLAMAS and ELAS The first is a large dataset, split into 58,269 images for training, 20,844 for validation, and 20,929 for test, with a resolution of 1280\u00d7717 pixels Both TuSimple and LLAMAS are datasets from the USA ELAS is a dataset with 16,993 images from various cities in Brazil, with a resolution of 640\u00d7480 pixels Since the dataset was originally proposed fora non-learning based method, it does not provide training/testing splits The main difference between ELAS and the other two datasets is that in ELAS only the ego-lane is",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Lane Detection\", \"Dataset\": \"TuSimple\", \"Metric\": \"Accuracy\", \"Score\": \"93.36%\"}}, {\"LEADERBOARD\": {\"Task\": \"Lane Detection\", \"Dataset\": \"TuSimple\", \"Metric\": \"F1 score\", \"Score\": \"90.62\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Lane Detection\", \"Dataset\": \"TuSimple\", \"Metric\": \"Accuracy\", \"Score\": \"95.24%\"}}, { \"LEADERBOARD\": { \"Task\": \"Lane Detection\", \"Dataset\": \"TuSimple\", \"Metric\": \"F1 score\", \"Score\": \"95.31\"}} ]"
    },
    {
        "Context": "Milking CowMask for Semi-Supervised Image Classification Consistency regularization is a technique for semi-supervised learning that underlies a number of strong results for classification with few labeled data. It works by encouraging a learned model to be robust to perturbations on unlabeled data. Here, we present a novel mask-based augmentation method called CowMask. Using it to provide perturbations for semi-supervised consistency regularization, we achieve a state-of-the-art result on ImageNet with 10% labeled data, with a top-5 error of 8.76% and top-1 error of 26.06%. Moreover, we do so with a method that is much simpler than many alternatives. We further investigate the behavior of CowMask for semi-supervised learning by running many smaller scale experiments on the SVHN, CIFAR-10 and CIFAR-100 data sets, where we achieve results competitive with the state of the art, indicating that CowMask is widely applicable. We open source our code at https://github.com/google-research/google-research/tree/ master/milking_cowmask. We first evaluate CowMix for semi-supervised consistency regularization on the challenging ImageNet dataset, where we match the state of the art Next, we examine CowOut and CowMix further and compare with previously proposed methods by trying multiple versions of our approach combined with multiple models on three small image datasets: CIFAR-10, CIFAR-100 and SVHN The training regimes used for both ImageNet and the small image datasets are sufficiently similar that we used the same codebase for all of our experiments Our results are obtained by using the teacher network for evaluation Supervised sets are consistent for all experiments fora given dataset and number of supervised samples Alongside CowOut and CowMix we implemented and evaluated Mean Teacher, CutOut/RandErase and CutMix, and we compare our method against these using the CIFAR-10, CIFAR-100, and SVHN datasets Table 1: Results on ImageNet with 10% labels. Note that S 4 L involves three steps with different training procedures, while CowMix involves a single training run. SimCLR is able to beat CowMix, but only when using a very large model. 4 Our results - Other work Our baselines Table 2: Results on CIFAR-10 test set, error rates as mean \u00b1 std \u2212 dev of 5 independent runs. Other work : uses 26M parameter models Augmentation / erasure based regularization Our results : uses 27M parameter Wide ResNet 28 - 96x2d with shake - shake Mix based regularization 100 ALL 4000 500 1000 2000 50 40 250 Table 3: Results on SVHN test set, error rates as mean \u00b1 stdev of 5 independent runs. Other work : uses 26M parameter models Augmentation / erasure based regularization Our results : uses 27M parameter Wide ResNet 28 - 96x2d with shake - shake",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 10% labeled data\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"73.94%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 10% labeled data\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"91.24%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 1% labeled data\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"89.9%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 1% labeled data\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"89.5%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 10% labeled data\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"83.9%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 1% labeled data\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"96.7%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 1% labeled data\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"83.1%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 1% labeled data\", \"Metric\": \"Top 5 Accuracy\", 'Score'}]"
    },
    {
        "Context": "An End-to-End Neighborhood-based Interaction Model for Knowledge-enhanced Recommendation This paper studies graph-based recommendation, where an interaction graph is constructed from historical records and is leveraged to alleviate data sparsity and cold start problems. We reveal an early summarization problem in existing graph-based models, and propose Neighborhood Interaction (NI) model to capture each neighbor pair (between user-side and item-side) distinctively. NI model is more expressive and can capture more complicated structural patterns behind user-item interactions. To further enrich node connectivity and utilize high-order structural information, we incorporate extra knowledge graphs (KGs) and adopt graph neural networks (GNNs) in NI, called Knowledge-enhanced Neighborhood Interaction (KNI). Compared with the state-of-the-art recommendation methods, e.g., feature-based, meta path-based, and KG-based models, our KNI achieves superior performance in click-through rate prediction (1.1%-8.4% absolute AUC improvements) and outperforms by a wide margin in top-N recommendation on 4 real world datasets. We combine 4 recommendation datasets with 2 public knowledge graphs in our experiments The datasets and experiment code are publicly available 3 for reproducibility and further study The first two smaller datasets are released by We follow the procedures of to process the other two larger datasets, which are then linked to Freebase Note that another dataset LFM in KB4Rec is not included in our experiments, because it follows a quite different scheme from the others and does not contain any rating or click information After the datasets are processed, we split each dataset into training/validation/test sets at 6:2:2 For each dataset, we use the linked items as initial queries to find related non-item entities We repeat this process 4 times to ensure sufficient knowledge is included in the final dataset The basic statistics of the 4 datasets are presented in We evaluate these models on 2 tasks, click-through rate (CTR) Table 1: Statistics for the expanded datasets. Note: \"entities\" contain both items and non-item entities. 11 , 895 17 , 860 64 , 067 59 , 296 1 , 181 , 684 182 , 011 Movie - 20M 265 , 478 77 , 881 9 , 104 , 038 139 , 746 Movie - 1M 6 , 036 753 , 772 32 , 389 78 , 809 C - Book A - Book 14 , 967 2 , 445 Table 2: The results of CTR prediction. Note: \"*\" indicates the statistically significant improvements over the best baseline, with p-value smaller than 10 \u22126 in two-sided t-test. Model ACC Movie - 1M C - Book A - Book Movie - 20M AUC Table 3: Data sparsity statistics and AUC improvements. Note: The n-hop columns represent the number of n-hop neighbors. The sparsity is calculated as # missing edges / # node pairs.",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Click-Through Rate Prediction\", \"Dataset\": \"MovieLens 20M\", \"Metric\": \"AUC\", \"Score\": \"0.9704\"}}, {\"LEADERBOARD\": {\"Task\": \"Click-Through Rate Prediction\", \"Dataset\": \"MovieLens 1M\", \"Metric\": \"AUC\", \"Score\": \"0.9449\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Citeseer\", \"Metric\": \"Accuracy\", \"Score\": \"74.4%\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Pubmed\", \"Metric\": \"Accuracy\", \"Score\": \"74.4%\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Cora\", \"Metric\": \"Accuracy\", \"Score\": \"82.3%\"}} ]"
    },
    {
        "Context": "A Fast and Accurate Unconstrained Face Detector We propose a method to address challenges in unconstrained face detection, such as arbitrary pose variations and occlusions. First, anew image feature called Normalized Pixel Difference (NPD) is proposed. NPD feature is computed as the difference to sum ratio between two pixel values, inspired by the Weber Fraction in experimental psychology. The new feature is scale invariant, bounded, and is able to reconstruct the original image. Second, we propose a deep quadratic tree to learn the optimal subset of NPD features and their combinations, so that complex face manifolds can be partitioned by the learned rules. This way, only a single soft-cascade classifier is needed to handle unconstrained face detection. Furthermore, we show that the NPD features can be efficiently obtained from a lookup table, and the detection template can be easily scaled, making the proposed face detector very fast. Experimental results on three public face datasets (FDDB, GENKI, and CMU-MIT) show that the proposed method achieves state-of-the-art performance in detecting unconstrained faces with arbitrary pose variations and occlusions in cluttered scenes. We evaluate the performance of the NPD face detector on three public-domain databases, FDDB, GENKI, and CMU-MIT The FDDB dataset covers challenging scenarios for face detection Images in FDDB comes from the Faces in the Wild dataset, which is a large collection of Internet images collected from the Yahoo News For benchmark evaluation, Jain and Learned-Miller provided an evaluation code fora comparison of different face detection algorithms There are two metrics for performance evaluation based on ROC: discrete score metric and continuous score metric, which correspond to coarse match (similar to previous evaluations in the face detection literature) and precise match, respectively, between the detection and the ground truth The database is divided into 10 subsets for performance evaluation, and the obtained detection results are accumulated to generate the ROC curve According to, ten ROC curves should be obtained and averaged for the final performance report, however, what is actually done and continuous metrics on the FDDB database [ 3 ] * FP = 100 TABLE 1 Continuous Metric Discrete Metric FP = 0 FP = 10 NPD POF DQT Haar 150 140 176 276 DQT - Soft 1 , 597 108 LBP 72 Stump CART n / a SURF [ 13 ] * NPD OpenCV Resolution i5@3 . 1GHz i7@3 . 9GHz Yan - DPM [ 46 ] X5650@2 . 66GHz JCascade [ 48 ] NPD @2 . 93GHz ACF [ 49 ]",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Face Detection\", \"Dataset\": \"FDDB\", \"Metric\": \"AP\", \"Score\": \"0.864\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Detection\", \"Dataset\": \"PASCAL Face\", \"Metric\": \"AP\", \"Score\": \"0.9029\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Detection\", \"Dataset\": \"Annotated Faces in the Wild\", \"Metric\": \"AP\", \"Score\": \"0.9721\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Face Detection\", \"Dataset\": \"FDDB\", \"Metric\": \"AP\", \"Score\": \"0.990\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Detection\", \"Dataset\": \"CMU-PIE\", \"Metric\": \"AP\", \"Score\": \"0.985\"}} ]"
    },
    {
        "Context": "A Sim2Real Deep Learning Approach for the Transformation of Images from Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird's Eye View* Accurate environment perception is essential for automated driving. When using monocular cameras, the distance estimation of elements in the environment poses a major challenge. Distances can be more easily estimated when the camera perspective is transformed to a bird's eye view (BEV). For flat surfaces, Inverse Perspective Mapping (IPM) can accurately transform images to a BEV. Three-dimensional objects such as vehicles and vulnerable road users are distorted by this transformation making it difficult to estimate their position relative to the sensor. This paper describes a methodology to obtain a corrected 360 \u2022 BEV image given images from multiple vehicle-mounted cameras. The corrected BEV image is segmented into semantic classes and includes a prediction of occluded areas. The neural network approach does not rely on manually labeled data, but is trained on a synthetic dataset in such away that it generalizes well to real-world data. By using semantically segmented images as input, we reduce the reality gap between simulated and real-world data and are able to show that our method can be successfully applied in the real world. Extensive experiments conducted on the synthetic data demonstrate the superiority of our approach compared to IPM. Source code and datasets are available at https://github.com/ika-rwth-aachen/Cam2BEV. In order to evaluate the methodology presented before, we train the neural networks entirely on simulated data In the following, we present the synthetic dataset and the training setup",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Cross-View Image-to-Image Translation\", \"Dataset\": \"Cam2BEV\", \"Metric\": \"Mean IoU\", \"Score\": \"71.92\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cam2BEV\", \"Metric\": \"Mean IoU\", \"Score\": \"71.92\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Semantic Segmentation\", \"Dataset\": \"TuSimple\", \"Metric\": \"Accuracy\", \"Score\": \"75.70%\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Semantic Segmentation\", \"Dataset\": \"TuSimple\", \"Metric\": \"F1 score\", \"Score\": \"94.31\"}} ]"
    },
    {
        "Context": "Control and Optimization Meet the Smart Power Grid: Scheduling of Power Demands for Optimal Energy Management The smart power grid aims at harnessing information and communication technologies to enhance reliability and enforce sensible use of energy. Its realization is geared by the fundamental goal of effective management of demand load. In this work, we envision a scenario with real-time communication between the operator and consumers. The grid operator controller receives requests for power demands from consumers, each with different power requirement, duration, and a deadline by which it is to be completed. The objective of the operator is to devise a power demand task scheduling policy that minimizes the grid operational cost over a time horizon. The operational cost is a convex function of instantaneous total power consumption and reflects the fact that each additional unit of power needed to serve demands is more expensive as the demand load increases.First, we study the off-line demand scheduling problem, where parameters are fixed and known a priori. If demands maybe scheduled preemptively, the problem is a load balancing one, and we present an iterative algorithm that optimally solves it. If demands need to be scheduled non-preemptively, the problem is a bin packing one. Next, we devise a stochastic model for the case when demands are generated continually and scheduling decisions are taken online and focus on long-term average cost. We present two instances of power consumption control based on observing current consumption. In the first one, the controller may choose to serve anew demand request upon arrival or to postpone it to the end of its deadline. The second one has the additional option to activate one of the postponed demands when an active demand terminates. For both instances, the optimal policies are thresholdbased. We derive a lower performance bound overall policies, which is asymptotically tight as deadlines increase. We propose the Controlled Release threshold policy and prove it is asymptotically optimal. The policy activates anew demand request if the current power consumption is less than a threshold, otherwise it is queued. Queued demands are scheduled when their deadline expires or when the consumption drops below the threshold.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Properties of polynomial bases used in a line-surface intersection algorithm * In [5], Srijuntongsiri and Vavasis propose the Kantorovich-Test Subdivision algorithm, or KTS, which is an algorithm for finding all zeros of a polynomial system in a bounded region of the plane. This algorithm can be used to find the intersections between a line and a surface. The main features of KTS are that it can operate on polynomials represented in any basis that satisfies certain conditions and that its efficiency has an upper bound that depends only on the conditioning of the problem and the choice of the basis representing the polynomial system. This article explores in detail the dependence of the efficiency of the KTS algorithm on the choice of basis. Three bases are considered: the power, the Bernstein, and the Chebyshev bases. These three bases satisfy the basis properties required by KTS. Theoretically, Chebyshev case has the smallest upper bound on its running time. The computational results, however, do not show that Chebyshev case performs better than the other two. * Supported in part by NSF DMS 0434338 and NSF CCF 0085969.  Table 1: Comparison of the efficiency of KTS algorithm operating on the power, the Bernstein, and the Chebyshev bases. The number of patches examined during the course of the algorithm and the width of the smallest patch examined are shown for each version of KTS. Bernstein basis patches of width Power basis cond ( f ) Chebyshev basis Smallest Table 2: The numbers of test polynomials out of 1000 that bounding intervals associated with the Bernstein basis is tighter than the those associated with the Chebyshev basis, and vice versa. 37 40 that Chebyshev is tighter that Bernstein is tighter Table 3: The numbers of test polynomials out of 1000 that bounding intervals associated with the Bernstein basis and those associated with the Chebyshev basis having at least one endpoint exactly at the boundary of the ranges of the polynomials. 0 2 13 Bernstein with Chebyshev with exact endpoint",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Improving Semantic Segmentation via Video Propagation and Label Relaxation Semantic segmentation requires large amounts of pixelwise annotations to learn accurate models. In this paper, we present a video prediction-based methodology to scale up training sets by synthesizing new training samples in order to improve the accuracy of semantic segmentation networks. We exploit video prediction models' ability to predict future frames in order to also predict future labels. A joint propagation strategy is also proposed to alleviate mis-alignments in synthesized samples. We demonstrate that training segmentation models on datasets augmented by the synthesized samples leads to significant improvements inaccuracy. Furthermore, we introduce a novel boundary label relaxation technique that makes training robust to annotation noise and propagation artifacts along object boundaries. Our proposed methods achieve state-of-the-art mIoUs of 83.5% on Cityscapes and 82.9% on CamVid. Our single model, without model ensembles, achieves 72.8% mIoU on the KITTI semantic segmentation test set, which surpasses the winning entry of the ROB challenge 2018. Our code and videos can be found at https://nv-adlr.github. io/publication/2018-Segmentation. In this section, we evaluate our proposed method on three widely adopted semantic segmentation datasets, including Cityscapes, CamVid and KITTI For all three datasets, we use the standard mean Intersection over Union (mIoU) metric to report segmentation accuracy Table 1: Effectiveness of Mapillary pre-training and class uniform sampling on both fine and coarse annotations. mIoU ( % ) Table 2: Comparison between (1) label propagation (LP) and joint propagation (JP); (2) video prediction (VPred) and video recon- struction (VRec). Using the proposed video reconstruction and joint propagation techniques, we improve over the baseline by 1.08% mIoU (79.46% 80.54%). joint propagation techniques , we improve over the baseline by 80 . 54% ) . 0 \u00b11 \u00b12 \u00b13 \u00b14 Table 2 : Comparison between ( 1 ) label propagation ( LP ) and joint \u00b15 struction ( VRec ) . Using the proposed video reconstruction and Table 3: Per-class mIoU results on Cityscapes. Top: our ablation improvements on the validation set. Bottom: comparison with top- performing models on the test set. sky bus bicycle truck veg . tsign pole rider swalk mIoU mcycle road car person tlight wall fence terrain",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"CamVid\", \"Metric\": \"Mean IoU\", \"Score\": \"81.7%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"KITTI Semantic Segmentation\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"72.8%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"83.5%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"CamVid\", \"Metric\": \"Mean IoU\", \"Score\": \"82.78\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"82.7%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"82.0%\"}} ]"
    },
    {
        "Context": "Shape-Aware Human Pose and Shape Reconstruction Using Multi-View Images We propose a scalable neural network framework to reconstruct the 3D mesh of a human body from multi-view images, in the subspace of the SMPL model [23]. Use of multi-view images can significantly reduce the projection ambiguity of the problem, increasing the reconstruction accuracy of the 3D human body underclothing. Our experiments show that this method benefits from the synthetic dataset generated from our pipeline since it has good flexibility of variable control and can provide ground-truth for validation. Our method outperforms existing methods on real-world images, especially on shape estimations. Since it is often time-and labor-intensive to gather a dataset large enough for training a deep neural network, an increasing amount of attention is drawn to synthetic dataset generation Recent studies have shown that using a synthetic dataset, if sufficiently close to the real-world data, is helpful in training neural networks for real tasks built up a dataset (SURREAL) which contains human motion sequences with clothing using the SMPL model and CMU MoCap data While the SURREAL dataset is large enough and is very close to our needs, it is still insufficient in that (a) the clothing of the human is only a set of texture points on the body mesh, meaning that it is a tight clothing, (b) the body shape is drawn from the CAESAR dataset, where the uneven distribution of the shape parameters can serve as a \"prior bias\" to the neural network, and (c) the data only Table 1: Comparison results on Human3.6M using MPJPE. Smaller errors implies higher accuracy. Method w / syn . training Ours ( multi ) MPJPE / HD HMR Ours ( single ) w / o syn . training MPJPE Table 2: Comparison results on MPI INF 3DHP in PCK/AUC/ MPJPE. Better results have higher PCK/AUC and lower MPJPE. Table 4: Comparison on Human3.6M with other multi-view methods. Our method has comparable performance with previous work even without the assistance of camera calibration or temporal information. PA stands for Procrustes Aligned results for ours. Sitting Standing Table 5: Comparison results on tape-measured data using average relative errors (lower the better). Sitting Standing Table 6: Results on MPI INF 3DHP, validation set, before Pro- crustes aligment. also be drawn from these data . Hu - The same conclusion about over - Table 7: Results on MPI INF 3DHP, test set. The results of [19]",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"Human3.6M\", \"Metric\": \"Average MPJPE (mm)\", \"Score\": \"44.4\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"Human3.6M\", \"Metric\": \"Multi-View or Monocular\", \"Score\": \"Multi-View\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"Human3.6M\", \"Metric\": \"Using 2D ground-truth joints\", \"Score\": \"No\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"Human3.6M\", \"Metric\": \"Average MPJPE (mm)\", \"Score\": \"48.0\"}} ]"
    },
    {
        "Context": "Optimal Gradient Clock Synchronization in Dynamic Networks We study the problem of clock synchronization in highly dynamic networks, where communication links can appear or disappear at anytime. The nodes in the network are equipped with hardware clocks, but the rate of the hardware clocks can vary arbitrarily within specific bounds, and the estimates that nodes can obtain about the clock values of other nodes are inherently inaccurate. Our goal in this setting is to output a logical clock at each node such that the logical clocks of any two nodes are not too far apart, and nodes that remain close to each other in the network fora longtime are better synchronized than distant nodes. This property is called gradient clock synchronization.Gradient clock synchronization has been widely studied in the static setting, where the network topology does not change. We show that the asymptotically optimal bounds obtained for the static case also apply to our highly dynamic setting: if two nodes remain at distance d from each other for sufficiently long, it is possible to upper bound the difference between their clock values by O(d log(D/d)), where Dis the diameter of the network. This is known to be optimal even for static networks. Furthermore, we show that our algorithm has optimal stabilization time: when a path of length d appears between two nodes, the time required until the clock skew between the two nodes is reduced to O(d log(D/d)) is O(D), which we prove to be optimal. Finally, the techniques employed for the more intricate analysis of the algorithm for dynamic graphs provide additional insights that are also of interest for the static setting. In particular, we establish self-stabilization of the gradient property within O(D) time.A distributed clock synchronization algorithm computes at each node a logical clock, and the goal is to synchronize these clocks as tightly as possible. Traditionally, distributed clock synchronization algorithms focus on minimizing the clock skew between the logical clocks of any two nodes in the network. The clock skew between two clocks is simply the difference between the two clock values. The maximum clock skew that may occur in the worst case between any two nodes at anytime is called the global skew of a clock synchronization algorithm. A well-known result states that no algorithm can guarantee a global skew better than \u2126(D), where D denotes the diameter of the network [1]. However, in many cases it is more important to tightly synchronize the logical clocks of nearby nodes in the network than it is to minimize the global skew. For example, if a time division multiple access (TDMA) protocol is used to coordinate access to a shared communication medium in a wireless sensor network, it suffices to synchronize the clocks of nodes that interfere with each other when transmitting. The problem of providing better guarantees on the synchronization quality between nodes that are closer is called gradient clock synchronization. The problem was introduced in a seminal paper by Fan and Lynch [7], where the authors show that a clock skew of \u2126(log D/ log log D) cannot be prevented between immediate neighbors in the network. The largest possible clock skew that may occur between the logical clocks of any two adjacent nodes at anytime is called the local skew of a clock synchronization algorithm. For static networks, it has been proved that the best possible local skew that an algorithm can achieve is bounded by \u0398(log D) [15,16].While tight bounds have been shown for the static model, the dynamic case has not been as well understood. A dynamic network arises in many natural contexts: for example, when nodes are mobile, or when communication links are unreliable and may fail and recover. The dynamic network model we consider in this article is general: it allows communication links to appear and disappear arbitrarily, subject only to a global connectivity constraint (which is required to maintain a bounded global skew). Hence the model is suitable for modeling various types of dynamic networks which remain connected overtime.Ina dynamic network the distances between nodes changeover time as communication links appear and disappear. Consequently, we divide the synchronization guarantee into two parts: a global skew guarantee bounds the skew between any two nodes in the network at anytime, and a dynamic gradient skew guarantee that bounds the skew between two nodes as a function of the distance between them and how long they remain at that distance.In [11], three of the authors showed that a clock synchronization algorithm cannot react immediately to the formation of new links, and that a certain stabilization time is required before the clocks of newly-adjacent nodes can be brought into synch. The stabilization time is inversely related to the synchronization guarantee: the tighter the synchronization required in stable state, the longer the time to reach that state. Intuitively, this is because when strict synchronization guarantees are imposed, the algorithm cannot change clock values quickly without violating the guarantee, and hence it takes longer to react. The algorithm given in [11] achieves the optimal trade-off between skew bound and stabilization time; however, its local skew bound is O( \u221a D), which is far from optimal.In this article, we propose an algorithm, referred to as A OPT , that achieves the same asymptotically optimal skew bounds as in the static model: if two nodes remain at distance d for sufficiently long, the skew between them is reduced to O(d log(D/d)), where Dis the dynamic diameter of the network (corresponding roughly to the time it takes for information to propagate from one end of the network to the other). The stabilization time of the algorithm, that is, the time to reach this guarantee, is O(D).In the sequel, we refer to estimate edges of the sort described above simply as edges; similarly, when we say \"the graph\" we mean the estimate graph. We do not reason explicitly about the communication graph, as the salient aspects of communication are encapsulated by the estimate layer.Dynamic networks. We consider dynamic networks over a fixed set of nodes V of size n := |V |. Edge insertions and removals are modeled as discrete events controlled by a worst-case adversary. In keeping with the abstract representation from [12], we say that there is an estimate edge {u, v} between two nodes u, v \u2208 V at time t \u2265 0 iff u and v have a means of obtaining clock value estimates about each other at time t. As explained above, this does not necessarily mean that there is a direct communication link between u and vat time t.We do not assume that nodes detect the formation or failure of a communication link between them at the same time, which introduces some asymmetry into the model. Hence, we model the network as a directed dynamic graph G = (V, E), where E : R + 0 \u2192 2 (V \u00d7V ) maps non-negative times t to a set of directed estimate edges E(t) that exist at time t. If (u, v) \u2208 E(t), then at time t node u has an estimate for node v's logical clock, but not necessarily vice-versa. Formally, the set of node u's neighbors at time t is defined as N u (t) := {v | (u, v) \u2208 E(t)}. We assume that any asymmetry in the graph corresponds to the delay in nodes finding out about link status changes and is only temporary; this is explained below.In the following, we frequently refer to undirected edges {u, v}; when we write {u, v} \u2208 E(t), we mean that both (u, v) \u2208 E(t) and (v, u) \u2208 E(t). We say that edge {u, v} exists throughout a time interval [t 1 , t 2 ] if for all t \u2208 [t 1 , t 2 ] we have {u, v} \u2208 E(t). By extension, a path p is said to exist throughout [t 1 , t 2 ] if all its edges exist throughout the interval.Each undirected estimate edge {u, v} is associated with three parameters:\u2022 The estimate uncertainty \u01eb {u,v} , as explained above.\u2022 The detection delay \u03c4 {u,v} . We assume that u and v detect if the edge disappears \"at\" the respective other node within \u03c4 {u,v} \u2208 R + time. Formally, (a) if (u, v) / \u2208 E(t), then there is sometime t \u2032 \u2208 [t \u2212 \u03c4 {u,v} , t + \u03c4 {u,v} ] so that (v, u) / \u2208 E(t \u2032 ); and, symmetrically, (b) if (v, u) / \u2208 E(t), then there is sometime t \u2032 \u2208 [t \u2212 \u03c4 {u,v} , t + \u03c4 {u,v} ] so that (u, v) / \u2208 E(t \u2032 ).\u2022 We assume that u and v can exchange messages with message delay T {u,v} . More precisely, nodes that share an estimate edge can actively exchange information if required (possibly",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Polylogarithmic Approximation for Edit Distance and the Asymmetric Query Complexity We present a near-linear time algorithm that approximates the edit distance between two strings within a polylogarithmic factor; specifically, for strings of length n and every fixed \u03b5 > 0, it can compute a (log n) O(1/\u03b5) approximation inn 1+\u03b5 time. This is an exponential improvement over the previously known factor, 2\u00d5 ( \u221a log n) , with a comparable running time [OR07, AO09]. Previously, no efficient polylogarithmic approximation algorithm was known for any computational task involving edit distance (e.g., nearest neighbor search or sketching).This result arises naturally in the study of anew asymmetric query model. In this model, the input consists of two strings x and y, and an algorithm can access yin an unrestricted manner, while being charged for querying every symbol of x. Indeed, we obtain our main result by designing an algorithm that makes a small number of queries in this model. We then provide a nearly-matching lower bound on the number of queries.Our lower bound is the first to expose hardness of edit distance stemming from the input strings being \"repetitive\", which means that many of their substrings are approximately identical. Consequently, our lower bound provides the first rigorous separation between edit distance and Ulam distance, which is edit distance on non-repetitive strings, such as permutations. \u221a log n) approximation with constant communication via [OR07, KOR00]. The only known lower bound says that approximation \u03b1 requires \u2126( log n / log log n \u03b1 ) communication [AK10, AJP10].The asymmetric model is \"harder\", in the sense that the query complexity is at least the communication complexity, up to a factor of log |\u03a3| in the complexity, since Alice and Bob can simulate the asymmetric query algorithm. In fact, our upper bound implies a communication protocol for the same DTEP \u03b2 problem with the same complexity, and it is a one-way communication protocol. Specifically, Alice can just send the O(\u03b2n \u03b5 ) characters queried by the query algorithm in the asymmetric query model. This is the first communication protocol achieving polylogarithmic approximation for DTEP \u03b2 under edit distance with o(n) communication.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Structure-Aware Human-Action Generation Generating long-range skeleton-based human actions has been a challenging problem since small deviations of one frame can cause a malformed action sequence. Most existing methods borrow ideas from video generation, which naively treat skeleton nodes/joints as pixels of images without considering the rich inter-frame and intra-frame structure information, leading to potential distorted actions. Graph convolutional networks (GCNs) is a promising way to leverage structure information to learn structure representations. However, directly adopting GCNs to tackle such continuous action sequences both in spatial and temporal spaces is challenging as the action graph could be huge. To overcome this issue, we propose a variant of GCNs (SA-GCNs) to leverage the powerful self-attention mechanism to adaptively sparsify a complete action graph in the temporal space. Our method could dynamically attend to important past frames and construct a sparse graph to apply in the GCN framework, wellcapturing the structure information inaction sequences. Extensive experimental results demonstrate the superiority of our method on two standard human action datasets compared with existing methods. The code to reproduce our analysis is available at https://github.com/PingYuiris/SA-GCN. We perform experiments to evaluate the proposed method on two standard skeleton-based human-action benchmarks, the Human-3.6m dataset and the NTU RGB+D dataset We also conduct human evaluation on the Amazon Mechanical Turk (AMT) to access the perceptual quality of generated sequences To examine the functionality of each component of the proposed model, we also perform detailed ablation studies on the Human-3.6m dataset NTU RGB+D This dataset contains 56,000 video clips on 60 classes performed by 40 subjects and recorded with 3 different camera views We then apply two commonly used benchmarks fora further evaluation in the ten classes: (i)cross-view : the training set contains actions captured by two cameras and remaining data are left for testing The metric has also been applied to evaluate the similarity between generated actions and the ground truth in, which has been proved consistent with human evaluation Recognition Accuracy Apart from using MMD to evaluate the Table 1: Model comparisons in terms of MMD on Human-3.6m. Pretrain MMDavg \u2193 MMDseq \u2193 Table 2: Action recognition accuracy on the generated actions on the Direct Discuss Eat Greet Phone Pose Sit SitD Smoke Walk Average Table 3: Model comparisons in terms of MMD on NTU RGB+D. MMDavg \u2193 MMDseq \u2193 MMDavg \u2193 MMDseq \u2193 cross - subject cross - view Table 4: Ablation study results. Baselines MMDavg \u2193 MMDseq \u2193 Table 5: AMT Evaluations Evaluation Score\u2191 Table 5 : AMT Evaluations",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Human action generation\", \"Dataset\": \"Human3.6M\", \"Metric\": \"MMD\", \"Score\": \"0.134\"}}, {\"LEADERBOARD\": {\"Task\": \"Human action generation\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"MMD\", \"Score\": \"0.285\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Human action generation\", \"Dataset\": \"Human3.6M\", \"Metric\": \"MMD\", \"Score\": \"0.195\"}}, { \"LEADERBOARD\": { \"Task\": \"Human action generation\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"MMD\", \"Score\": \"0.338\"}}, { \"LEADERBOARD\": { \"Task\": \"Human action generation\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"MMD\", \"Score\": \"0.133\"}}, { \"LEADERBOARD\": { \"Task\": \"Human action generation\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"MMD\", \"Score\": \"0.44\"}}, { \"LEADERBOARD\": { \"Task\": \"Human action generation\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"MMD\", \"Score\": \"0.535\"}}, { \"LEADERBOARD\": { \"Task\": \"Human action generation\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"METEOR\", \"Score\": \"21.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Human action generation\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"Accuracy (%)\", \"Score\": \"86.5\"}}, { 'LEADERBOARD'}]"
    },
    {
        "Context": "Covering of ordinals The paper focuses on the structure of fundamental sequences of ordinals smaller than \u03b50. A first result is the construction of a monadic second-order formula identifying a given structure, whereas such a formula cannot exist for ordinals themselves. The structures are precisely classified in the pushdown hierarchy. Ordinals are also located in the hierarchy, and a direct presentation is given.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Peer to Peer Optimistic Collaborative Editing on XML-like Trees Collaborative editing consists in editing a common document shared by several independent sites. Conflicts occurs when different users perform simultaneous uncompatible operations. Centralized systems solve this problem by using locks that prevent some modifications to occur and leave the resolution of conflicts to users. Optimistic peer to peer (P2P) editing doesn't allow locks and uses a Integration Transformation IT that reconciliates conflicting operations and ensures convergence (all copies are identical on each site). Two properties TP1 and TP2, relating the set of allowed operations Op and the transformation IT, have been shown to ensure convergence. The choice of the set Op is crucial to define an integration operation that satisfies TP1 and TP2. Many existing algorithms don't satisfy these properties and are incorrect. No algorithm enjoying both properties is known for strings and little work has been done for XML trees in a pure P2P framework. We focus on editing XML-like trees, i.e. unranked-unordered labeled trees also considered in the Harmony project. We show that no transformation satisfying TP1 and TP2 exists fora first set of operations but that TP1 and TP2 hold fora richer set of operations, provided that some decoration is added to the tree. We show how to combine our approach with any convergent editing process on strings to get a convergent process. We have implemented our transformation using a P2P algorithm inspired by Ressel et al. whose correctness relies on underlying partial order structure generated by the dependence relation on operations.45EmileCaplantStreet P hone 0491... Add(Henri, P hone)(t \u2032 ) = t \u2032 since Henri.P hone already exists. -Nop() : Do nothing. Nop()(t) = t -Del 1 (p, n): Replace a edge labeled n at end of path p by the set of its successors.Del 1 (n \u2032 .p, n)(t) = t, if n \u2208 Dom(t) Del 1 (n i .p, n)({n 1 (t 1 ), ..., n i (t i ), ..., n q (t q )}) = {n 1 (t 1 ), ..., n i (Del 1 (p, n)(t i )), ..., n q (t q )} Del 1 (\u01eb, n)(t) = t, if n \u2208 Dom(t) Del 1 (\u01eb, n i )({n 1 (t 1 ), ..., n i (t i ), ..., n q (t q )}) = {n 1 (t 1 ), ..., n q (t q )} \u2295 ti",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Published as a conference paper at ICLR 2018 LEARNING LATENT REPRESENTATIONS IN NEURAL NETWORKS FOR CLUSTERING THROUGH PSEUDO SUPERVISION AND GRAPH-BASED ACTIVITY REGULARIZATION In this paper, we propose a novel unsupervised clustering approach exploiting the hidden information that is indirectly introduced through a pseudo classification objective. Specifically, we randomly assign a pseudo parent-class label to each observation which is then modified by applying the domain specific transformation associated with the assigned label. Generated pseudo observation-label pairs are subsequently used to train a neural network with Auto-clustering Output Layer (ACOL) that introduces multiple softmax nodes for each pseudo parent-class. Due to the unsupervised objective based on Graph-based Activity Regularization (GAR) terms, softmax duplicates of each parent-class are specialized as the hidden information captured through the help of domain specific transformations is propagated during training. Ultimately we obtain a k-means friendly latent representation. Furthermore, we demonstrate how the chosen transformation type impacts performance and helps propagate the latent information that is useful in revealing unknown clusters. Our results show state-of-the-art performance for unsupervised clustering tasks on MNIST, SVHN and USPS datasets, with the highest accuracies reported to date in the literature.Published as a conference paper at ICLR 2018 auxiliary target distribution derived from the soft cluster assignments. Similarly, Joint Unsupervised Learning (JULE) (Yang et al., 2016) combines agglomerative clustering with convolutional neural networks (CNN) and formulates them as a recurrent process. Although JULE proposes an end-to-end learning framework, it suffers scalability issues due to its agglomerative clustering. Open source code is available at http://github.com/ozcell/LALNets that can be used to reproduce the experimental results obtained on three benchmark image datasets,, SVHN and USPS Specifications of these datasets are presented in To ensure that the representation obtained through the proposed approach is well-generalized for never-seen-before data, we train the neural network parameters using only the training set examples of each dataset and obtain the clustering performances using k-means with k = 10 on the latent space representation F of the untransformed test set examples (through T 1 )., we evaluate the test performances using unsupervised clustering accuracy given as where t * i is the ground-truth label, y i is the assigned cluster, and F is the set of all possible one-to-one mappings between assignments and labels Table 1. Number of classes Dimension Table 1: Datasets used in the experiments. Number of classes Dimension Table 2: Specifications of the CNN model used in the experiments. 6 - layer CNN Model name Table 3: Quantitative unsupervised clustering performance (ACC) on MNIST, USPS and SVHN datasets. Results of a broad range of recent existing solutions are also presented for comparison. The last row demonstrates the benchmark scores of the proposed framework in this article. - - 11 . 9% ( \u00b10 . 40 ) \u2020 \u2020 MNIST - test 87 . 82% ( \u00b15 . 33 ) 82 . 31% ( \u00b13 . 75 ) 92 . 77% ( \u00b11 . 60 ) 98 . 40% ( \u00b10 . 40 ) k 95 . 90% ( \u00b11 . 13 ) USPS - full \u2020 - 90 . 45% ( \u00b12 . 05 )",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Unsupervised Image Classification\", \"Dataset\": \"SVHN\", \"Metric\": \"Acc\", \"Score\": \"76.80\"}}, {\"LEADERBOARD\": {\"Task\": \"Unsupervised Image Classification\", \"Dataset\": \"MNIST\", \"Metric\": \"Accuracy\", \"Score\": \"98.32\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"MNIST-full\", \"Metric\": \"Accuracy\", \"Score\": \"0.965\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"MNIST-full\", \"Metric\": \"NMI\", \"Score\": \"0.913\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"MNIST-test\", \"Metric\": \"Accuracy\", \"Score\": \"0.967\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"MNIST-test\", \"Metric\": \"NMI\", \"Score\": \"0.919\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"Fashion-MNIST\", \"Metric\": \"Accuracy\", \"Score\": \"0.628\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"Fashion-MNIST\", \"Metric\": \"NMI\", \"Score\": \"0.644\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"USPS\", \"Metric\": \"Accuracy\", \"Score\": \"0.957\"}}, { \"LEADERBOARD\": { "
    },
    {
        "Context": "EfficientNetV2: Smaller Models and Faster Training This paper introduces EfficientNetV2, anew family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller.Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop inaccuracy. To compensate for this accuracy drop, we propose an improved method of progressive learning, which adaptively adjusts regularization (e.g., dropout and data augmentation) along with image size.With progressive learning, our EfficientNetV2 significantly outperforms previous models on Im-ageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our Effi-cientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while training 5x-11x faster using the same computing resources. Code will be available at https Setup: We evaluate our models on four transfer learning datasets: CIFAR-10, CIFAR-100, Flowers and Cars includes the statistics of these datasets For all datasets, we train each model for fixed 10,000 steps In general, our models outperform previous Con-vNets and Vision Transformers for all these datasets, sometimes by a non-trivial margin: for example, on CIFAR-100, EfficientNetV2-L achieves 0.6% better accuracy than prior GPipe/EfficientNets and 1.5% better accuracy than prior ViT/DeiT models Table 1. EfficientNets have good parameter and FLOPs efficiency. 192M 43M Top - 1 Acc . Table 2. EfficientNet-B6 accuracy and training throughput for dif- ferent batch sizes and image size. OOM batch=24 batch=12 TPUv3 imgs / sec / core batch=32 batch=128 V100 imgs / sec / gpu Top - 1 Acc . Table 3. Replacing MBConv with Fused-MBConv. No fused denotes all stages use MBConv, Fused stage1-3 denotes re- placing MBConv with Fused-MBConv in stage {2, 3, 4}. ( B ) FLOPs imgs / sec / core TPU Params V100 ( M ) Top - 1 Acc . imgs / sec / gpu Table 4. EfficientNetV2-S architecture -MBConv and Fused- MBConv blocks are described in Figure 2. Stride 15 Stage #Layers 272 Table 5. ImageNet top-1 accuracy. We use RandAug Size=128 Size=192 Size=300 Table 6. Progressive training settings for EfficientNetV2. 25 S min 300 15 max 5 128 380 L",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of params\", \"Score\": \"121M\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"86.8%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of params\", \"Score\": \"55M\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"86.1%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"85.7%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"85.1%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of params\", \"Score\": \"24M\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"85.0%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"83.9%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"Stanford Cars\", \"Metric\": \"Accuracy\", \"Score\": \"95.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"Stanford Cars\", \"Metric\": \"Accuracy\", \"Score\": \"94.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"Stanford Cars\", \"Metric\": \"Accuracy\", \"Score\": \"93.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"Flowers-102\", \"Metric\": \"Accuracy\", \"Score\": \"98.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"Flowers-102\", \"Metric\": \"Accuracy\", \"Score\": \"98.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"Flowers-102\", \"Metric\": \"Accuracy\", \"Score\": \"97.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-100\", \"Metric\": \"Percentage correct\", \"Score\": \"92.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-100\", \"Metric\": \"Percentage correct\", \"Score\": \"92.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-100\", \"Metric\": \"Percentage correct\", \"Score\": \"91.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"PARAMS\", \"Score\": \"121M\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Percentage correct\", \"Score\": \"99.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"PARAMS\", \"Score\": \"55M\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Percentage correct\", \"Score\": \"99.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"PARAMS\", \"Score\": \"24M\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Percentage correct\", \"Score\": \"98.7\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"78.35%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"94.83%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"77.21%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"93.57%\"}} ]"
    },
    {
        "Context": "A Bi-model based RNN Semantic Frame Parsing Model for Intent Detection and Slot Filling Intent detection and slot filling are two main tasks for building a spoken language understanding(SLU) system. Multiple deep learning based models have demonstrated good results on these tasks . The most effective algorithms are based on the structures of sequence to sequence models (or \"encoder-decoder\" models), and generate the intents and semantic tags either using separate models ((Yao et al., 2014;Mesnil et al., 2015;Peng and Yao, 2015;Kurata et al., 2016;Hahn et al., 2011)) or a joint model ((Liu and Lane, 2016a;Hakkani-T\u00fcr et al., 2016;Guo et al., 2014)). Most of the previous studies, however, either treat the intent detection and slot filling as two separate parallel tasks, or use a sequence to sequence model to generate both semantic tags and intent. Most of these approaches use one (joint) NN based model (including encoderdecoder structure) to model two tasks, hence may not fully take advantage of the crossimpact between them. In this paper, new Bi-model based RNN semantic frame parsing network structures are designed to perform the intent detection and slot filling tasks jointly, by considering their cross-impact to each other using two correlated bidirectional LSTMs (BLSTM). Our Bi-model structure with a decoder achieves state-of-the-art result on the benchmark ATIS data (Hemphill et al., 1990;Tur et al., 2010), with about 0.5% intent accuracy improvement and 0.9 % slot filling improvement. In this section, our new proposed Bi-model structures are trained and tested on two datasets, one is the public ATIS dataset containing audio recordings of flight reservations, and the other is our self-collected datset in three different domains: Food, Home and Movie The ATIS dataset used in this paper follows the same format as in The number of data for our self-collected dataset will be given in the corresponding experiment sections with a more detailed explanation The performance is evaluated based on the classification accuracy for intent detection task and F1-score for slot filling task Our first experiment is conducted on the ATIS benchmark dataset, and compared with the current existing approaches, by evaluating their intent detection accuracy and slot filling F1 scores It can be observed that the new proposed Bi-model structures outperform the current state-of-the-art results on both intent detection and slot filling tasks, and the Bi-model with a Table 1: Performance of Different Models on ATIS Dataset ( Guo et al . , 2014 ) ( Peng and Yao , 2015 ) ( Liu and Lane , 2015 ) ( Liu and Lane , 2016b ) ( Xu and Sarikaya , 2013 ) ( Liu and Lane , 2016a ) ( Mesnil et al . , 2015 ) ( Kurata et al . , 2016 ) ( Zhang and Wang , 2016 ) F1 Score Intent Accuracy Table 1. Some of the models are designed for single slot filling task, hence only F1 scores are given. It can be observed that the new proposed Bi-model structures outperform the current state-of-the-art results on both intent detection and slot filling tasks, and the Bi-model with a decoder also outperform that without a decoder on our ATIS dataset. The current Bi-model with a decoder shows the state-of-the-art performance on ATIS benchmark dataset",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Slot Filling\", \"Dataset\": \"ATIS\", \"Metric\": \"F1\", \"Score\": \"0.969\"}}, {\"LEADERBOARD\": {\"Task\": \"Intent Detection\", \"Dataset\": \"ATIS\", \"Metric\": \"Accuracy\", \"Score\": \"98.99\"}}, {\"LEADERBOARD\": {\"Task\": \"Intent Detection\", \"Dataset\": \"ATIS\", \"Metric\": \"F1\", \"Score\": \"96.89\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Slot Filling\", \"Dataset\": \"ATIS\", \"Metric\": \"F1\", \"Score\": \"0.958\"}}, { \"LEADERBOARD\": { \"Task\": \"Intent Detection\", \"Dataset\": \"SNIPS\", \"Metric\": \"Intent Accuracy\", \"Score\": \"97.43\"}}, { \"LEADERBOARD\": { \"Task\": \"Intent Detection\", \"Dataset\": \"SNIPS\", \"Metric\": \"Slot F1 Score\", \"Score\": \"91.46\"}}, { \"LEADERBOARD\": { \"Task\": \"Intent Detection\", \"Dataset\": \"ATIS\", \"Metric\": \"Accuracy\", \"Score\": \"95.76\"}}, { \"LEADERBOARD\": { \"Task\": \"Intent Detection\", \"Dataset\": \"ATIS\", \"Metric\": \"F1\", \"Score\": \"95.80\"}} ]"
    },
    {
        "Context": "Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis In this paper we propose Flowtron: an autoregressive flow-based generative network for textto-speech synthesis with control over speech variation and style transfer. Flowtron borrows insights from IAF and revamps Tacotron in order to provide high-quality and expressive melspectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be manipulated to control many aspects of speech synthesis (pitch, tone, speech rate, cadence, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. In addition, we provide results on control of speech variation, interpolation between samples and style transfer between speakers seen and unseen during training. Code and pretrained models will be made publicly available at https://github.com/NVIDIA/flowtron.  Table 1: Mean Opinion Score (MOS) evaluations with 95% confidence intervals for various sources. manipulation of the latent space . with 95% con - Mean Opinion Score ( MOS )",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Text-To-Speech Synthesis\", \"Dataset\": \"LJSpeech\", \"Metric\": \"Pleasantness MOS\", \"Score\": \"3.665\"}}, {\"LEADERBOARD\": {\"Task\": \"Text-To-Speech Synthesis\", \"Dataset\": \"LJSpeech\", \"Metric\": \"Pleasantness MOS\", \"Score\": \"3.521\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Speech Enhancement\", \"Dataset\": \"DEMAND\", \"Metric\": \"CBAK\", \"Score\": \"3.18\"}}, { \"LEADERBOARD\": { \"Task\": \"Speech Enhancement\", \"Dataset\": \"DEMAND\", \"Metric\": \"COVL\", \"Score\": \"2.96\"}}, { \"LEADERBOARD\": { \"Task\": \"Speech Enhancement\", \"Dataset\": \"DEMAND\", \"Metric\": \"CSIG\", \"Score\": \"3.52\"}}, { \"LEADERBOARD\": { \"Task\": \"Speech Enhancement\", \"Dataset\": \"DEMAND\", \"Metric\": \"PESQ\", \"Score\": \"2.4\"}} ]"
    },
    {
        "Context": "Synthetic Training for Accurate 3D Human Pose and Shape Estimation in the Wild This paper addresses the problem of monocular 3D human shape and pose estimation from an RGB image. Despite great progress in this field in terms of pose prediction accuracy, state-of-the-art methods often predict inaccurate body shapes. We suggest that this is primarily due to the scarcity of in-the-wild training data with diverse and accurate body shape labels. Thus, we propose STRAPS (Synthetic Training for Real Accurate Pose and Shape), a system that utilises proxy representations, such as silhouettes and 2D joints, as inputs to a shape and pose regression neural network, which is trained with synthetic training data (generated on-the-fly during training using the SMPL statistical body model) to overcome data scarcity. We bridge the gap between synthetic training inputs and noisy real inputs, which are predicted by keypoint detection and segmentation CNNs at test-time, by using data augmentation and corruption during training. In order to evaluate our approach, we curate and provide a challenging evaluation dataset for monocular human shape estimation, Sports Shape and Pose 3D (SSP-3D). It consists of RGB images of tightly-clothed sports-persons with a variety of body shapes and corresponding pseudo-ground-truth SMPL shape and pose parameters, obtained via multi-frame optimisation. We show that STRAPS outperforms other state-of-the-art methods on SSP-3D in terms of shape prediction accuracy, while remaining competitive with the state-of-the-art on pose-centric datasets and metrics. Evaluations are carried outwith two types of input proxy representations: synthetic silhouettes and 2D joints generated from GT SMPL labels and \"real\" silhouettes and 2D joints predicted from test RGB images using DensePose Results from SPIN, CMR and HMR are shown for comparison SSP-3D evaluates 3D shape prediction across a diverse range of body shapes, while Human 3.6M and 3DPW evaluate 3D pose prediction Incorporating shape augmentation alleviates the first problem, since the network sees a greater variety of shapes during training Incorporating PR augmentation shrinks the performance deterioration when using real versus synthetic inputs by explicitly modelling input noise and occlusion during the synthetic training process By combining PR and shape augmentation, we are able to predict a diverse range of body shapes, improve our pose accuracy significantly over the baseline and produce semantically-plausible outputs on all datasets, even when the input is heavily corrupted (see) The dis-, suggests that",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"3DPW\", \"Metric\": \"PA-MPJPE\", \"Score\": \"66.8\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"3DPW\", \"Metric\": \"MPJPE\", \"Score\": \"72.7\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"3DPW\", \"Metric\": \"MPVPE\", \"Score\": \"88.2\"}} ]"
    },
    {
        "Context": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this paper, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly-available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation fora wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition (NER). To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark  Table 3. Datasets used in the BLURB biomedical NLP benchmark. We list the numbers of instances in train, dev, and test (e.g., entity mentions in NER and PICO elements in evidence-based medical information extraction). F1 entity - level Micro F1 16364 18035 339167 960 Dev Test 787 535 85321 Train 11268 64 15745 Table 6. Comparison of pretrained language models on the BLURB biomedical NLP benchmark. The standard task-specific models are used in the same fine-tuning process for all BERT models. The BLURB score is the macro average of average test results for each of the six tasks (NER, PICO, relation extraction, sentence similarity, document classification, question answering). See PubMedBERT RoBERTa uncased cased ClinicalBERT SciBERT BlueBERT BioBERT Table 7. Evaluation of the impact of vocabulary and whole word masking on the performance of PubMedBERT on BLURB. Word Piece Wiki + Books Whole Word PubMed Table 8. Comparison of the average input",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Relation Extraction\", \"Dataset\": \"GAD\", \"Metric\": \"Micro F1\", \"Score\": \"82.34\"}}, {\"LEADERBOARD\": {\"Task\": \"Relation Extraction\", \"Dataset\": \"ChemProt\", \"Metric\": \"Micro F1\", \"Score\": \"77.24\"}}, {\"LEADERBOARD\": {\"Task\": \"Relation Extraction\", \"Dataset\": \"DDI\", \"Metric\": \"Micro F1\", \"Score\": \"82.36\"}}, {\"LEADERBOARD\": {\"Task\": \"Question Answering\", \"Dataset\": \"PubMedQA\", \"Metric\": \"Accuracy\", \"Score\": \"55.84\"}}, {\"LEADERBOARD\": {\"Task\": \"Question Answering\", \"Dataset\": \"BioASQ\", \"Metric\": \"Accuracy\", \"Score\": \"87.56\"}}, {\"LEADERBOARD\": {\"Task\": \"Named Entity Recognition\", \"Dataset\": \"BC5CDR-chemical\", \"Metric\": \"F1\", \"Score\": \"94.06\"}}, {\"LEADERBOARD\": {\"Task\": \"Named Entity Recognition\", \"Dataset\": \"NCBI Disease\", \"Metric\": \"F1\", \"Score\": \"88.81\"}}, {\"LEADERBOARD\": {\"Task\": \"Named Entity Recognition\", \"Dataset\": \"BC5CDR-disease\", \"Metric\": \"F1\", \"Score\": \"86.63\"}}, {\"LEADERBOARD\": {\"Task\": \"Named Entity Recognition\", \"Dataset\": \"JNLPBA\", \"Metric\": \"F1\", \"Score\": \"81.36\"}}, {\"LEADERBOARD\": {\"Task\": \"Named Entity Recognition\", \"Dataset\": \"BC2GM\", \"Metric\": \"F1\", \"Score\": \"87.81\"}}, {\"LEADERBOARD\": {\"Task\": \"Document Classification\", \"Dataset\": \"HoC\", \"Metric\": \"Micro F1\", \"Score\": \"82.32\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"ChemProt\", \"Metric\": \"F1\", \"Score\": \"77.0\"}}, { \"LEADERBOARD\": { \"Task\": \"Natural Language Inference\", \"Dataset\": \"MedNLI\", \"Metric\": \"F1\", \"Score\": \"84\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Similarity\", \"Dataset\": \"MedSTS\", \"Metric\": \"Pearson Correlation\", \"Score\": \"0.948\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Similarity\", \"Dataset\": \"BIOSSES\", \"Metric\": \"Pearson Correlation\", \"Score\": \"0.9159999999999999\"}}, { \"LEADERBOARD\": { \"Task\": \"Named Entity Recognition\", \"Dataset\": \"CoNLL 2003 (English)\", \"Metric\": \"F1\", \"Score\": \"91.82\"}}, { \"LEADERBOARD\": { \"Task\": \"Named Entity Recognition\", \"Dataset\": \"BC5CDR-chemical\", \"Metric\": \"F1\", \"Score\": \"93.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Named Entity Recognition\", \"Dataset\": \"BC5CDR-disease\", \"Metric\": \"F1\", \"Score\": \"8}}]"
    },
    {
        "Context": "Temporal Smoothing for 3D Human Pose Estimation and Localization for Occluded People In multi-person pose estimation actors can be heavily occluded, even become fully invisible behind another person. While temporal methods can still predict a reasonable estimation fora temporarily disappeared pose using past and future frames, they exhibit large errors nevertheless. We present an energy minimization approach to generate smooth, valid trajectories in time, bridging gaps in visibility. We show that it is better than other interpolation based approaches and achieves state of the art results. In addition, we present the synthetic MuCo-Temp dataset, a temporal extension of the MuCo-3DHP dataset. Our code is made publicly available. 1 We evaluated our method on the multi-person 3D pose dataset MuPoTS-3D Therefore, we created the MuCo-Temp synthetic dataset The latter dataset was recorded in a green-screen studio, so segmenting of the actors is easy We did not augmented the background, as the 2D pose estimator was already trained on a visually diverse dataset Our method was trained on the concatenation of the MPI-INF-3DHP and MuCo-Temp datasets Table 1. Comparison with state-of-the-art MPJPE and MRPE errors are in mm. * Non-temporal methods. \u2020 Error is calculated on detected frames only. 3D - PCK MRPE MPJPE Table 2. Comparison with baselines Interpolation uses simple linear interpolation for unseen poses. 1-Euro applies a 1-Euro filter on interpolated poses. (N-)MPJPE and MRPE errors are in mm. N - MPJPE N - MRPE MRPE MPJPE 3D - PCK Table 3. Results of ablation studies. a) Results when components are turned on sequentially. b) Errors calculated on visible poses only. b ) Results on visible poses a ) Performance of components N - MPJPE N - MRPE MRPE MPJPE 3D - PCK",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"3D Multi-Person Pose Estimation (root-relative)\", \"Dataset\": \"MuPoTS-3D\", \"Metric\": \"3DPCK\", \"Score\": \"85.3\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Multi-Person Pose Estimation (root-relative)\", \"Dataset\": \"MuPoTS-3D\", \"Metric\": \"MPJPE\", \"Score\": \"103\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Multi-Person Pose Estimation\", \"Dataset\": \"MuPoTS-3D\", \"Metric\": \"3DPCK\", \"Score\": \"85.3\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Multi-Person Pose Estimation (root-relative)\", \"Dataset\": \"MuPoTS-3D\", \"Metric\": \"MPJPE\", \"Score\": \"89.2\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Multi-Person Pose Estimation (absolute)\", \"Dataset\": \"MuPoTS-3D\", \"Metric\": \"MPJPE\", \"Score\": \"88.2\"}} ]"
    },
    {
        "Context": "A FUZZY COMMITMENT SCHEME This paper attempt has been made to explain a fuzzy commitment scheme. In the conventional Commitment schemes, both committed string m and valid opening key are required to enable the sender to prove the commitment. However there could be many instances where the transmission involves noise or minor errors arising purely because of the factors over which neither the sender nor the receiver have any control. The fuzzy commitment scheme presented in this paper is to accept the opening key that is close to the original one in suitable distance metric, but not necessarily identical. The concept itself is illustrated with the help of simple situation.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "The Incidence Chromatic Number of Toroidal Grid\u015b An incidence in a graph G is a pair (v, e) with v \u2208 V (G) and e \u2208 E(G), such that v and e are incident. Two incidences (v, e) and (w, f ) are adjacent if v = w, ore = f , or the edge vw equals e or f . The incidence chromatic number of G is the smallest k for which there exists a mapping from the set of incidences of G to a set of k colors that assigns distinct colors to adjacent incidences.In this paper, we prove that the incidence chromatic number of the toroidal grid T m,n = Cm \u2737C n equals 5 when m, n \u2261 0 (mod 5) and 6 otherwise.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Multi-view Convolutional Neural Networks for 3D Shape Recognition A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.  Table 1. Classification and retrieval results on the ModelNet40 dataset. On the top are results using state-of-the-art 3D shape descriptors. Our view-based descriptors including Fisher vectors (FV) significantly outperform these even when a single view is available at test time (#Views = 1). When multiple views (#Views=12 or 80) are available at test time, the performance of view-based methods improve significantly. The multi-view CNN (MVCNN) architecture outperforms the view-based methods, especially for retrieval. Method Retrieval Classification ( Accuracy ) #Views ( mAP )",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"3D Point Cloud Classification\", \"Dataset\": \"ModelNet40\", \"Metric\": \"Overall Accuracy\", \"Score\": \"90.1\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Object Recognition\", \"Dataset\": \"ModelNet40\", \"Metric\": \"Accuracy\", \"Score\": \"33.5\"}} ]"
    },
    {
        "Context": "Meta Pseudo Labels We present Meta Pseudo Labels, a semi-supervised learning method that achieves anew state-of-the-art top-1 accuracy of 90.2% on ImageNet, which is 1.6% better than the existing state-of-the-art [16]. Like Pseudo Labels, Meta Pseudo Labels has a teacher network to generate pseudo labels on unlabeled data to teach a student network. However, unlike Pseudo Labels where the teacher is fixed, the teacher in Meta Pseudo Labels is constantly adapted by the feedback of the student's performance on the labeled dataset. As a result, the teacher generates better pseudo labels to teach the student. 1 We first study the role of feedback in Meta Pseudo Labels on the simple TwoMoon dataset We conclude the section with experiments on the standard ResNet-50 architecture with the full ImageNet dataset To understand the role of feedback in Meta Pseudo Labels, we conduct an experiment on the simple and classic TwoMoon dataset The 2D nature of the TwoMoon dataset allows us to visualize how Meta Pseudo Labels behaves compared to Supervised Learning and Pseudo Labels We randomly generate the TwoMoon dataset fora few times and repeat the three methods: Supervised Learning, Pseudo Labels, and Meta Pseudo Labels As a result, Meta Pseudo Labels finds a good classifier for this dataset Datasets We directly compare Meta Pseudo Labels against two baselines: Supervised Learning with full dataset and Unsupervised Data Augmentation (UDA) Supervised Learning with full dataset represents the headroom because it unfairly makes use of all labeled data (e.g., for CIFAR-10, Table 1: Summary of our key results on ImageNet ILSVRC 2012 validation set Top - 1 Accuracy Precision@1 Table 2: Image classification accuracy on CIFAR-10-4K, SVHN-1K, and ImageNet-10%. Higher is better. For CIFAR-10-4K and SVHN- 1K, we report mean \u00b1 std over 10 runs, while for ImageNet-10%, we report Top-1/Top-5 accuracy of a single run. For fair comparison, we only include results that share the same model architecture: WideResNet-28-2 for CIFAR-10-4K and SVHN-1K, and ResNet-50 for ImageNet-10%. * indicates our implementation which uses the same experimental protocols. Except for UDA, results in the first two blocks are from representative important papers, and hence do not share the same controlled environment with ours. \u2212 Method CIFAR - 10 - 4K SVHN - 1K ( mean \u00b1 std ) ImageNet - 10% Top - 5 Top - 1 Table 4: Top-1 and Top-5 accuracy of Meta Pseudo Labels and previous state-of-the-art methods on",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet ReaL\", \"Metric\": \"Accuracy\", \"Score\": \"91.12%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet ReaL\", \"Metric\": \"Accuracy\", \"Score\": \"91.02%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of params\", \"Score\": \"480M\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"90.2%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"98.8%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of params\", \"Score\": \"390M\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"90%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"98.7%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"83.2%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"96.5%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 10% labeled data\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"73.89%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 10% labeled data\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"91.38%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"SVHN, 1000 labels\", \"Metric\": \"Accuracy\", \"Score\": \"98.01\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"CIFAR-10, 4000 Labels\", \"Metric\": \"Accuracy\", \"Score\": \"96.11\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"78.95%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"92.21%\"}} ]"
    },
    {
        "Context": "Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training The width of a neural network matters since increasing the width will necessarily increase the model capacity. However, the performance of a network does not improve linearly with the width and soon gets saturated. In this case, we argue that increasing the number of networks (ensemble) can achieve better accuracy-efficiency trade-offs than purely increasing the width. To prove it, one large network is divided into several small ones regarding its parameters and regularization components. Each of these small networks has a fraction of the original one's parameters. We then train these small networks together and make them see various views of the same data to increase their diversity. During this co-training process, networks can also learn from each other. As a result, small networks can achieve better ensemble performance than the large one with few or no extra parameters or FLOPs. Small networks can also achieve faster inference speed than the large one by concurrent running on different devices. We validate our argument with 8 different neural architectures on common benchmarks through extensive experiments. The code is available at https://github. com/mzhaoshuai/Divide-and-Co-training. Datasets We adopt CIFAR-10, CIFAR-100, and Im-ageNet 2012 datasets CIFAR-10 and CIFAR-100 datasets contain 50K training and 10K test RGB images of size 32\u00d732, labeled with 10 and 100 classes, respectively ImageNet 2012 dataset contains 1.28 million training images and 50K validation images from 1000 classes For {EfficientNet-B3, ResNeXt-29 (8\u00d764d), WRN-28-10, WRN-40-10} on CIFAR datasets, wd =5e-4 Results on CIFAR-100 dataset Results on CIFAR-100 are shown in Table 1: Influence of various settings of ResNet-110 on CIFAR-100. step-lr means step learning rate decay policy as described in ResNet of some settings is shown in Table 1 . The baseline is solid . mixup Top - 1 err . ( % ) random erasing Method ResNet variants adopt the modifications introduced of some settings is shown in Table 1 . The baseline is solid . mixup Top - 1 err . ( % ) random erasing Method ResNet variants adopt the modifications introduced Table 2. Dividing and co-training achieve consistent improvements with few extra or even fewer parameters or FLOPs. Additional cost occurs since the division of a network is not perfect, as mentioned in Sec. 3.1. Some conclusions can be drawn from the data. of some settings is shown in Table 1 . The baseline is solid . mixup Top - 1 err . ( % ) random",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"83.34%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"96.61%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"82.13%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"95.98%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-100\", \"Metric\": \"PARAMS\", \"Score\": \"32.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-100\", \"Metric\": \"Percentage correct\", \"Score\": \"89.46\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-100\", \"Metric\": \"PARAMS\", \"Score\": \"26.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-100\", \"Metric\": \"Percentage correct\", \"Score\": \"87.44\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-100\", \"Metric\": \"Percentage correct\", \"Score\": \"86.90\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-100\", \"Metric\": \"Percentage correct\", \"Score\": \"85.74\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"PARAMS\", \"Score\": \"32.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Percentage correct\", \"Score\": \"98.71\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Percentage correct\", \"Score\": \"98.38\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"PARAMS\", \"Score\": \"36.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Percentage correct\", \"Score\": \"98.32\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"PARAMS\", \"Score\": \"26.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Percentage correct\", \"Score\": \"98.31\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"78.35%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"94.35%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"77.21%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"93.57%\"}} ]"
    },
    {
        "Context": "Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model We present a novel training framework for neural sequence models, particularly for grounded dialog generation. The standard training paradigm for these models is maximum likelihood estimation (MLE), or minimizing the cross-entropy of the human responses. Across a variety of domains, a recurring problem with MLE trained generative neural dialog models (G) is that they tend to produce 'safe' and generic responses ('I don't know', 'I can't tell'). In contrast, discriminative dialog models (D) that are trained to rank a list of candidate human responses outperform their generative counterparts; in terms of automatic metrics, diversity, and informativeness of the responses. However, Dis not useful in practice since it cannot be deployed to have real conversations with users. Our work aims to achieve the best of both worlds -the practical usefulness of G and the strong performance of D -via knowledge transfer from D to G. Our primary contribution is an end-to-end trainable generative visual dialog model, where G receives gradients from D as a perceptual (not adversarial) loss of the sequence sampled from G. We leverage the recently proposed Gumbel-Softmax (GS) approximation to the discrete distribution -specifically, a RNN augmented with a sequence of GS samplers, coupled with the straight-through gradient estimator to enable end-to-end differentiability. We also introduce a stronger encoder for visual dialog, and employ a self-attention mechanism for answer encoding along with a metric learning loss to aid D in better capturing semantic similarities in answer responses. Overall, our proposed model outperforms state-of-the-art on the VisDial dataset by a significant margin (2.67% on recall@10). The source code can be downloaded from https://github.com/jiasenlu/visDial.pytorch * Work was done while at Facebook AI Research. Dataset and Setup We evaluate our proposed approach on the VisDial dataset, which was collected by Das et al Evaluation Protocol Following the evaluation protocol established in, we use a retrieval setting to evaluate the responses at each round in the dialog Specifically, every question in VisDial is coupled with a list of 100 candidate answer options, which the models are asked to sort for evaluation purposes Models are evaluated on standard retrieval metrics -(1) mean rank, (2) recall @k, and (3) mean reciprocal rank (MRR) -of the human response in the returned sorted list Table 1: Results (generative) on VisDial dataset. \"MRR\" is mean reciprocal rank and \"Mean\" is mean rank. Model MRR R@1 R@5 R@10 Mean is mean reciprocal rank and \" Mean \" is mean rank . Table 1 : Results ( generative ) on VisDial dataset . \" MRR \" R@1 MRR R@5 R@10 Mean Table 2: Results (discriminative) on VisDial dataset. R@1 MRR R@5 R@10 Mean Table 4: Adversarial training results on VisDial dataset. Mean R@1 Generative Discriminative R@5 R@10 MRR -",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"MRR\", \"Score\": \"62.22\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"Mean Rank\", \"Score\": \"4.81\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"R@1\", \"Score\": \"48.48\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"R@10\", \"Score\": \"87.59\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"R@5\", \"Score\": \"78.75\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"MRR\", \"Score\": \"0.6285\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"Mean Rank\", \"Score\": \"4.57\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"R@1\", \"Score\": \"48.95\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"R@10\", \"Score\": \"88.36\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"R@5\", \"Score\": \"79.65\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Dialog\", \"Dataset\": \"Visual Dialog v1.0 test-std\", \"Metric\": \"MRR (x 100)\", \"Score\": \"61.37\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Dialog\", \"Dataset\": \"Visual Dialog v"
    },
    {
        "Context": "Type-Driven Incremental Semantic Parsing with Polymorphism Semantic parsing has made significant progress, but most current semantic parsers are extremely slow (CKY-based) and rather primitive in representation. We introduce three new techniques to tackle these problems. First, we design the first linear-time incremental shift-reduce-style semantic parsing algorithm which is more efficient than conventional cubic-time bottom-up semantic parsers. Second, our parser, being type-driven instead of syntax-driven, uses type-checking to decide the direction of reduction, which eliminates the need fora syntactic grammar such as CCG. Third, to fully exploit the power of type-driven semantic parsing beyond simple types (such as entities and truth values), we borrow from programming language theory the concepts of subtype polymorphism and parametric polymorphism to enrich the type system in order to better guide the parsing. Our system learns very accurate parses in GEOQUERY, JOBS and ATIS domains. We implement our type-driven incremental semantic parser (TISP) using Python, and evaluate its performance of both speed and accuracy on GEOQUERY and JOBS datasets For evaluation, we follow We first evaluate TISP on GEOQUERY dataset Soto find the best number of iterations to stop the training, we do a 10-fold cross-validation training over the training set, and choose to train 20 iterations and then evaluate This is actually because our method parses a lot more questions in the dataset, as the column of the percentage of successfully parsed sentences suggests We also evaluate the performance of TISP on ATIS dataset as in ATIS dataset contains more than 5,000 examples and is a lot larger than GEOQUERY and JOBS Our method achieves comparable performance on this dataset use \"on-the-fly\" matching to fetch the most possible predicate in the dataset for some MR subexpression Table 1: Performances (precision, recall, and F1) of various parsing algorithms on GEOQUERY, JOBS, and ATIS datasets. : \u03bb-WASP for GEOQUERY is trained on 792 examples. P R ATIS JOBS GEOQUERY F1 -",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semantic Parsing\", \"Dataset\": \"ATIS\", \"Metric\": \"Accuracy\", \"Score\": \"84.2\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Parsing\", \"Dataset\": \"ATIS\", \"Metric\": \"Accuracy\", \"Score\": \"85.3\"}} ]"
    },
    {
        "Context": "THE RELATIONAL MODEL IS INJECTIVE FOR MULTIPLICATIVE EXPONENTIAL LINEAR LOGIC (WITHOUT WEAKENINGS) We show that for Multiplicative Exponential Linear Logic (without weakenings) the syntactical equivalence relation on proofs induced by cut-elimination coincides with the semantic equivalence relation on proofs induced by the multiset based relational model: one says that the interpretation in the model (or the semantics) is injective. We actually prove a stronger result: two cut-free proofs of the full multiplicative and exponential fragment of linear logic whose interpretations coincide in the multiset based relational model are the same \"up to the connections between the doors of exponential boxes\". 1 terms) when they have the same interpretation. It is worth noticing that the study of both these equivalence relations is at the heart of the whole research area between proof-theory and theoretical computer science: cut-elimination is a crucial property of logical systems since Gentzen (with a renewal of interest in this property after the discovery of the Curry-Howard correspondence: a proof is a program whose execution corresponds to applying the cut-elimination procedure to the proof) and the general goal of denotational semantics is to give a \"mathematical\" counterpart to syntactical devices such as proofs and programs, bringing to the fore their essential properties. The basic pattern is to associate with every formula/type an object of some category and with every proof/program a morphism of this category (its interpretation).The works [12] and [13] give partial results and counterexamples to the question of injectivity, mainly for the (multiset based) coherent model: in particular the counterexamples show that this model is not injective for multiplicative and exponential LL (M ELL). Also, it was conjectured that the (multiset based) relational model is injective for M ELL, but despite many efforts ([12], [13], [14], [15], [9], [16]...) all the attempts to prove the conjecture failed up to now: no real progress has been done since [13], where a proof of injectivity of the relational model is given fora fragment of M ELL 1 . Game semantics is much closer to syntax than relational and coherent semantics, and positive answers have been obtained for little fragments like the multiplicative fragment M LL or the fragment corresponding to the \u03bb-calculus ([17], [18]), but also for the polarized fragment of LL ([19]).We prove here that for M ELL without weakenings (and without the multiplicative unit \u22a5) relational semantics is injective (Corollary 3). This tremendous improvement w.r.t. the previous situation is an immediate consequence of a much stronger result: in the full M ELL fragment (with units) two proof-nets Rand R \u2032 with the same interpretation are the same \"up to the connections between the doors of exponential boxes\" (we say they have the same LPS: Theorem 1 and Corollary 1). This result can be expressed in terms of differential nets ([20]): two cut-free proof-nets with different LPS have different Taylor expansions. We also believe this work is an essential step towards the proof of the full conjecture.In the style of [21] and [22] we work in an untyped framework; we do not define (proof-)nets nor cut-elimination but only cut-free proof-structures (PS, Definition 13): we prove that two PS with the same interpretation have the same LPS (Corollary 1). A (proof-)net (as defined in [22]) is a particular case of PS so that the result holds for untyped (so as for typed) M ELL (proof-)nets (Remark 6). Since we want to prove that two PS are isomorphic in Theorem 1, it is mandatory to have a (simple and clear) notion of isomorphism between PS (Definition 15) 2 , and this is why in Section 2 we give a very sharp description of the syntax in the style of interaction nets ([23], [24]): we cannot only rely on a graphic intuition. The notion of Linear Proof-Structure (LPS), which comes from [13], is our main syntactical tool: with every (proof-)net R of (say) [22] is associated a LPS, which is obtained from R by forgetting some informations about R's exponential boxes, namely which auxiliary doors correspond to which !-link (using standard LL's terminology); this is particularly clear in Definition 13 of PS: a PS is a LPS and a function allowing to recover boxes. Recovering this function from the interpretation of a PS is the only missing point in the proof of the full conjecture, but a simple remark shows that the function can be recovered from the LPS when the PS is a connected graph: this yields injectivity for M ELL without 1 Precisely, for the (?\u2118)LL fragment given by A ::= X | ?A\u2118A | A\u2118?A | A\u2118A | A \u2297 A | !A . 2 We actually use in our theorem an even subtler notion: the one of isomorphism between k-experiments of indexed LPS (Definition 35). FIGURE 1. Example of PS. In the standard syntax of [22] we have a box with a unique auxiliary door represented by the port p 2 (the dashed arrow allows to determine the doors of the box) and a dereliction link (the port p 1 ); the conclusions of the auxiliary door and the dereliction are then contracted.weakenings and \u22a5 (Corollary 3). In Section 3, we introduce a domain D to interpret PS which is exactly the one already defined in [22]. Like in [13], we use here experiments (introduced in [5]) which can bethought as objects in between syntax and semantics and are related to type derivations in the \u03bb-calculus ([25]). Experiments are functions defined on (proof-)nets allowing to compute the interpretation pointwise: the set of results of all the experiments of a given (proof-)net is its interpretation 3 . Usually an experiment e of a (proof-)net R is a labeling of R at depth 0 and a function associating with every !-link l of Ra set of experiments of the content of the box associated with l. We noticed that a particular kind of experiment called k-experiment (Definition 30) can be defined directly on LPS (boxes are not needed). We conclude Section 3 by stating our results and reducing the problem of injectivity to Proposition 1, which is proven in Section 4. The paper ends with a technical appendix, containing some obvious definitions and the formal details of some constructions previously used.In [13], a single (well-chosen!) point of the interpretation of a proof-net allowed to \"rebuild\" the entire proof-net (in some particular cases and for coherent semantics). Something similar happens in this paper, with a notable difference that makes everything much more complicated: in [13] the well-chosen point of the interpretation of a proof-net allowed not only to rebuild the proof-net but also the experiment having this point as result. This is not the case here, where the well-chosen points of the interpretation of a PS are atomic injective k-points (Definition 20): we show using Figure 1 that there exist different experiments having as result the same atomic injective k-point. We can define two experiments e 1 and e 2 of the PS R represented in Figure 1 in such away that e 1 (p 1 ) = [\u03b6 1 ], e 2 (p 1 ) = [\u03b6 2 ], e 1 (p 2 ) = [\u03b6 2 , \u03b6 3 , \u03b6 4 ] and e 2 (p 2 ) = [\u03b6 1 , \u03b6 3 , \u03b6 4 ], where \u03b6 j = (\u2212, \u03b3 j , \u03b3 j ) and the \u03b3 j are distinct atoms. The two (different) experiments have the same result, which is anatomic and injective 3-point. Let us conclude by mentioning the main novelties in our proof:\u2022 the use of injective experiments in a completely different sense than in [13]: intuitively, our injective k-experiments associate with an axiom link with depth d, k d different labels, while the injective k-obsessional experiments of [13] associate a unique label with such an axiom link (see Remark 2). A crucial aspect of our new injective k-experiments is that they can be recognized by their results 3  The result of an experiment e is the image of the conclusions of the (proof-)net through the function e; so that contrary to an experiment its result is a truly semantic object. Fact 17. Let k \u2208 N. Let \u03b2 \u2208 D \u2032 such that (dig k 1 ([\u03b2])) * = []. Then ([\u03b2]) * = [\u03b2].Proof. From (dig k 1 ([\u03b2])) * = [], we deduce that At'(\u03b2) = \u2205.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Attention-Based Models for Speech Recognition Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis [1, 2] and image caption generation [3]. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in [2] reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level. All experiments were performed on the TIMIT corpus Table 1: Phoneme error rates (PER). The bold-faced PER corresponds to the best error rate with an attention-based recurrent sequence generator (ARSG) incorporating convolutional attention features and a smooth focus. Model Dev Test Baseline Model 15.9% 18.7% Baseline + Conv. Features 16.1% 18.0% Baseline + Conv. Features + Smooth Focus 15.8% 17.6% RNN Transducer [16] N/A 17.7% HMM over Time and Frequency Convolutional Net [25] 13.9% 16.7% and a smooth focus . Dev Test Table 1 : Phoneme error rates ( PER ) . The bold - faced PER corresponds to the best error rate with an",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Speech Recognition\", \"Dataset\": \"TIMIT\", \"Metric\": \"Percentage error\", \"Score\": \"17.6\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Speech Recognition\", \"Dataset\": \"TIMIT\", \"Metric\": \"Percentage error\", \"Score\": \"18.0\"}} ]"
    },
    {
        "Context": "Unobtrusive Pain Monitoring in Older Adults with Dementia using Pairwise and Contrastive Training Although pain is frequent in old age, older adults are often undertreated for pain. This is especially the case for longterm care residents with moderate to severe dementia who cannot report their pain because of cognitive impairments that accompany dementia. Nursing staff acknowledge the challenges of effectively recognizing and managing pain in long-term care facilities due to lack of human resources and, sometimes, expertise to use validated pain assessment approaches on a regular basis. Vision-based ambient monitoring will allow for frequent automated assessments so care staff could be automatically notified when signs of pain are displayed. However, existing computer vision techniques for pain detection are not validated on faces of older adults or people with dementia, and this population is not represented in existing facial expression datasets of pain. We present the first fully automated vision-based technique validated on a dementia cohort. Our contributions are threefold. First, we develop a deep learning-based computer vision system for detecting painful facial expressions on a video dataset that is collected unobtrusively from older adult participants with and without dementia. Second, we introduce a pairwise comparative inference method that calibrates to each person and is sensitive to changes in facial expression while using training data more efficiently than sequence models. Third, we introduce a fast contrastive training method that improves cross-dataset performance. Our pain estimation model outperforms baselines by a wide margin, especially when evaluated on faces of people with dementia. Pre-trained model and demo code available at https://github.com/TaatiTeam/pain_detection_demo We used two datasets to conduct our experiments, the University of Regina (UofR) Pain in Severe Dementia dataset and the UNBC-McMaster Shoulder Pain Expression Archive Database The publicly available portion of the UNBC-McMaster dataset contains video data from 25 participants (13 females) with a shoulder injury during painful and non-painful movements, recorded at 30 frames per second (fps) and in Quarter VGA (240\u00d7320) resolution The dataset contains 48,391 image frames in total (1936 \u00b1 837 per participant) The UofR dataset contains video data from 102 older adult participants with and without dementia, recorded at 15 fps Videos of 95 people from the dataset (74 females) were annotated manually by trained annotators according to PSPI and PACSLAC-II pain rating scales depicts the distribution of pain levels (PSPI) in each dataset For instance, the ratio of frames with PSPI > 5 to those with PSPI < 2 is 1.8% for the UNBC-McMaster dataset,",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Pain Intensity Regression\", \"Dataset\": \"UNBC-McMaster ShoulderPain dataset\", \"Metric\": \"Pearson Correlation Coefficient\", \"Score\": \"0.71\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Anomaly Detection\", \"Dataset\": \"UCF-MNIST\", \"Metric\": \"AUC\", \"Score\": \"0.94\"}} ]"
    },
    {
        "Context": "MUSE: PARALLEL MULTI-SCALE ATTENTION FOR SEQUENCE TO SEQUENCE LEARNING In sequence to sequence learning, the self-attention mechanism proves to be highly effective, and achieves significant improvements in many tasks. However, the self-attention mechanism is not without its own flaws. Although self-attention can model extremely long dependencies, the attention in deep layers tends to overconcentrate on a single token, leading to insufficient use of local information and difficultly in representing long sequences. In this work, we explore parallel multi-scale representation learning on sequence data, striving to capture both long-range and short-range language structures. To this end, we propose the Parallel MUlti-Scale attEntion (MUSE) and MUSE-simple. MUSE-simple contains the basic idea of parallel multi-scale sequence representation learning, and it encodes the sequence in parallel, in terms of different scales with the help from self-attention, and pointwise transformation. MUSE builds on MUSE-simple and explores combining convolution and self-attention for learning sequence representations from more different scales. We focus on machine translation and the proposed approach achieves substantial performance improvements over Transformer, especially on long sequences. More importantly, we find that although conceptually simple, its success in practice requires intricate considerations, and the multi-scale attention must build on unified semantic space. Under common setting, the proposed model achieves substantial performance and outperforms all previous models on three main machine translation tasks. In addition, MUSE has potential for accelerating inference due to its parallelism. Code will be available at https://github.com/lancopku/MUSE. We evaluate MUSE on four machine translation tasks This section describes the datasets, experimental settings, detailed results, and analysis The length penalty is set to 0.8 for En-Fr according to the validation results, 1 for the two small datasets following the default setting of The BLEU 1 metric is adopted to evaluate the model performance during evaluation Table 1: MUSE-large outperforms all previous models under the standard training and evaluation setting on WMT14 En-De and WMT14 En-Fr datasets. - Table 2: MUSE-base outperforms previous state-of-the-art models on IWSLT De-En translation datasets and outperforms previous models without BPE processing on IWSLT En-Vi. - De - En En - Vi Table 3: Comparisons between MUSE and its variants on the IWSLT 2015 De-En translation task. BLEU Table 4: The comparison between the inference speed of MUSE and Transformer. modules and Transformer with 6 base blocks . The hidden size is set to 512 . 31% increase in inference speed can be obtained . The experiments use MUSE with 6 MUSE - simple Inference Speed ( tokens / s )",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-French\", \"Metric\": \"BLEU score\", \"Score\": \"43.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"IWSLT2014 German-English\", \"Metric\": \"BLEU score\", \"Score\": \"36.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-German\", \"Metric\": \"BLEU score\", \"Score\": \"29.9\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-French\", \"Metric\": \"BLEU score\", \"Score\": \"43.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-German\", \"Metric\": \"BLEU score\", \"Score\": \"21.7\"}} ]"
    },
    {
        "Context": "Obstacle Constrained Total Area Coverage in Wireless Sensor Networks This paper deals with the accomplishment of total area coverage of an arbitrary region using sensors with a finite sensing radius of rs. For a given region, we aim to obtain a deterministic placement of sensors which, apart from ensuring that the entire region comes under the purview of at least a single sensor, minimises the number of sensors utilised. We begin by considering regions devoid of obstacles and thus having every location amenable for placement. Herein, we formalise the popular notion that sensors at the centres of the hexagons of a hexagonal tessellation provide the most optimal placement. We then move onto regions which may comprise obstacles of arbitrary size at arbitrary locations. We recognise two distinct classes of obstacles, namely transparent and opaque obstacles, which are distinguished by their ability (or the lack of it) to permit sensing radiation through them. In the real world, transparent obstacles model lakes, ponds and swamps, while the opaque ones stand for, inter alia, hills, trees and walls. We propose a polynomial-time algorithm for achieving optimal placement in the aforesaid scenarios and we prove its convergence.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "'Q-Feed' -An Effective Solution for the Free-riding Problem in Unstructured P2P Networks This paper presents a solution for reducing the ill effects of free-riders in decentralised unstructured P2P networks. An autonomous replication scheme is proposed to improve the availability and enhance system performance. Q-learning is widely employed in different situations to improve the accuracy in decision making by each peer. Based on the performance of neighbours of a peer, every neighbour is awarded different levels of ranks. At the same time a low-performing node is allowed to improve its rank in different ways. Simulation results show that Q-learning based free riding control mechanism effectively limits the services received by free-riders and also encourages the low-performing neighbours to improve their position. The popular files are autonomously replicated to nodes possessing required parameters. Due to this improvement of quantity of popular files, free riders are given opportunity to lift their position for active participation in the network for sharing files. Q-feed effectively manages queries from free riders and reduces network traffic significantly.  Table 1. Q-Table of a node Node-id N 1 N 2 N 3 N 4 N 5 Q-value 150 80 35 245 70 Table 1 . Q - Table of a node Table 3. A Replication Q-Table nodes 3 - hops away A Replication Q - Table B C D E F H Table 3 . N Table 4. Status of Q-table of node 'A' before and after replication operations Bandwidth , Table 4 . Status of Q - table of node ' A ' before and after replication B C D E F H N 175 682 122 441 466 324 336",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "A Strong Baseline and Batch Normalization Neck for Deep Person Re-identification This study proposes a simple but strong baseline for deep person re-identification (ReID). Deep person ReID has achieved great progress and high performance in recent years. However, many state-of-the-art methods design complex network structures and concatenate multi-branch features. In the literature, some effective training tricks briefly appear in several papers or source codes. The present study collects and evaluates these effective training tricks in person ReID. By combining these tricks, the model achieves 94.5% rank-1 and 85.9% mean average precision on Market1501 with only using the global features of ResNet50. The performance surpasses all existing global-and part-based baselines in person ReID. We propose a novel neck structure named as batch normalization neck (BNNeck). BNNeck adds a batch normalization layer after global pooling layer to separate metric and classification losses into two different feature spaces because we observe they are inconsistent in one embedding space. Extended experiments show that BNNeck can boost the baseline, and our baseline can improve the performance of existing state-of-the-art methods. Our codes and models are available at: https://github.com/michuanhaohao/reid-strongbaseline We evaluate our models on Market1501 and DukeMTMC-reID datasets, because both datasets are widely used and large scale Following the previous works, we use rank-1 accuracy and mAP for evaluation on both datasets Single-query evaluation is used in this study DukeMTMC-reID is anew large-scale person ReID dataset and collects 36,411 images from 1,404 identities of eight camera views Single-query evaluation is used in this study ID and triplet loss curves of different models on Market1501 and DukeMTMC-reID datasets",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Person Re-Identification\", \"Dataset\": \"Market-1501\", \"Metric\": \"MAP\", \"Score\": \"88.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Person Re-Identification\", \"Dataset\": \"DukeMTMC-reID\", \"Metric\": \"MAP\", \"Score\": \"79.1\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Person Re-Identification\", \"Dataset\": \"Market-1501\", \"Metric\": \"MAP\", \"Score\": \"83.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Person Re-Identification\", \"Dataset\": \"Market-1501\", \"Metric\": \"Rank-1\", \"Score\": \"93.6\"}} ]"
    },
    {
        "Context": "An in-place truncated Fourier transform and applications to polynomial multiplication The truncated Fourier transform (TFT) was introduced by van der Hoeven in 2004 as a means of smoothing the \"jumps\" in running time of the ordinary FFT algorithm that occur at power-of-two input sizes. However, the TFT still introduces these jumps in memory usage. We describe in-place variants of the forward and inverse TFT algorithms, achieving time complexity O(n log n) with only O(1) auxiliary space. As an application, we extend the second author's results on space-restricted FFT-based polynomial multiplication to polynomials of arbitrary degree.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "INDEPENDENCE AND CONCURRENT SEPARATION LOGIC * A compositional Petri net-based semantics is given to a simple language allowing pointer manipulation and parallelism. The model is then applied to give a notion of validity to the judgements made by concurrent separation logic that emphasizes the process-environment duality inherent in such rely-guarantee reasoning. Soundness of the rules of concurrent separation logic with respect to this definition of validity is shown. The independence information retained by the Petri net model is then exploited to characterize the independence of parallel processes enforced by the logic. This is shown to permit a refinement operation capable of changing the granularity of atomic actions. C Tc(t 1 ) in N t 1 and \u03c0 2 :Proof. Induction on the length of path \u03c0. * C Tc(t 0 ) in N t 0 .",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Co-localization with Category-Consistent Features and Geodesic Distance Propagation Co-localization is the problem of localizing categorical objects using only positive set of example images, without any form of further supervision. This is a challenging task as there is no pixel-level annotations. Motivated by human visual learning, we find the common features of an object category from convolutional kernels of a pretrained convolutional neural network (CNN). We call these category-consistent CNN features. Then, we co-propagate their activated spatial regions using superpixel geodesic distances for localization. In our first set of experiments, we show that the proposed method achieves state-of-the-art performance on three related benchmarks: PASCAL 2007, PASCAL-2012, and the Object Discovery dataset. We also show that our method is able to detect and localize truly unseen categories, using six held-out ImagNet subset of categories with state-of-the-art accuracies. Our intuitive approach achieves this success without any region proposals or object detectors, and can be based on a CNN that was pre-trained purely on image classification tasks without further fine-tuning. * We evaluate our proposed 2-step framework with different parameter settings to illustrate different characteristics of our method We also evaluate our method on multiple benchmarks, with intermediate and final results to show the localization effects of our proposed method We use the conventional CorLoc metric to evaluate our co-localization results To benchmark our method performance, we evaluate our method on three commonly used datasets for the problem of co-localization These are VOC 2007 and 2012, and Object Discovery dataset For experiments on VOC datasets, we followed previous works that used all images on the trainval set excluding the images that only contain the object instances annotated as difficult or truncated For experiments on the Object Discovery dataset, we used the 100image subset following in order to make an appropriate comparison with related methods The ground truth bounding box for each image in the Object Discovery dataset is defined as the smallest Table 1. CorLoc scores of our approach and state-of-the-art co-localization methods on Pascal VOC 2007 dataset. Table 1 . CorLoc scores of our approach and state - of - the - art co - localization methods on Pascal VOC 2007 dataset . mbike bus sofa tv chair cow bottle aero boat bike horse car person mean bird cat plant dog table sheep train Table 2. CorLoc scores of our approach and state-of-the-art co-localization methods on Pascal VOC 2012 dataset. Table 2 . CorLoc scores of our approach and state - of - the - art co - localization methods on Pascal VOC 2012 dataset . Table 1 . CorLoc scores of our approach and state - of - the - art co - localization methods on Pascal VOC 2007 dataset . mbike bus sofa tv chair cow bottle aero boat bike horse car person mean bird cat plant dog table sheep",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Object Localization\", \"Dataset\": \"PASCAL VOC 2012\", \"Metric\": \"CorLoc\", \"Score\": \"47.45\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Localization\", \"Dataset\": \"PASCAL VOC 2007\", \"Metric\": \"CorLoc\", \"Score\": \"41.2\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL VOC 2007\", \"Metric\": \"Mean IoU\", \"Score\": \"74.4%\"}} ]"
    },
    {
        "Context": "Minimizing the Maximum Interference is Hard We consider the following interference model for wireless sensor and ad hoc networks: the receiver interference of anode is the number of transmission ranges it lies in. We model transmission ranges as disks. For this case we show that choosing transmission radii which minimize the maximum interference while maintaining a connected symmetric communication graph is NP-complete.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "A Capsule Network-based Embedding Model for Knowledge Graph Completion and Search Personalization In this paper, we introduce an embedding model, named CapsE, exploring a capsule network to model relationship triples (subject, relation, object). Our CapsE represents each triple as a 3-column matrix where each column vector represents the embedding of an element in the triple. This 3-column matrix is then fed to a convolution layer where multiple filters are operated to generate different feature maps. These feature maps are reconstructed into corresponding capsules which are then routed to another capsule to produce a continuous vector. The length of this vector is used to measure the plausibility score of the triple. Our proposed CapsE obtains better performance than previous state-of-the-art embedding models for knowledge graph completion on two benchmark datasets WN18RR and FB15k-237, and outperforms strong search personalization baselines on SEARCH17. Dataset: We use the SEARCH17 dataset of query logs of 106 users collected by a large-scale web search engine The rank position of the relevant labeled documents is used as the ground truth to evaluate the search performance before and after re-ranking The dataset was uniformly split into the training, validation and test sets Evaluation protocol: Our CapsE is used to rerank the original list of documents returned by a search engine as follows: (i) We train our model and employ the trained model to calculate the score for each (s, r, o) triple To evaluate the performance of our proposed model, we use two standard evalu-ation metrics: mean reciprocal rank (MRR) and Hits@1 Table 1: Statistics of the experimental datasets. #E is the number of entities. #R is the number of relations. and FB15k - 237 . 11 presents the statistics of WN18RR #R Table 2: Experimental results on the WN18RR and FB15k-237 test sets. Hits@10 (H@10) is reported in %. Results of DISTMULT, ComplEx and ConvE are taken from Dettmers et al. (2018). Results of TransE on FB15k- 237 are taken from Nguyen et al. (2018). Our CapsE Hits@1 scores are 33.7% on WN18RR and 48.9% on FB15k-237. Formulas of MRR and Hits@1 show a strong correlation, so using Hits@1 does not really reveal any additional information for this task. The best score is in bold, while the second best score is in underline. denotes our new results for TransE and ConvKB, which are better than those published by Nguyen et al. (2018). belled 1 - 1 , 1 - M , M",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@10\", \"Score\": \"0.56\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"MR\", \"Score\": \"719.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"MRR\", \"Score\": \"0.415\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"Evaluation Protocol\", \"Score\": \"Can be affected with a more appropriate protocol. See the row of CapsE (Corrected).\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"Hits@10\", \"Score\": \"0.593\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"MR\", \"Score\": \"303.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"MRR\", \"Score\": \"0.523\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@1\", \"Score\": \"0.461\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@10\", \"Score\": \"0.581\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@3\", \"Score\": \"0.483\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"MRR\", \"Score\": \"0.44\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"Hits@1\", \"Score\": \"0.261\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"Hits@10\", \"Score\": \"0.547\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"Hits@3\", \"Score\": \"0.304\"}}, { \"LEADERBOARD\": { 'Ta"
    },
    {
        "Context": "Improved Fully Dynamic Reachability Algorithm for Directed Graph We propose a fully dynamic algorithm for maintaining reachability information in directed graphs. The proposed deterministic dynamic algorithm has an update time of O((ins * n 2 ) + (del * (m + n * log(n)))) where m is the current number of edges, n is the number of vertices in the graph, ins is the number of edge insertions and del is the number of edge deletions. Each query can be answered in O(1) time after each update. The proposed algorithm combines existing fully dynamic reachability algorithm with well known witness counting technique to improve efficiency of maintaining reachability information when edges are deleted. The proposed algorithm improves by a factor of O( n 2 m+n * log(n) ) for edge deletion over the best existing fully dynamic algorithm for maintaining reachability information.  Table 3: Insert(E v ) from scratch when node v acts as an witness . So we decrement witness count by one for each pair in [ RZ04 ] v v is a previous insertion center then for each u in In [ v ] do end if This we perform before we compute new In [ v ] and Out [ v ] . Now we remove assumption that v is not a insertion center before . compute new In and Out trees rooted at node v using algorithm mentioned current number of edges in the graph and n is number of vertices in the graph . for each z in Out [ v ] do end for Table 5: U pdateT CM witness for directed path from node u to node z any more iff u \u2208 In delete [ v ] and z \u2208 Out [ v ]",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Similarity-based Reasoning in Qualified Logic Programming Revised Edition Similarity-based Logic Programming (briefly, SLP ) has been proposed to enhance the LP paradigm with a kind of approximate reasoning which supports flexible information retrieval applications. This approach uses a fuzzy similarity relation R between symbols in the program's signature, while keeping the syntax for program clauses as in classical LP . Another recent proposal is the QLP (D) scheme for Qualified Logic Programming, an extension of the LP paradigm which supports approximate reasoning and more. This approach uses annotated program clauses and a parametrically given domain D whose elements qualify logical assertions by measuring their closeness to various users' expectations. In this paper we propose a more expressive scheme SQLP (R, D) which subsumes both SLP and QLP (D) as particular cases. We also show that SQLP (R, D) programs can be transformed into semantically equivalent QLP (D) programs. As a consequence, existing QLP (D) implementations can be used to give efficient support for similarity-based reasoning.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Supervised Relation Classification as Two-way Span-Prediction The current supervised relation classification (RC) task uses a single embedding to represent the relation between a pair of entities. We argue that a better approach is to treat the RC task as span-prediction (SP) problem, similar to Question answering (QA). We present a span-prediction based system for RC and evaluate its performance compared to the embedding based system. We demonstrate that the supervised SP objective works significantly better then the standard classification based objective. We achieve state-of-the-art results on the TACRED and SemEval task 8 datasets. We compare ourselves on three RC datasets TACRED is the currently most popular and largest RC dataset To show this bias empirically, they created a Wikipedia based dataset intended to be used only for testing, which contains 3000 manually tagged sentences from the TACRED relations Each sentence in the dataset contains two entity pairs that are compatible with the same relation The evaluation of the CRE is binary -the model goal is to indicate if a given relation is found or not found in the dataset The model was evaluated with both SP and RC models Table 1: CRE. Span prediction model result on CRE, compared to traditional RC and QA model. RC models are relation classification models and SQuAD models are QA models that were trained on the SQuAD 2.0 dataset. are relation classification models and SQuAD models dataset . P R F 1 Table 1 : CRE . Span prediction model result on CRE , Table 2: TACRED. Supervised results on the TACRED datasets. Top: Using BERT. This is a direct compari- son to the MTB span-prediction model. MTB F 1 is taken from the original paper. SP models (except token) suppress MTB. Bottom: Using ALBERT. Here the ref- erence point is KEPLLER, the current best performing model on this dataset. All the supervised SP-ALBERT models outperform KEPPLER. P R F 1 Table 3: SemEval. Supervised results on the SemEval datasets. LiTian is the current state of the art. P 1 R F Table 4:",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Relation Extraction\", \"Dataset\": \"TACRED\", \"Metric\": \"F1\", \"Score\": \"74.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Relation Extraction\", \"Dataset\": \"SemEval-2010 Task 8\", \"Metric\": \"F1\", \"Score\": \"91.9\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"Re-TACRED\", \"Metric\": \"F1\", \"Score\": \"83.6\"}}, { \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"TACRED\", \"Metric\": \"F1\", \"Score\": \"68.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"TACRED\", \"Metric\": \"F1\", \"Score\": \"67.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"TACRED\", \"Metric\": \"F1\", \"Score\": \"66.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"TACRED\", \"Metric\": \"F1\", \"Score\": \"64.0\"}}, { \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"SemEval-2010 Task 8\", \"Metric\": \"F1\", \"Score\": \"89.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"SemEval-2010 Task 8\", \"Metric\": \"F1\", \"Score\": \"90.3\"}} ]"
    },
    {
        "Context": "Self-Supervised Model Adaptation for Multimodal Semantic Segmentation Learning to reliably perceive and understand the scene is an integral enabler for robots to operate in the realworld. This problem is inherently challenging due to the multitude of object types as well as appearance changes caused by varying illumination and weather conditions. Leveraging complementary modalities can enable learning of semantically richer representations that are resilient to such perturbations. Despite the tremendous progress in recent years, most multimodal convolutional neural network approaches directly concatenate feature maps from individual modality streams rendering the model incapable of focusing only on the relevant complementary information for fusion. To address this limitation, we propose a mutimodal semantic segmentation framework that dynamically adapts the fusion of modalityspecific features while being sensitive to the object category, spatial location and scene context in a self-supervised manner. Specifically, we propose an architecture consisting of two modality-specific encoder streams that fuse intermediate encoder representations into a single decoder using our proposed self-supervised model adaptation fusion mechanism which optimally combines complementary features. As intermediate representations are not aligned across modalities, we introduce an attention scheme for better correlation. In addition, we propose a computationally efficient unimodal segmentation architecture termed AdapNet++ that incorporates anew encoder with multiscale residual units and an efficient atrous spatial pyramid pooling that has a lar-Abhinav Valada ger effective receptive field with more than 10\u00d7 fewer parameters, complemented with a strong decoder with a multiresolution supervision scheme that recovers high-resolution details. Comprehensive empirical evaluations on Cityscapes, Synthia, SUN RGB-D, ScanNet and Freiburg Forest benchmarks demonstrate that both our unimodal and multimodal architectures achieve state-of-the-art performance while simultaneously being efficient in terms of parameters and inference time as well as demonstrating substantial robustness in adverse perceptual conditions. In this section, we first describe the datasets that we benchmark on, followed by comprehensive quantitative results for unimodal segmentation using our proposed AdapNet++ architecture in Section 5.3 and the results for model compression in Section 5.4 We present the multimodal fusion benchmarking experiments with the various modalities contained in the datasets in Section 5.7 and the ablation study on our multimodal fusion architecture in Section 5.10 We evaluate our proposed AdapNet++ architecture on five publicly available diverse scene understanding benchmarks ranging from urban driving scenarios to unstructured forested scenes and cluttered indoor environments The datasets were particularly chosen based on the criteria of containing scenes with challenging perceptual conditions including rain, snow, fog, night-time, glare, motion blur and other seasonal appearance changes Each of the datasets contain multiple modalities that we utilize for benchmarking our fusion approach We briefly describe the datasets and their constituting semantic categories in this section Table 1 Performance comparison of AdapNet++ with baseline models on the Cityscapes validation set with 11 semantic class labels (input image dim: 768 \u00d7 384). Note that no left-right flips or multiscale testing is performed. Note: Corresponding multimodal results are reported in Table 17. ( % ) Sky Building mIoU Vegetation Pole Car Road Sign Cyclist Sidewalk Fence Person Table 2 Benchmarking results on the Cityscapes dataset with full resolution evaluation on 19 semantic class labels. Only the eight top performing published models in the leaderboard are listed in this table. The inference time is reported for an input image resolution of 768 \u00d7 384 pixels and it was computed on an NVIDIA TITAN X (PASCAL) GPU using the official implementation of each method. val mIoU ( % ) test Parms . Time ( M ) ( ms ) Table 3 Performance comparison of AdapNet++ with baseline models on the Synthia",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Scene Recognition\", \"Dataset\": \"ScanNet\", \"Metric\": \"Average Recall\", \"Score\": \"54.28\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"SUN-RGBD\", \"Metric\": \"Mean IoU\", \"Score\": \"45.73\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"SUN-RGBD\", \"Metric\": \"Mean IoU\", \"Score\": \"38.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Freiburg Forest\", \"Metric\": \"Mean IoU\", \"Score\": \"84.18\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Freiburg Forest\", \"Metric\": \"Mean IoU\", \"Score\": \"83.09\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"82.3%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"81.24%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"SYNTHIA-CVPR\u201916\", \"Metric\": \"Mean IoU\", \"Score\": \"92.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"SYNTHIA-CVPR\u201916\", \"Metric\": \"Mean IoU\", \"Score\": \"87.87\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"ScanNetV2\", \"Metric\": \"Mean IoU\", \"Score\": \"57.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"ScanNetV2\", \"Metric\": \"Mean IoU\", \"Score\": \"50.3\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"82.6%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"S3DIS\", \"Metric\": \"Mean IoU\", \"Score\": \"61.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"S3DIS\", \"Metric\":\"mIoU\", \"Score\": \"51.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"ScanNet\", \"Metric\": \"3DIoU\", \"Score\": \"0.44\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Semantic Segmentation\", \"Dataset\": \"SemanticKITTI\", \"Metric\":\"mIoU\", \"Score\": \"35.9%\"}} ]"
    },
    {
        "Context": "Generative Image Modeling Using Spatial LSTMs Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multidimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.  Table 1: Average log-likelihoods and log-likelihood rates for image patches (without/with DC comp.) and large images extracted from BSDS300 [25]. each patch ' s DC component was zero . Because the resulting image patches live on a 63 dimensional Model - \u221e dim . 64 dim . [ nat ] [ bit / px ] 63 dim . Table 2: Average log-likelihood rates for im- age patches and large images extracted from van Hateren's dataset [48]. Model - 256 dim . \u221e dim . [ bit / px ] Table 3: Average log-likelihood rates on dead leaf images. A deep recurrent image model is on a par with a deep diffusion model [ bit / px ]",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Generation\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"bits/dimension\", \"Score\": \"3.47\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"bits/dimension\", \"Score\": \"3.03\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"ImageNet 64x64\", \"Metric\": \"Bits per dim\", \"Score\": \"3.57\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"ImageNet 32x32\", \"Metric\": \"bpd\", \"Score\": \"3.83\"}} ]"
    },
    {
        "Context": "R-FCN: Object Detection via Region-based Fully Convolutional Networks We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN [6,18] that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets) [9], for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20\u00d7 faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn. * This work was done when Yi Li was an intern at Microsoft Research. 2 Only the last layer is fully-connected, which is removed and replaced when fine-tuning for object detection. We train the models on the union set of VOC 2007 trainval and VOC 2012 trainval (\"07+12\") following, and evaluate on VOC 2007 test set Next we evaluate on the MS COCO dataset that has 80 object categories We extend the alternating training from 4-step to 5-step (i.e., stopping after one more RPN training step), which slightly improves accuracy on this dataset when the features are shared; we also report that 2-step training is sufficient to achieve comparably good accuracy but the features are not shared Considering COCO's wide range of object scales, we further evaluate a multi-scale testing variant following, and use testing scales of {200,400,600,800,1000} Table 2: Comparisons among fully convolutional (or \"almost\" fully convolutional) strategies using ResNet-101. All competitors in this table use the \u00e0 trous trick. Hard example mining is not conducted. fail RoI output size ( k \u00d7 k ) mAP on VOC 07 ( % ) Table 3: Comparisons between Faster R-CNN and R-FCN using ResNet-101. Timing is evaluated on a single Nvidia K40 GPU. With OHEM, N RoIs per image are computed in the forward pass, and 128 samples are selected for backpropagation. 300 RoIs are used for testing following [18]. N / A mAP ( % ) on VOC07 train time test time ( sec / img ) Table 4: Comparisons on PASCAL VOC 2007 test set using ResNet-101. \"Faster R-CNN +++\" [9] uses iterative box regression, context, and multi-scale testing. mAP ( % ) test time ( sec / img ) Table 5: Comparisons on PASCAL VOC 2012 test",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"PASCAL VOC 2007\", \"Metric\": \"MAP\", \"Score\": \"80.5%\"}}, {\"LEADERBOARD\": {\"Task\": \"Real-Time Object Detection\", \"Dataset\": \"PASCAL VOC 2007\", \"Metric\": \"FPS\", \"Score\": \"9\"}}, {\"LEADERBOARD\": {\"Task\": \"Real-Time Object Detection\", \"Dataset\": \"PASCAL VOC 2007\", \"Metric\": \"MAP\", \"Score\": \"80.5%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"PASCAL VOC 2007\", \"Metric\": \"MAP\", \"Score\": \"82.6%\"}}, { \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP50\", \"Score\": \"91.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP75\", \"Score\": \"79.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APL\", \"Score\": \"83.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APM\", \"Score\": \"47.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APS\", \"Score\": \"26.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"box AP\", \"Score\": \"43.3\"}}, { '"
    },
    {
        "Context": "Canonical Coin Systems for Change-Making Problems The Change-Making Problem is to represent a given value with the fewest coins under a given coin system. As a variation of the knapsack problem, it is known to be NP-hard. Nevertheless, inmost real money systems, the greedy algorithm yields optimal solutions. In this paper, we study what type of coin systems that guarantee the optimality of the greedy algorithm. We provide new proofs fora sufficient and necessary condition for the so-called canonical coin systems with four or five types of coins, and a sufficient condition for non-canonical coin systems, respectively. Moreover, we present an O(m 2 ) algorithm that decides whether a tight coin system is canonical.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "FABRICATION OF SWTICHES ON POLYMER-BASED BY HOT EMBOSSING In MEMS technology, most of the devices are fabricated on glass or silicon substrate. However, this research presents a novel manufacture method that is derived from conventional hot embossing technology to fabricate the electrostatic switches on polymer material. The procedures of fabrication involve the metal deposition, photolithography, electroplating, hot embossing and hot embed techniques. The fundamental concept of the hot embed technology is that the temperature should be increased above Tg of polymer, and the polymer becomes plastic and viscous and could be molded. According to the fundamental concept, the metal layer on the silicon/glass substrate could be embedded into polymer material during the hot embossing process. Afterward, the metal layer is bonded together with the polymer after removing the substrate in the de-embossing step. Finally, the electrostatic switch is fabricated on polymethylmethacrylate(PMMA) material to demonstrate the novel method.  Table 1 The polymer materials, PMMA and PC. Polymer material 105 conductivity kg / m ( W / m \u2022 K ) Density \u00b0C ) Thermal Table 2. The parameters of hot embossing Fixed electrode mold temperature ( \u00b0C ) 2 Table 2 . The parameters of hot embossing electrode Gap layer Actuating mold ) Fixed 120",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Activity Driven Weakly Supervised Object Detection Weakly supervised object detection aims at reducing the amount of supervision required to train detection models. Such models are traditionally learned from images/videos labelled only with the object class and not the object bounding box. In our work, we try to leverage not only the object class labels but also the action labels associated with the data. We show that the action depicted in the image/video can provide strong cues about the location of the associated object. We learn a spatial prior for the object dependent on the action (e.g. \"ball\" is closer to \"leg of the person\" in \"kicking ball\"), and incorporate this prior to simultaneously train a joint object detection and action classification model. We conducted experiments on both video datasets and image datasets to evaluate the performance of our weakly supervised object detection model. Our approach outperformed the current state-of-the-art (SOTA) method by more than 6% in mAP on the Charades video dataset. Object bounding box annotations are used only during evaluation Video datasets: The Charades dataset includes 9,848 videos of 157 action classes, among which, 66 are interactive actions with objects The official Charades dataset doesn't provide object bounding box annotations and we use the annotations released by We follow the same practice as in: train on 7,986 videos (54,000 clips) and evaluate on 5,000 randomly selected test frames from 200 test videos The EPIC-KITCHENS is an ego-centric video dataset which is captured by head-mounted camera in different kitchen scenes Image dataset The HICO-DET dataset is designed for human-object interaction (HOI) detection task This dataset includes 38,118 training images and 9,658 test images We use the HOI labels as action class labels during training and the object bounding box annotations are used only for evaluation Table 1: Detection performance of different variants on Charades Table 1 : Detection performance of different variants on Charades mAP CorLoc Table 2: AP performance (%) on each object class and mAP (%) comparison with different weakly supervised methods on Charades. bed sofa tv chair broom towel vacuum window shelf mAP ( % ) dish door laptop mirror pillow table cup refri Table 3: AP performance (%) on selected object classes and mAP (%) comparison with other weakly supervised methods on HICO-DET. apple mAP ( % ) bicycle surfboard umbrella chair cellphone bottle frisbee kite train Table 4: mAP (%) comparison with other weakly supervised meth- ods on EPIC KITCHENS ods on EPIC KITCHENS mAP Table 4 : mAP ( % ) comparison with other weakly supervised meth - CorLoc",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Weakly Supervised Object Detection\", \"Dataset\": \"Charades\", \"Metric\": \"MAP\", \"Score\": \"10.03\"}}, {\"LEADERBOARD\": {\"Task\": \"Weakly Supervised Object Detection\", \"Dataset\": \"HICO-DET\", \"Metric\": \"MAP\", \"Score\": \"5.39\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Weakly Supervised Object Detection\", \"Dataset\": \"HICO-DET\", \"Metric\":\"mAP\", \"Score\": \"41.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Weakly Supervised Object Detection\", \"Dataset\": \"Charades\", \"Metric\": \"MAP\", \"Score\": \"21.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Weakly Supervised Object Detection\", \"Dataset\": \"Charades\", \"Metric\":\"mAP\", \"Score\": \"46.0\"}}, { \"LEADERBOARD\": { \"Task\": \"Weakly Supervised Object Detection\", \"Dataset\": \"HICO-DET\", \"Metric\": \"MAP\", \"Score\": \"36.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Weakly Supervised Object Detection\", \"Dataset\": \"HICO-DET\", \"Metric\":\"mAP\", \"Score\": \"17.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Weakly Supervised Object Detection\", \"Dataset\": \"HICO-DET\", \"Metric\": \"MAP\", \"Score\": \"42.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Weakly Supervised Object Detection\", \"Dataset\": "
    },
    {
        "Context": "Predicting intubation support requirement of patients using Chest X-ray with Deep Representation Learning Recent developments in medical imaging with Deep Learning presents an evidence of automated diagnosis and prognosis. It can also be a complement to currently available diagnosis methods. Deep Learning can be leveraged for diagnosis, severity prediction, intubation support prediction and many similar tasks. We present prediction of intubation support requirement for patients from the Chest X-ray using Deep representation learning. We release our source code publicly on https://github.com/aniketmaurya/covid-research. We use covid-chestxray-dataset, an open dataset collected from public and indirect collection from hospitals and physicians The dataset is available on GitHub The metadata of this dataset contains labels of 25 lungs disease, shown in Table 1: covid-chestxray-datset pathology negative and positive frequency 22 11 23 48 15 16 17 1 3 4 5 8 9 Negative Positive",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Intubation Support Prediction\", \"Dataset\": \"COVID chest X-ray\", \"Metric\": \"AUC-ROC\", \"Score\": \"0.84\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Multi-tissue Nucleus Segmentation\", \"Dataset\": \"Kumar\", \"Metric\": \"Dice\", \"Score\": \"0.826\"}}, { \"LEADERBOARD\": { \"Task\": \"Multi-tissue Nucleus Segmentation\", \"Dataset\": \"Kumar\", \"Metric\": \"Hausdorff Distance (mm)\", \"Score\": \"59.7\"}} ]"
    },
    {
        "Context": "On Compact Routing for the Internet The Internet's routing system is facing stresses due to its poor fundamental scaling properties. Compact routing is a research field that studies fundamental limits of routing scalability and designs algorithms that try to meet these limits. In particular, compact routing research shows that shortest-path routing, forming a core of traditional routing algorithms, cannot guarantee routing table (RT) sizes that on all network topologies grow slower than linearly as functions of the network size. However, there are plenty of compact routing schemes that relax the shortest-path requirement and allow for improved, sublinear RT size scaling that is mathematically provable for all static network topologies. In particular, there exist compact routing schemes designed for grids, trees, and Internet-like topologies that offer RT sizes that scale logarithmically with the network size.In this paper, we demonstrate that in view of recent results in compact routing research, such logarithmic scaling on Internet-like topologies is fundamentally impossible in the presence of topology dynamics or topology-independent (flat) addressing. We use analytic arguments to show that the number of routing control messages per topology change cannot scale better than linearly on Internet-like topologies. We also employ simulations to confirm that logarithmic RT size scaling gets broken by topology-independent addressing, a cornerstone of popular locator-identifier split proposals aiming at improving routing scaling in the presence of network topology dynamics or host mobility. These pessimistic findings lead us to the conclusion that a fundamental re-examination of assumptions behind routing models and abstractions is needed in order to find a routing architecture that would be able to scale \"indefinitely.\"1 We clarify upfront that scalability is only one of several problems of the current Internet routing architecture. Other problems include security, isolation, configuration control, etc. See [2] fora long list of future routing architecture requirements.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Vehicle Re-identification Using Quadruple Directional Deep Learning Features In order to resist the adverse effect of viewpoint variations for improving vehicle re-identification performance, we design quadruple directional deep learning networks to extract quadruple directional deep learning features (QD-DLF) of vehicle images. The quadruple directional deep learning networks are with similar overall architecture, including the same basic deep learning architecture but different directional feature pooling layers. Specifically, the same basic deep learning architecture is a shortly and densely connected convolutional neural network to extract basic feature maps of an input square vehicle image in the first stage. Then, the quadruple directional deep learning networks utilize different directional pooling layers, i.e., horizontal average pooling (HAP) layer, vertical average pooling (VAP) layer, diagonal average pooling (DAP) layer and anti-diagonal average pooling (AAP) layer, to compress the basic feature maps into horizontal, vertical, diagonal and anti-diagonal directional feature maps, respectively. Finally, these directional feature maps are spatially normalized and concatenated together as a quadruple directional deep learning feature for vehicle re-identification. Extensive experiments on both VeRi and VehicleID databases show that the proposed QD-DLF approach outperforms multiple state-of-the-art vehicle re-identification methods.Jianqing Zhu received the B.S. degree in communication engineering and the M.S. degree in communication and information system from the And two commonly used criteria in the re-identification field, i.e., cumulative match curve (CMC) and mean average precision (MAP), are used to evaluate the performance The MAP is used to evaluate the overall performance Then, the mean value of APs of all queries is calculated as MAP, which considers both precision and recall of a re-identification method, and thus provides a more comprehensive performance evaluation",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Vehicle Re-Identification\", \"Dataset\": \"VeRi-776\", \"Metric\": \"mAP\", \"Score\": \"61.83\"}}, {\"LEADERBOARD\": {\"Task\": \"Vehicle Re-Identification\", \"Dataset\": \"VehicleID Small\", \"Metric\": \"mAP\", \"Score\": \"76.54\"}}, {\"LEADERBOARD\": {\"Task\": \"Vehicle Re-Identification\", \"Dataset\": \"VehicleID Medium\", \"Metric\": \"mAP\", \"Score\": \"74.63\"}}, {\"LEADERBOARD\": {\"Task\": \"Vehicle Re-Identification\", \"Dataset\": \"VehicleID Large\", \"Metric\": \"mAP\", \"Score\": \"68.41\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Vehicle Re-Identification\", \"Dataset\": \"VeRi-7\", \"Metric\": \"Rank-1\", \"Score\": \"96.78\"}}, { \"LEADERBOARD\": { \"Task\": \"Vehicle Re-Identification\", \"Dataset\": \"VeRi-7\", \"Metric\":\"mAP\", \"Score\": \"83.41\"}}, { \"LEADERBOARD\": { \"Task\": \"Vehicle Re-Identification\", \"Dataset\": \"VehicleID Small\", \"Metric\": \"Rank-1\", \"Score\": \"83.64\"}}, { \"LEADERBOARD\": { \"Task\": \"Vehicle Re-Identification\", \"Dataset\": \"VeRi\", \"Metric\": \"Rank-5\", \"Score\": \"96.78\"}}, { \"LEADERBOARD\": { \"Task\": \"Vehicle Re-Identification\", \"Dataset\": \"VeRi\", \"Metric\":\"mAP\", \"Score\": \"83.41\"}}, { \"LEADERBOARD\": { \"Task\": \"Vehicle Re-Identification\", \"Dataset\": \"VehicleID Medium\", \"Metric\": \"Rank-1\", \"Score\": \"81.35\"}}, { \"LEADERBOARD\": { \"Task\": \"Vehicle Re-Identification\", \"Dataset\": \"Veh"
    },
    {
        "Context": "Learning to Learn via Self-Critique In few-shot learning, a machine learning system learns from a small set of labelled examples relating to a specific task, such that it can generalize to new examples of the same task. Given the limited availability of labelled examples in such tasks, we wish to make use of all the information we can. Usually a model learns taskspecific information from a small training-set (support-set) to predict on an unlabelled validation set (target-set). The target-set contains additional task-specific information which is not utilized by existing few-shot learning methods. Making use of the target-set examples via transductive learning requires approaches beyond the current methods; at inference time, the target-set contains only unlabelled input data-points, and so discriminative learning cannot be used. In this paper, we propose a framework called Self-Critique and Adapt or SCA, which learns to learn an label-free loss function, parameterized as a neural network. A base-model learns on a support-set using existing methods (e.g. stochastic gradient descent combined with the cross-entropy loss), and then is updated for the incoming target-task using the learnt loss function. The label-free loss function is learned such that the target-set-updated model achieves higher generalization performance. Experiments demonstrate that SCA offers substantially reduced errorrates compared to baselines which only adapt on the support-set, and results instate of the art benchmark performance on Mini-ImageNet and Caltech-UCSD Birds 200. To evaluate the proposed methods we first establish baselines on both the low-end and high-end variants of MAML++ on the Mini-ImageNet and Caltech-UCSD Birds 200 (CUB) 5-way 1/5-shot tasks Table 1: SCA Ablation Studies on Mini-ImageNet and CUB: All variants utilizing the proposed SCA method perform substantially better than the non-SCA baseline variant. Interestingly, the best type of critic conditioning features varies depending on the backbone architecture. Based on our experiments, the best critic conditioning features for the Low-End MAML++ is the combination of predictions, task-embedding and network parameters, whereas on High-End MAML++, using just the target-set predictions appears to be enough to obtain the highest performance observed in our experiments. 5 - shot - 1 - shot Table 2: Comparative Results on Mini-ImageNet and CUB: The proposed method appears to im- prove the baseline model by over 4 percentage points, allowing it to set a new state-of-the-art result on both the 1/5-way Mini-ImageNet tasks. 5 - shot CUB Mini - ImageNet Test Accuracy - 1 - shot",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"CUB 200 5-way 1-shot\", \"Metric\": \"Accuracy\", \"Score\": \"70.46\"}}, {\"LEADERBOARD\": {\"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"CUB 200 5-way 1-shot\", \"Metric\": \"Accuracy\", \"Score\": \"67.48\"}}, {\"LEADERBOARD\": {\"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"CUB 200 5-way 5-shot\", \"Metric\": \"Accuracy\", \"Score\": \"85.63\"}}, {\"LEADERBOARD\": {\"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"CUB 200 5-way 5-shot\", \"Metric\": \"Accuracy\", \"Score\": \"83.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"Mini-Imagenet 5-way (1-shot)\", \"Metric\": \"Accuracy\", \"Score\": \"62.86\"}}, {\"LEADERBOARD\": {\"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"Mini-Imagenet 5-way (5-shot)\", \"Metric\": \"Accuracy\", \"Score\": \"77.64\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"Mini-ImageNet - 1-Shot Learning\", \"Metric\": \"Accuracy\", \"Score\": \"63.73%\"}} ]"
    },
    {
        "Context": "On cycles through two arcs in strong multipartite tournaments A multipartite tournament is an orientation of a complete c-partite graph. In [L. Volkmann, A remark on cycles through an arc in strongly connected multipartite tournaments, Appl. Math. Lett. 20 (2007) 1148-1150], Volkmann proved that a strongly connected cpartite tournament with c 3 contains an arc that belongs to a directed cycle of length m for every m \u2208 {3, 4, . . . , c}. He also conjectured the existence of three arcs with this property. In this note, we prove the existence of two such arcs.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Switchable Whitening for Deep Representation Learning Normalization methods are essential components in convolutional neural networks (CNNs). They either standardize or whiten data using statistics estimated in predefined sets of pixels. Unlike existing works that design normalization techniques for specific tasks, we propose Switchable Whitening (SW), which provides a general form unifying different whitening methods as well as standardization methods. SW learns to switch among these operations in an end-to-end manner. It has several advantages. First, SW adaptively selects appropriate whitening or standardization statistics for different tasks (see Fig.1), making it well suited fora wide range of tasks without manual design. Second, by integrating the benefits of different normalizers, SW shows consistent improvements over its counterparts in various challenging benchmarks. Third, SW serves as a useful tool for understanding the characteristics of whitening and standardization techniques.We show that SW outperforms other alternatives on image classification (CIFAR-10/100, ImageNet), semantic segmentation (ADE20K, Cityscapes), domain adaptation (GTA5, Cityscapes), and image style transfer (COCO). For example, without bells and whistles, we achieve state-ofthe-art performance with 45.33% mIoU on the ADE20K dataset. Code is available at https://github.com/ XingangPan/Switchable-Whitening. We evaluate SW on image classification (CIFAR-10/100, ImageNet), semantic segmentation (ADE20K, Cityscapes), domain adaptation (GTA5, Cityscapes), and image style transfer (COCO) Table 2. Test errors (%) on CIFAR-10/100 and ImageNet valida- tion sets [16]. For each model, we evaluate different normalization or whitening methods. SW a and SW b correspond to \u2126 = {bw, iw} and \u2126 = {bw, iw, bn, in, ln} respectively. Results on CIFAR are averaged over 5 runs. SW BW CIFAR - 10 SN SW has \u2126 = {bw , iw} . CIFAR - 100 SW a b SW BW ( a ) ResNet20 on CIFAR - 10 SN BN epochs Table 3. Results on Cityscapes and ADE20K datasets. 'ss' and 'ms' indicate single-scale and multi-scale test respectively. a idation set . * indicates our implementation . Method - ADE20K mIoU ss mIoU ( % ) mIoU ms Cityscapes Pixel Acc . ( % ) Table 4. Comparison with advanced methods on the ADE20K val- idation set. * indicates our implementation. Method mIoU(%) Pixel Acc.(%) DilatedNet [35] 32.31",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Synthetic-to-Real Translation\", \"Dataset\": \"GTAV-to-Cityscapes Labels\", \"Metric\": \"mIoU\", \"Score\": \"35.7\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-100\", \"Metric\": \"Percentage correct\", \"Score\": \"75.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Percentage correct\", \"Score\": \"90.7\"}} ]"
    },
    {
        "Context": "Circle Loss: A Unified Perspective of Pair Similarity Optimization This paper provides a pair similarity optimization viewpoint on deep feature learning, aiming to maximize the within-class similarity s p and minimize the between-class similarity s n . We find a majority of loss functions, including the triplet loss and the softmax cross-entropy loss, embed s n and s pinto similarity pairs and seek to reduce (s n \u2212 s p ). Such an optimization manner is inflexible, because the penalty strength on every single similarity score is restricted to be equal. Our intuition is that if a similarity score deviates far from the optimum, it should be emphasized. To this end, we simply re-weight each similarity to highlight the less-optimized similarity scores. It results in a Circle loss, which is named due to its circular decision boundary. The Circle loss has a unified formula for two elemental deep feature learning paradigms, i.e., learning with class-level labels and pair-wise labels. Analytically, we show that the Circle loss offers a more flexible optimization approach towards a more definite convergence target, compared with the loss functions optimizing (s n \u2212 s p ). Experimentally, we demonstrate the superiority of the Circle loss on a variety of deep feature learning tasks. On face recognition, person re-identification, as well as several finegrained image retrieval datasets, the achieved performance is on par with the state of the art. We comprehensively evaluate the effectiveness of Circle loss under two elemental learning approaches, i.e., learning with class-level labels and learning with pair-wise labels For the former approach, we evaluate our method on face recognition (Section 4.2) and person re-identification (Section 4.3) tasks For the latter approach, we use the fine-grained image retrieval datasets (Section 4.4), which are relatively small and encourage learning with pair-wise labels Table 1: Face identification and verification results on MFC1 dataset. \"Rank 1\" denotes rank-1 identification ac- curacy. \"Veri.\" denotes verification TAR (True Accepted Rate) at 1e-6 FAR (False Accepted Rate) with 1M dis- tractors. \"R34\" and \"R100\" denote using ResNet34 and ResNet100 backbones, respectively. Loss function R34 R100 Veri . ( % ) Rank 1 ( % ) Table 2: Face verification accuracy (%) on LFW, YTF and CFP-FP with ResNet34 backbone. LFW [ 10 ] YTF [ 37 ] CFP - FP [ 23 ] Table 3: Comparison of TARs on the IJB-C 1:1 verification task. Loss function LFW [ 10 ] YTF [ 37 ] CFP - FP [ 23 ] 1e - 5 1e - 4 1e - 3 Table 4: Evaluation of Circle loss on re-ID task. We report R-1 accuracy (%) and mAP (%). bone , Circle loss surpasses the most competitive one ( Ar -",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Face Recognition\", \"Dataset\": \"CFP-FP\", \"Metric\": \"Accuracy\", \"Score\": \"0.9602\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Recognition\", \"Dataset\": \"LFW\", \"Metric\": \"Accuracy\", \"Score\": \"0.9973\"}}, {\"LEADERBOARD\": {\"Task\": \"Person Re-Identification\", \"Dataset\": \"MSMT17\", \"Metric\": \"Rank-1\", \"Score\": \"76.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Person Re-Identification\", \"Dataset\": \"MSMT17\", \"Metric\": \"mAP\", \"Score\": \"52.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Person Re-Identification\", \"Dataset\": \"MSMT17\", \"Metric\": \"Rank-1\", \"Score\": \"76.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Person Re-Identification\", \"Dataset\": \"MSMT17\", \"Metric\": \"mAP\", \"Score\": \"50.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Person Re-Identification\", \"Dataset\": \"Market-1501\", \"Metric\": \"MAP\", \"Score\": \"87.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Person Re-Identification\", \"Dataset\": \"Market-1501\", \"Metric\": \"Rank-1\", \"Score\": \"96.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Person Re-Identification\", \"Dataset\": \"Market-1501\", \"Metric\": \"MAP\", \"Score\": \"84.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Person Re-Identification\", \"Dataset\": \"Market-1501\", \"Metric\": \"Rank-1\", \"Score\": \"94.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Metric Learning\", \"Dataset\": \"Stanford Online Products\", \"Metric\": \"R@1\", \"Score\": \"78.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Metric Learning\", \"Dataset\": \"CARS196\", \"Metric\": \"R@1\", \"Score\": \"83.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Metric Learning\", \"Dataset\": \"CUB-200-2011\", \"Metric\": \"R@1\", \"Score\": \"66.7\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"Mega\", \"Metric\": \"Accuracy\", \"Score\": \"81.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"IJB-A\", \"Metric\": \"TAR @ FAR=0.01\", \"Score\": \"87.8\"}} ]"
    },
    {
        "Context": "Finetuning Pretrained Transformers into RNNs Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. This comes with a significant computational overhead, as the attention mechanism scales with a quadratic complexity in sequence length. Efficient transformer variants have received increasing interest from recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train or yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving the efficiency while retaining the accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process needs lower training cost than training these recurrent variants from scratch. As many recent models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process.  Table 1: WikiText-103 language modeling results in perplexity. Train time is measured in GPU hours. The top two rows are our reimplementations of Katharopou- los et al. (2020) and Peng et al. (2021). Pretrain in- dicates initialization with a pretrained transformer for language modeling. T2R 75% indicates a model where every fourth layer from the top is kept as the original transformer layer. Perplexity (ppl.) is measured by pre- dicting the last 256 words out of the input of 512 con- secutive words. All models use 128 head dimensions. 470h 104h 98h 512h 97h 95h - 474h dev . ppl . test Table 2: Machine translation test results. The top two rows are our reimplementations of 80h 123h 90h 135h - 120h 82h EN - DE EN - FR WMT17 ZH - EN WMT14 Table 3: Language modeling hyperparameters when randomly initialized in the fairseq library. 4096 1e - 9 1e",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-French\", \"Metric\": \"BLEU score\", \"Score\": \"42.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"WMT2017 Chinese-English\", \"Metric\": \"BLEU\", \"Score\": \"23.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-German\", \"Metric\": \"BLEU score\", \"Score\": \"28.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-103\", \"Metric\": \"Test perplexity\", \"Score\": \"19.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-103\", \"Metric\": \"Validation perplexity\", \"Score\": \"19\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-French\", \"Metric\": \"BLEU score\", \"Score\": \"38.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-French\", \"Metric\": \"BLEU score\", \"Score\": \"34.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-103\", \"Metric\": \"Number of params\", \"Score\": \"23.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-103\", \"Metric\": \"Test perplexity\", \"Score\": \"29.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-103\", \"Metric\": \"Validation perplexity\", \"Score\": \"32.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-103\", \"Metric\": \"Test perplexity\", \"Score\": \"32.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-103\", "
    },
    {
        "Context": "WHEN ARE TWO ALGORITHMS THE SAME? People usually regard algorithms as more abstract than the programs that implement them. The natural way to formalize this idea is that algorithms are equivalence classes of programs with respect to a suitable equivalence relation. We argue that no such equivalence relation exists.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "On Delay Constrained Multicast Capacity of Large-Scale Mobile Ad-Hoc Networks This paper studies the delay constrained multicast capacity of large scale mobile ad hoc networks (MANETs). We consider a MANET consists of ns multicast sessions. Each multicast session has one source and p destinations. The wireless mobiles move according to a two-dimensional i.i.d. mobility model. Each source sends identical information to the p destinations in its multicast session, and the information is required to be delivered to all the p destinations within D time-slots. Given the delay constraint D, we first prove that the capacity per multicast session is O \" min n 1, (log p)(log (nsp)) q",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Implicit Class-Conditioned Domain Alignment for Unsupervised Domain Adaptation We present an approach for unsupervised domain adaptation-with a strong focus on practical considerations of within-domain class imbalance and between-domain class distribution shift-from a class-conditioned domain alignment perspective. Current methods for class-conditioned domain alignment aim to explicitly minimize a loss function based on pseudo-label estimations of the target domain. However, these methods suffer from pseudo-label bias in the form of error accumulation. We propose a method that removes the need for explicit optimization of model parameters from pseudo-labels directly. Instead, we present a sampling-based implicit alignment approach, where the sample selection procedure is implicitly guided by the pseudo-labels. Theoretical analysis reveals the existence of a domain-discriminator shortcut in misaligned classes, which is addressed by the proposed implicit alignment approach to facilitate domain-adversarial learning. Empirical results and ablation studies confirm the effectiveness of the proposed approach, especially in the presence of within-domain class imbalance and between-domain class distribution shift. We find it ineffective to incorporate prototype-based explicit alignment into MDD Table 1. Per-class average accuracy on Office-Home dataset with RS-UT label shifts (ResNet-50). Pr Avg Rw Cl Table 2. Accuracy (%) on Office-31 (standard) for unsupervised domain adaptation (ResNet-50). We repeated each experiment 5 times with different random seeds and report the average and the standard error of the accuracy. A Avg D W Table 3. Accuracy (%) on Office-Home (standard) for unsupervised domain adaptation (ResNet-50). Rw Pr Cl Ar Pr Avg Ar Cl Ar Pr Ar Cl Pr Ar Pr Rw Rw Pr Cl Ar Rw Cl Rw Rw Cl Table 4. VisDA2017 target accuracy (ResNet-50) acc . ( % ) Table 6. Per-class average accuracy (%) with mismatched prior where the source domain is balanced while the target domain is imbalanced. Table 7. Per-class average accuracy (%) with mismatched prior where the source domain is imbalanced while the target domain is balanced. Table 8. Per-class average accuracy (%) with",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Unsupervised Domain Adaptation\", \"Dataset\": \"Office-Home (RS-UT imbalance)\", \"Metric\": \"Average Per-Class Accuracy\", \"Score\": \"61.67\"}}, {\"LEADERBOARD\": {\"Task\": \"Unsupervised Domain Adaptation\", \"Dataset\": \"Office-Home (RS-UT imbalance)\", \"Metric\": \"Average Per-Class Accuracy\", \"Score\": \"58.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Unsupervised Domain Adaptation\", \"Dataset\": \"Office-Home (RS-UT imbalance)\", \"Metric\": \"Average Per-Class Accuracy\", \"Score\": \"56.91\"}}, {\"LEADERBOARD\": {\"Task\": \"Unsupervised Domain Adaptation\", \"Dataset\": \"Office-Home (RS-UT imbalance)\", \"Metric\": \"Average Per-Class Accuracy\", \"Score\": \"55.44\"}}, {\"LEADERBOARD\": {\"Task\": \"Unsupervised Domain Adaptation\", \"Dataset\": \"Office-Home (RS-UT imbalance)\", \"Metric\": \"Average Per-Class Accuracy\", \"Score\": \"52.81\"}}, {\"LEADERBOARD\": {\"Task\": \"Unsupervised Domain Adaptation\", \"Dataset\": \"Office-Home\", \"Metric\": \"Avg accuracy\", \"Score\": \"69.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Unsupervised Domain Adaptation\", \"Dataset\": \"Office-31\", \"Metric\": \"Avg accuracy\", \"Score\": \"88.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Unsupervised Domain Adaptation\", \"Dataset\": \"VisDA2017\", \"Metric\": \"Accuracy\", \"Score\": \"75.8\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Domain Adaptation\", \"Dataset\": \"Office-31\", \"Metric\": \"Average Accuracy\", \"Score\": \"89.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Domain Adaptation\", \"Dataset\": \"Office-Home\", \"Metric\": \"Accuracy\", \"Score\": \"72.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Domain Adaptation\", \"Dataset\": \"VisDA2017\", \"Metric\": \"Accuracy\", \"Score\": \"84.4\"}} ]"
    },
    {
        "Context": "End-to-end Learning of a Fisher Vector Encoding for Part Features in Fine-grained Recognition Part-based approaches for fine-grained recognition do not show the expected performance gain over global methods, although being able to explicitly focus on small details that are relevant for distinguishing highly similar classes. We assume that part-based methods suffer from a missing representation of local features, which is invariant to the order of parts and can handle a varying number of visible parts appropriately. The order of parts is artificial and often only given by ground-truth annotations, whereas viewpoint variations and occlusions result in parts that are not observable. Therefore, we propose integrating a Fisher vector encoding of part features into convolutional neural networks. The parameters for this encoding are estimated jointly with those of the neural network in an end-to-end manner. Our approach improves state-of-the-art accuracies for bird species classification on CUB-200-2011 from 90.40% to 90.95%, on NA-Birds from 89.20% to 90.30%, and on Birdsnap from 84.30% to 86.97%. We have evaluated our method on widely used datasets for fine-grained bird species categorization since it is the most challenging domain in our opinion For bird species classification, results are slightly below 90 % except for one approach that exceeds this value on one specific dataset In the following, we give a short description of the datasets used in our experiments is the most popular fine-grained dataset for benchmarking Besides the class labels for 200 bird species, the dataset provides additional annotations Table 1. Comparison of our proposed FVE with global average pooling (GAP) re- garding order and visibility of parts. separate estimation of GMM parameters after CNN training [ % ] Method Accuracy ( std ) Table 2. Comparison of our proposed FVE for part features with various state-of-the- art methods (bold = best per dataset). - CUB - 200 - 2011 NA - Birds Birdsnap - Table 3. Computation times for our proposed FVE-Layer. - CUB - 200 - 2011 NA - Birds Birdsnap -",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"CUB-200-2011\", \"Metric\": \"Accuracy\", \"Score\": \"90.95%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Part Segmentation\", \"Dataset\": \"ShapeNet-Part\", \"Metric\": \"Class Average IoU\", \"Score\": \"84.0\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Part Segmentation\", \"Dataset\": \"ShapeNet-Part\", \"Metric\": \"Instance Average IoU\", \"Score\": \"85.3\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Point Cloud Classification\", \"Dataset\": \"ScanObjectNN\", \"Metric\": \"Overall Accuracy\", \"Score\": \"73.7\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Point Cloud Classification\", \"Dataset\": \"ModelNet40\", \"Metric\": \"Overall Accuracy\", \"Score\": \"92.4\"}} ]"
    },
    {
        "Context": "A Single Stream Network for Robust and Real-time RGB-D Salient Object Detection Existing RGB-D salient object detection (SOD) approaches concentrate on the cross-modal fusion between the RGB stream and the depth stream. They do not deeply explore the effect of the depth map itself. In this work, we design a single stream network to directly use the depth map to guide early fusion and middle fusion between RGB and depth, which saves the feature encoder of the depth stream and achieves a lightweight and real-time model. We tactfully utilize depth information from two perspectives: (1) Overcoming the incompatibility problem caused by the great difference between modalities, we build a single stream encoder to achieve the early fusion, which can take full advantage of ImageNet pre-trained backbone model to extract rich and discriminative features. (2) We design a novel depth-enhanced dual attention module (DEDA) to efficiently provide the fore-/back-ground branches with the spatially filtered features, which enables the decoder to optimally perform the middle fusion. Besides, we put forward a pyramidally attended feature extraction module (PAFE) to accurately localize the objects of different scales. Extensive experiments demonstrate that the proposed model performs favorably against most state-of-the-art methods under different evaluation metrics. Furthermore, this model is 55.5% lighter than the current lightest model and runs at a real-time speed of 32 FPS when processing a 384 \u00d7 384 image. We evaluate the proposed model on six public RGB-D SOD datasets which are NJUD, RGBD135 NLPR, SSD, DUTLF-D and SIP Following most state-ofthe-art methods, we randomly select 1400 samples from the NJUD dataset and 650 samples from the NLPR dataset for training Their remaining images and other three datasets are used for testing We adopt several widely used metrics for quantitative evaluation: precision-recall (PR) curves, F-measure score, mean absolute error (MAE, M), the recently released S-measure (S m ) and E-measure (E m ) scores S-measure: It evaluates the spatial structure similarity by combining the region-aware structural similarity Sr and the object-aware structural similarity S o : where \u03b1 is set to 0.5 Table 1. Quantitative comparison. \u2191 and \u2193 indicate that the larger and smaller scores are better, respectively. Among the CNN-based methods, the best results are shown in red. The subscript in each model name is the publication year. \u03b2 Metric [ 46 ] [ 8 ] Ours [ 16 ] [ 26 ] CDCP17 DF17 [ 25 ] [ 18 ] [ 28 ] \u03b2 CTMF18 PCANet18 MMCI19 TANet19 CPFP19 DANet DMRA19 DANet DES14 DCMC16 [ 51 ] [ 3 ] VGG - 19 Traditional Methods [ 2 ] VGG - 16 [ 4 ] [ 7 ] Table 2. The model sizes and average speed of different methods. \u03b2 Metric [ 46 ] [ 8 ] Ours [ 16 ] [ 26 ] CDCP17 DF17 [ 25 ] [ 18 ] [ 28 ] \u03b2 CTMF18 PCANet18 MMCI19 TANet19 CPFP19 DANet DMRA19 DANet DES14 DCMC16 [ 51 ] [ 3",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"RGB-D Salient Object Detection\", \"Dataset\": \"NJU2K\", \"Metric\": \"Average MAE\", \"Score\": \"0.046\"}}, {\"LEADERBOARD\": {\"Task\": \"RGB-D Salient Object Detection\", \"Dataset\": \"NJU2K\", \"Metric\": \"S-Measure\", \"Score\": \"89.7\"}}, {\"LEADERBOARD\": {\"Task\": \"RGB-D Salient Object Detection\", \"Dataset\": \"NJU2K\", \"Metric\": \"max F-Measure\", \"Score\": \"90.5\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"RGB-D Salient Object Detection\", \"Dataset\": \"NJU2K\", \"Metric\": \"Average MAE\", \"Score\": \"0.037\"}}, { \"LEADERBOARD\": { \"Task\": \"RGB-D Salient Object Detection\", \"Dataset\": \"NJU2K\", \"Metric\": \"S-Measure\", \"Score\": \"91.1\"}} ]"
    },
    {
        "Context": "FUZZY FEEDBACK SCHEDULING OF RESOURCE-CONSTRAINED EMBEDDED CONTROL SYSTEMS The quality of control (QoC) of a resource-constrained embedded control system maybe jeopardized in dynamic environments with variable workload. This gives rise to the increasing demand of co-design of control and scheduling. To deal with uncertainties in resource availability, a fuzzy feedback scheduling (FFS) scheme is proposed in this paper. Within the framework of feedback scheduling, the sampling periods of control loops are dynamically adjusted using the fuzzy control technique. The feedback scheduler provides QoC guarantees in dynamic environments through maintaining the CPU utilization at a desired level. The framework and design methodology of the proposed FFS scheme are described in detail. A simplified mobile robot target tracking system is investigated as a case study to demonstrate the effectiveness of the proposed FFS scheme. The scheme is independent of task execution times, robust to measurement noises, and easy to implement, while incurring only a small overhead. To evaluate the performance of the proposed FFS scheme, this section conducts simulations for the case study system described in Section 4 Table 2. Look-up table for fuzzy feedback scheduler. - 1 \u03b7 ec - 1 Table 3. The nominal period for each control task is 3, 4 and 5 ms, respectively, where the period of non-control task 3 (i.e., h 3 ) is fixed. The maximum allowable sampling periods of two control loops are h max = 7 ms. 1 - 2 3 - 4 0 - 1 2 - 3 Table 3. Average execution times of the tasks. 1 - 2 3 - 4 0 - 1 2 - 3",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Intrusion-aware Alert Validation Algorithm for Cooperative Distributed Intrusion Detection Schemes of Wireless Sensor Networks Existing anomaly and intrusion detection schemes of wireless sensor networks have mainly focused on the detection of intrusions. Once the intrusion is detected, an alerts or claims will be generated. However, any unidentified malicious nodes in the network could send faulty anomaly and intrusion claims about the legitimate nodes to the other nodes. Verifying the validity of such claims is a critical and challenging issue that is not considered in the existing cooperative-based distributed anomaly and intrusion detection schemes of wireless sensor networks. In this paper, we propose a validation algorithm that addresses this problem. This algorithm utilizes the concept of intrusion-aware reliability that helps to provide adequate reliability at a modest communication cost. In this paper, we also provide a security resiliency analysis of the proposed intrusion-aware alert validation algorithm.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "\u00c5 \u00d1\u00d3\u00d6\u00dd \u00d2\u00d8 \u00d7 \u00d9\u00d0 \u00d2 \u00d3 \u00cb\u00d8\u00d6 \u00d7\u00d7 \u00d2\u00b9\u00cf \u00d2\u00d3 \u00d6 \u00b3\u00d7 \u00d1 \u00d8\u00d6 \u00dc \u00d1\u00d9\u00d0\u00d8 \u00d4\u00d0 \u00d8 \u00d3\u00d2 \u00d0 \u00d3\u00d6 \u00d8 \u00d1 * \u00d7\u00d8\u00d6 \u00d8 \u00cf \u00d4\u00d6\u00d3\u00d4\u00d3\u00d7 \u00d7 \u00da \u00d6 \u00d0 \u00d2 \u00db \u00d7 \u00d9\u00d0 \u00d7 \u00d3\u00d6 \u00cb\u00d8\u00d6 \u00d7\u00d7 \u00d2\u00b9\u00cf \u00d2\u00d3 \u00d6 \u00b3\u00d7 \u00d1 \u00d8\u00d6 \u00dc \u00d1\u00d9\u00d0\u00b9 \u00d8 \u00d4\u00d0 \u00d8 \u00d3\u00d2 \u00d0 \u00d3\u00d6 \u00d8 \u00d1\u00b8\u00d8 \u00dd \u00d6 \u00d9 \u00d8 \u00dc\u00d8\u00d6 \u00d1 \u00d1\u00d3\u00d6\u00dd \u00d0\u00d0\u00d3 \u00d8 \u00d3\u00d2 \u00d6 \u00d5\u00d9 \u00d6 \u00b9 \u00d1 \u00d2\u00d8\u00d7 \u00dd \u00d8 \u00d6 \u00d6 \u00d2\u00d8 \u00d1 \u00d2\u00d7 \u00dd \u00d2\u00d8\u00d6\u00d3 \u00d9 \u00d2 \u00db \u00d4\u00d6 \u00b9 \u00d8 \u00d3\u00d2\u00d7\u00b8 \u00dd \u00d3\u00da \u00d6\u00db\u00d6 \u00d8 \u00d2 \u00d8 \u00d2\u00d4\u00d9\u00d8 \u00d1 \u00d8\u00d6 \u00d7\u00b8\u00d3\u00d6 \u00dd \u00d9\u00d7 \u00d2 \u00d6\u00d7\u00d8 \u00d6 \u00d9\u00d6\u00d7 \u00da \u00d0 \u00da \u00d0 \u00d3 \u00d0 \u00d7\u00b9 \u00d7 \u00d0 \u00d1\u00d9\u00d0\u00d8 \u00d4\u00d0 \u00d8 \u00d3\u00d2\u00ba \u00c1\u00d2 \u00d4 \u00d6\u00d8 \u00d9\u00d0 \u00d6\u00b8\u00db \u00d7 \u00d3\u00db \u00d8\u00db\u00d3 \u00d9\u00d0\u00d0\u00dd \u00d2\u00b9\u00d4\u00d0 \u00d7 \u00d9\u00d0 \u00d7 \u00d3\u00d2 \u00da \u00d2 \u00d8 \u00d7 \u00d1 \u00d2\u00d9\u00d1 \u00d6 \u00d3 \u00d3\u00d4 \u00d6 \u00d8 \u00d3\u00d2\u00d7\u00b8 \u00d8 \u00d2\u00d4\u00d9\u00d8 \u00d1 \u00d8\u00d6 \u00d7 \u00d2 \u00d3\u00da \u00d6\u00db\u00d6 \u00d8\u00d8 \u00d2 \u00d8 \u00d3\u00d8 \u00d6 \u00d3\u00d2 \u00b8\u00d7\u00d0 \u00d8\u00d0\u00dd \u00d2\u00d6 \u00d7 \u00d2 \u00d8 \u00d3\u00d2\u00d7\u00d8 \u00d2\u00d8 \u00d3 \u00d8 \u00d0 \u00d2 \u00d8 \u00d6\u00d1 \u00d3 \u00d8 \u00d3\u00d1\u00d4\u00d0 \u00dc \u00d8\u00dd\u00b8 \u00d8 \u00d2\u00d4\u00d9\u00d8 \u00d1 \u00d8\u00d6 \u00d7 \u00d6 \u00d6 \u00b9\u00d3\u00d2\u00d0\u00dd\u00ba \u00c5 \u00d2\u00dd \u00d3 \u00d8 \u00d7 \u00d7 \u00d9\u00d0 \u00d7 \u00da \u00d2 \u00d3\u00d9\u00d2 \u00dd \u00d2 \u00d1\u00d4\u00d0 \u00d1 \u00d2\u00d8 \u00d8 \u00d3\u00d2 \u00d3 \u00d2 \u00dc \u00d9\u00d7\u00d8 \u00da \u00d7 \u00d6 \u00d0 \u00d3\u00d6 \u00d8 \u00d1 \u00d7 \u00d3\u00d2 \u00d4 \u00d0 \u00d1 \u00ba \u00c3 \u00dd\u00db\u00d3\u00d6 \u00d7 \u00c5 \u00d8\u00d6 \u00dc \u00d1\u00d9\u00d0\u00d8 \u00d4\u00d0 \u00d8 \u00d3\u00d2\u00b8\u00cb\u00d8\u00d6 \u00d7\u00d7 \u00d2\u00b9\u00cf \u00d2\u00d3 \u00d6 \u00b3\u00d7 \u00d0 \u00d3\u00d6 \u00d8 \u00d1\u00b8\u00c5 \u00d1\u00d3\u00d6\u00dd \u00d4\u00d0 \u00d1 \u00d2\u00d8\u00ba \u00bd \u00c1\u00d2\u00d8\u00d6\u00d3 \u00d9\u00d8 \u00d3\u00d2 \u00cb\u00d8\u00d6 \u00d7\u00d7 \u00d2\u00b3\u00d7 \u00d0 \u00d3\u00d6 \u00d8 \u00d1 \u00bd \u2104 \u00db \u00d7 \u00d8 \u00d6\u00d7\u00d8 \u00d7\u00d9 \u00b9\u00d9 \u00d0 \u00d3\u00d6 \u00d8 \u00d1 \u00d3\u00d6 \u00d1 \u00d8\u00d6 \u00dc \u00d1\u00d9\u00d0\u00d8 \u00d4\u00d0 \u00b9 \u00d8 \u00d3\u00d2\u00ba \u00c1\u00d8\u00d7 \u00d1\u00d4\u00d6\u00d3\u00da \u00d1 \u00d2\u00d8 \u00dd \u00cf \u00d2\u00d3 \u00d6 \u00bd \u2104 \u00d0 \u00d8\u00d3 \u00d0\u00dd \u00d4\u00d6 \u00d8 \u00d0 \u00d0 \u00d3\u00d6 \u00d8 \u00d1\u00ba * \u00c5\u00b8\u00be\u00bc\u00bc \u00ba \u00cc \u00d7 \u00d7 \u00d8 \u00d9\u00d8 \u00d3\u00d6\u00b3\u00d7 \u00da \u00d6\u00d7 \u00d3\u00d2 \u00d3 \u00d8 \u00db\u00d3\u00d6 \u00ba \u00c1\u00d8 \u00d7 \u00d4\u00d3\u00d7\u00d8 \u00d6 \u00dd \u00d4 \u00d6\u00d1 \u00d7\u00d7 \u00d3\u00d2 \u00d3 \u00c5 \u00d3\u00d6 \u00dd\u00d3\u00d9\u00d6 \u00d4 \u00d6\u00d7\u00d3\u00d2 \u00d0 \u00d9\u00d7 \u00ba AE\u00d3\u00d8 \u00d3\u00d6 \u00d6 \u00d7\u00d8\u00d6 \u00d9\u00d8 \u00d3\u00d2\u00ba \u00cc \u00d2 \u00d8 \u00da \u00da \u00d6\u00d7 \u00d3\u00d2 \u00db \u00d7 \u00d4\u00d9 \u00d0 \u00d7 \u00d2 \u00c1\u00cb\u00cb \u00be\u00bc\u00bc \u00ba \u2020 \u00c4 \u00d3\u00d6 \u00d8\u00d3 \u00d6 \u00c2\u00ba \u00c3\u00d9\u00d2\u00d8\u00de\u00d1 \u00d2\u00d2\u00b8\u00cd\u00d2 \u00da \u00d6\u00d7 \u00d8 \u00d6 \u00d2\u00d3 \u00d0 \u00ba \u00bd\u00b8\u00d6\u00d9 \u00d7 \u00c5 \u00d8 \u00d1 \u00d8 \u00d5\u00d9 \u00d7\u00b8\u00d9\u00d1\u00d6 AE\u00ca\u00cb \u00be\u00be \u00b8 \u00d4 \u00bf \u00b8 \u00bf \u00bc \u00bd \u00d6 \u00d2\u00d3 \u00d0 \u00b8 \u00d6 \u00d2 \u00b8 \u00d4\u00d6\u00d3\u00d8 \u00d8 \u00cc\u00bd \u00d8 \u00dc\u00d8 \u00d6 \u00d0 \u00d8 \u00d6 \u00ba \u00d3\u00dd \u00d6\u00b8\u00c2 \u00d2\u00b9 \u00d9 \u00d0\u00d0 \u00d9\u00d1 \u00ba \u00d9\u00d1 \u00d7 \u00d4\u00d6\u00d3\u00d8 \u00d8 \u00cc\u00bd \u00d8 \u00dc\u00d8 \u00d6 \u00d6 \u00d8 \u00d1 \u00ba \u00d6 \u2021 \u00c4 \u00d3\u00d6 \u00d8\u00d3 \u00d6 \u00c4\u00c1 \u00b8\u00cd\u00d2 \u00da \u00d6\u00d7 \u00d8 \u00d6 \u00d2\u00d3 \u00d0 \u00ba \u00d9\u00d1\u00d6 AE\u00ca\u00cb\u00b8 \u00bf \u00bf\u00bf\u00bc \u00c5\u00d3\u00d2\u00d8 \u00d3\u00d2\u00d2\u00d3\u00d8\u00b8 \u00d6 \u00d2 \u00ba \u00d0 \u00d1 \u00d2\u00d8\u00ba\u00c8 \u00d6\u00d2 \u00d8 \u00d1 \u00ba \u00d6 \u00a7 \u00cb \u00d3\u00d3\u00d0 \u00d3 \u00d3\u00d1\u00d4\u00d9\u00d8 \u00d6 \u00cb \u00d2 \u00b8\u00cd\u00d2 \u00da \u00d6\u00d7 \u00d8\u00dd \u00d3 \u00cf \u00d8 \u00d6\u00d0\u00d3\u00d3\u00b8\u00cf \u00d8 \u00d6\u00d0\u00d3\u00d3\u00b8\u00c7AE\u00b8AE\u00be \u00bf \u00bd\u00b8 \u00d2 \u00ba \u00db\u00be\u00de \u00d3\u00d9\u00d9\u00db \u00d8 \u00d6\u00d0\u00d3\u00d3\u00ba \u00bd",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Direct Output Connection fora High-Rank Language Model This paper proposes a state-of-the-art recurrent neural network (RNN) language model that combines probability distributions computed not only from a final RNN layer but also from middle layers. Our proposed method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by Yang et al. (2018). The proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate our proposed method contributes to two application tasks: machine translation and headline generation. Our code is publicly available at: https://github.com/nttcslabnlp/doc lm. Moreover, we evaluate various combinations of layers to explore which combination achieves the best score We used the Penn Treebank (PTB) and WikiText-2 datasets, which are the standard benchmark datasets for the word-level language modeling task and respectively published preprocessed PTB 3 and WikiText-2 4 datasets We used these preprocessed datasets for fair comparisons with previous studies To investigate the effect of DOC on an encoder-decoder model, we incorporate DOC into the decoder and examine its performance We used the Wall Street Journal of the Penn Treebank dataset We applied the preprocessing codes of Choe and Charniak (2016) 12 to the dataset and converted a token that appears fewer than ten times in the training dataset into a special token unk To investigate the effectiveness of DOC, we evaluate our language models following their configurations Table 1: Statistics of PTB and WikiText-2. 12 650 960 WikiText - 2 300 620 15 60 280 1150 20 PTB Table 3: Perplexities of AWD-LSTM with DOC on the PTB dataset. We varied the number of probability dis- tributions from each layer in situation J = 20 except for the top row. The top row ( \u2020) represents MoS scores reported in Yang et al. (2018) as a baseline. \u2021 represents the perplexity obtained by the implementation of Yang et al. (2018) 6 with identical hyperparameters except for i 3 . \u03b2 #DOC Valid Test i0 i1 i2 \u03bb Table 4: Coefficient of variation of Equation 10: \u221a \u03b2 in validation and test sets of PTB. \u03b2 Valid Test \u03bb Table 5: Rank of output matrix (\u00c3 in Equation 9) on the PTB dataset. D 3 of AWD-LSTM is 400. 60 AWD - LSTM - DOC Valid Test Model 10000",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-2\", \"Metric\": \"Number of params\", \"Score\": \"185M\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-2\", \"Metric\": \"Test perplexity\", \"Score\": \"53.09\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-2\", \"Metric\": \"Validation perplexity\", \"Score\": \"54.19\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-2\", \"Metric\": \"Number of params\", \"Score\": \"37M\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-2\", \"Metric\": \"Test perplexity\", \"Score\": \"58.03\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-2\", \"Metric\": \"Validation perplexity\", \"Score\": \"60.29\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"Penn Treebank (Word Level)\", \"Metric\": \"Params\", \"Score\": \"185M\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"Penn Treebank (Word Level)\", \"Metric\": \"Test perplexity\", \"Score\": \"47.17\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"Penn Treebank (Word Level)\", \"Metric\": \"Validation perplexity\", \"Score\": \"48.63\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"Penn Treebank (Word Level)\", \"Metric\": \"Params\", \"Score\": \"23M\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"Penn Treebank (Word Level)\", \"Metric\": \"Test perplexity\", \"Score\": \"52.38\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"Penn Treebank (Word Level)\", \"Metric\": \"Validation perplexity\", \"Score\": \"54.12\"}}, {\"LEADERBOARD\": {\"Task\": \"Constituency Parsing\", \"Dataset\": \"Penn Treebank\", \"Metric\": \"F1 score\", \"Score\": \"94.47\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-2\", \"Metric\": \"Number of params\", \"Score\": \"33M\"}}, { \"LEADERBOARD\": { \"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-2\", \"Metric\": \"Test perplexity\", \"Score\": \"43.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-2\", \"Metric\": \"Validation perplexity\", \"Score\": \"41.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-2\", \"Metric\": \"Test perplexity\", \"Score\": \"61.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-2\", \"Metric\": \"Validation perplexity\", \"Score\": \"64.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-2\", \"Metric\": \"Test perplexity\", \"Score\": \"61.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-2\", 'Met"
    },
    {
        "Context": "Why Systems-on-Chip Needs More UML like a Hole in the Head Let's be clear from the outset: SoC can most certainly make use of UML; SoC just doesn't need more UML, or even all of it. The advent of model mappings, coupled with marks that indicate which mapping rule to apply, enable a major simplification of the use of UML in SoC.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Howl: A Deployed, Open-Source Wake Word Detection System We describe Howl, an open-source wake word detection toolkit with native support for open speech datasets, like Mozilla Common Voice and Google Speech Commands. We report benchmark results on Speech Commands and our own freely available wake word detection dataset, built from MCV. We operationalize our system for Firefox Voice, a plugin enabling speech interactivity for the Firefox web browser. Howl represents, to the best of our knowledge, the first fully productionized yet open-source wake word detection toolkit with a web browser deployment target. Our codebase is at https://github.com/ castorini/howl.  Table 1: Model accuracy on Google Speech Com- mands. Bolded denotes the best and # par. the number of parameters. 202K 94 . 3 / 94 . 5 128K 96 . 8 / 97 . 1 478K 107K 97 . 0 / 97 . 8 111K 250K Dev / Test # Par . 110K",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Keyword Spotting\", \"Dataset\": \"Google Speech Commands\", \"Metric\": \"Google Speech Commands V1 12\", \"Score\": \"97.8\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Keyword Spotting\", \"Dataset\": \"Google Speech Commands\", \"Metric\": \"Google Speech Commands V2 12\", \"Score\": \"95.6\"}}, { \"LEADERBOARD\": { \"Task\": \"Keyword Spotting\", \"Dataset\": \"Google Speech Commands\", \"Metric\": \"Google Speech Commands V2 12\", \"Score\": \"97.7\"}} ]"
    },
    {
        "Context": "Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Following the evaluation protocol in, each test triple (h, r, t) is measured under two scenarios: head focused (?, r, t) and tail focused (h, r, ?) Those true triples observed in either train/validation/test set except the test triple will be excluded during evaluation Specially, for FB15k-237, we set embedding dimension d = 400, sub-embedding dimension d s = 20, and the learning rates to 2e-3 and 2e-4 for pre-training and fine-tuning stages respectively; for WN18RR dataset, we set d = 400, d s = 4, and the learning rates to 1e-4 and 3e-5 for pre-training and fine-tuning stages Table 1: Statistics of datasets. test leakage due to redundant inverse relation . Two commonly used benchmark datasets ( FB15k - 4 14 , 541 FB15k - 237 WN18RR WN18 consists of 18 relations and However , many text triples ob - Experiments Table 2: Link prediction for FB15k-237 and WN18RR on test sets. Model FB15k - 237 H10 WN18RR H1 H3 MRR - Table 3: Ablation study on FB15k-237 validation set. #param MRR @10 Table 4: H@10 from FB15-237 validation set by cate- gories (1-to-N, N-to-1 and N-to-N). RotatE - L GC - OTE A T H",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@1\", \"Score\": \"0.442\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@10\", \"Score\": \"0.583\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@3\", \"Score\": \"0.511\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"MR\", \"Score\": \"2715\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"MRR\", \"Score\": \"0.491\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"Hits@1\", \"Score\": \"0.267\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"Hits@10\", \"Score\": \"0.550\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"Hits@3\", \"Score\": \"0.396\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"MR\", \"Score\": \"154\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"MRR\", \"Score\": \"0.361\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@1\", \"Score\": \"0.490\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@10\", \"Score\": \"0.581\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@3\", \"Score\": \"0.495\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"MRR\", \"Score\": \"0.481\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"Hits@1\", \"Score\": \"0.261\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"Hits@10\", \"Score\": \"0.579\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"Hits@3\", \"Score\": \"0.319\"}}, { \"LEADERBOARD\": { 'Ta"
    },
    {
        "Context": "Estimating individual treatment effect: generalization bounds and algorithms There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give anew theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a \"balanced\" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art. One is by using synthetic or semi-synthetic datasets, where the outcome or treatment assignment are fully known; we use the semi-synthetic IHDP dataset from We partially overcome this problem by using the Jobs dataset from LaLonde (1986), which includes both a randomized and a non-randomized component We use both for training, but can only use the randomized component for evaluation This alleviates, but does not solve, the issue of a completely balanced dataset being unsuited for our method We evaluate our framework CFR, and its variant without Algorithm 1 CFR: Counterfactual regression with integral probability metrics 1: Input: Factual sample (x 1 , t 1 , y 1 ), We evaluate our model in two different settings On the Jobs dataset, we use the policy risk on the validation set Table 1. Results on IHDP (left) and Jobs (right). MMD is squared linear MMD. Lower is better. Within-sample IHDP JOBS \u221a Random policy of Out - of - sample Treatment inclusion rate CFR ATT ATE JOBS CFR MMD PEHE Causal Forests IHDP risk \u221a RPOL BART",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Causal Inference\", \"Dataset\": \"IDHP\", \"Metric\": \"Average Treatment Effect Error\", \"Score\": \"0.27\"}}, {\"LEADERBOARD\": {\"Task\": \"Causal Inference\", \"Dataset\": \"IDHP\", \"Metric\": \"Average Treatment Effect Error\", \"Score\": \"0.28\"}}, {\"LEADERBOARD\": {\"Task\": \"Causal Inference\", \"Dataset\": \"IDHP\", \"Metric\": \"Average Treatment Effect Error\", \"Score\": \"0.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Causal Inference\", \"Dataset\": \"IDHP\", \"Metric\": \"Average Treatment Effect Error\", \"Score\": \"0.42\"}}, {\"LEADERBOARD\": {\"Task\": \"Causal Inference\", \"Dataset\": \"IDHP\", \"Metric\": \"Average Treatment Effect Error\", \"Score\": \"0.79\"}}, {\"LEADERBOARD\": {\"Task\": \"Causal Inference\", \"Dataset\": \"IDHP\", \"Metric\": \"Average Treatment Effect Error\", \"Score\": \"0.93\"}}, {\"LEADERBOARD\": {\"Task\": \"Causal Inference\", \"Dataset\": \"IDHP\", \"Metric\": \"Average Treatment Effect Error\", \"Score\": \"0.96\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Multi-day Continuous Classification\", \"Dataset\": \"USHCN-Daily\", \"Metric\": \"MSE (10-2, 50% missing)\", \"Score\": \"0.748\"}} ]"
    },
    {
        "Context": "Enhanced Residual Networks for Context-based Image Outpainting Although humans perform well at predicting what exists beyond the boundaries of an image, deep models struggle to understand context and extrapolation through retained information. This task is known as image outpainting and involves generating realistic expansions of an images boundaries. Current models use generative adversarial networks to generate results which lack localized image feature consistency and appear fake. We propose two methods to improve this issue: the use of a local and global discriminator, and the addition of residual blocks within the encoding section of the network. Comparisons of our model and the baselines L1 loss, mean squared error (MSE) loss, and qualitative differences reveal our model is able to naturally extend object boundaries and produce more internally consistent images compared to current methods but produces lower fidelity images. 2 To evaluate the effectiveness of our model we compare MSE and L1 loss between each of our models We use MIT CSAIL Places365-Standard, a large scale scene dataset used to train models for image context and recognition This dataset contains around 2 million images, combined, of random scenes from outdoor and indoor scenery, including both simple landscapes and detailed, object-heavy images We use this dataset as our primary method of evaluation because of its usage in Van Hoorick Unfortunately, due to computing time constraints, training on the whole of the dataset was deemed implausible, therefore we drastically reduced the size of the training set to approximately 25,000 images and trained for 50 epochs During this process, we kept images which were chosen randomly from the full dataset, a process which adds some inherent weaknesses to the training of our model To address the weakness which training on a subset of the Table 1. Architecture of generator G, including parameters. Trans-Conv is transposed convolution. Table 4. Comparison of all loss results across each of the models. Adversarial L1 MSE",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Outpainting\", \"Dataset\": \"Places365-Standard\", \"Metric\": \"Adversarial\", \"Score\": \"0.0941\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Outpainting\", \"Dataset\": \"Places365-Standard\", \"Metric\": \"L1\", \"Score\": \"0.08\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Outpainting\", \"Dataset\": \"Places365-Standard\", \"Metric\": \"MSE\", \"Score\": \"0.7814\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Outpaining\", \"Dataset\": \"Places365-Standard\", \"Metric\": \"L1\", \"Score\": \"0.0791\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Outpaining\", \"Dataset\": \"Places365-Standard\", \"Metric\": \"MSE\", \"Score\": \"0.023\"}} ]"
    },
    {
        "Context": "Divide-and-conquer based Large-Scale Spectral Clustering Spectral clustering is one of the most popular clustering methods. However, how to balance the efficiency and effectiveness of the large-scale spectral clustering with limited computing resources has not been properly solved fora longtime. In this paper, we propose a divide-and-conquer based largescale spectral clustering method to strike a good balance between efficiency and effectiveness. In the proposed method, a divide-and-conquer based landmark selection algorithm and a novel approximate similarity matrix approach are designed to construct a sparse similarity matrix within low computational complexities. Then clustering results can be computed quickly through a bipartite graph partition process. The proposed method achieves the lower computational complexity than most existing large-scale spectral clustering methods. Experimental results on ten large-scale datasets have demonstrated the efficiency and effectiveness of the proposed methods. The MATLAB code of the proposed method and experimental datasets are available at https://github.com/Li-Hongmin/MyPaperWithCode. In this section, we conduct experiments on five real and five synthetic datasets to evaluate the performance of the proposed DnC-SC methods Our experiments are conducted on eight large-scale datasets, varying from nine thousand to as large as twenty million data shows the synthetic datasets The properties of the datasets are summarized in We adopt two widely used evaluation metrics, i.e., Normalized Mutual Information (NMI) and Accuracy (ACC), to evaluate the clustering results.., ] be the data matrix In the experiments, = 200 is used for the datasets whose size is less than 100,000, otherwise = 50 Table 2 Properties of the real and synthetic datasets. 7 2 3 #Dimension #Class Table 3 Clustering performance (ACC% \u00b1 std) for large-scale spectral clustering methods Clustering performance ( NMI% \u00b1 std ) for large - scale spectral clustering methods Clustering performance ( ACC% \u00b1 std ) for large - scale spectral clustering methods SC LSC - R KM DnC - SC Nystr\u00f6m LSC - RH U - SPEC Table 3 LSC - K LSC - KH Table 4 Clustering performance (NMI% \u00b1 std) for large-scale spectral clustering methods Clustering performance ( NMI% \u00b1 std ) for large - scale spectral clustering methods Clustering performance ( ACC% \u00b1 std ) for large - scale spectral clustering methods SC LSC - R KM DnC - SC Nystr\u00f6m LSC - RH U - SPEC Table 3 LSC - K LSC - KH Table 5 Time costs(s) of large-scale spectral clustering methods. SC LSC",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image/Document Clustering\", \"Dataset\": \"pendigits\", \"Metric\": \"Accuracy (%)\", \"Score\": \"81.55\"}}, {\"LEADERBOARD\": {\"Task\": \"Image/Document Clustering\", \"Dataset\": \"pendigits\", \"Metric\": \"NMI\", \"Score\": \"79.15\"}}, {\"LEADERBOARD\": {\"Task\": \"Image/Document Clustering\", \"Dataset\": \"pendigits\", \"Metric\": \"runtime (s)\", \"Score\": \"0.77\"}}, {\"LEADERBOARD\": {\"Task\": \"Image/Document Clustering\", \"Dataset\": \"pendigits\", \"Metric\": \"Accuracy (%)\", \"Score\": \"74.02\"}}, {\"LEADERBOARD\": {\"Task\": \"Image/Document Clustering\", \"Dataset\": \"pendigits\", \"Metric\": \"NMI\", \"Score\": \"81.37\"}}, {\"LEADERBOARD\": {\"Task\": \"Image/Document Clustering\", \"Dataset\": \"pendigits\", \"Metric\": \"runtime (s)\", \"Score\": \"1.20\"}}, {\"LEADERBOARD\": {\"Task\": \"Image/Document Clustering\", \"Dataset\": \"pendigits\", \"Metric\": \"Accuracy (%)\", \"Score\": \"81.68\"}}, {\"LEADERBOARD\": {\"Task\": \"Image/Document Clustering\", \"Dataset\": \"pendigits\", \"Metric\": \"NMI\", \"Score\": \"81.68\"}}, {\"LEADERBOARD\": {\"Task\": \"Image/Document Clustering\", \"Dataset\": \"pendigits\", \"Metric\": \"runtime (s)\", \"Score\": \"2.07\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"Den-full\", \"Metric\": \"Accuracy\", \"Score\": \"0.965\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"Den-full\", \"Metric\": \"NMI\", \"Score\": \"0.638\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"Darmstadt Noise Dataset\", \"Metric\": \"Accuracy\", \"Score\": \"0.951\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"Darmstadt Noise Dataset\", \"Metric\": \"NMI\", \"Score\": \"0.942\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"FRGC\", \"Metric\": \"Accuracy\", \"Score\": \"0.972\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"FRGC\", \"Metric\": \"NMI\", \"Score\": \"0.882\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"Darmstadt Noise Dataset\", \"Metric\": \"Accuracy\", \"Score\": \"0.929\"}}, { \"LEADERBOARD\": { 'Task'"
    },
    {
        "Context": "Bottom-up rewriting for words and terms For the whole class of linear term rewriting systems, we define bottom-up rewriting which is a restriction of the usual notion of rewriting. We show that bottom-up rewriting effectively inverse-preserves recognizability and analyze the complexity of the underlying construction. The Bottom-Up class (BU) is, by definition, the set of linear systems for which every derivation can be replaced by a bottom-up derivation. Membership to BU turns out to be undecidable, we are thus lead to define more restricted classes: the classes SBU(k), k \u2208 N of Strongly Bottom-Up(k) systems for which we show that membership is decidable. We define the class of Strongly Bottom-Up systems by SBU = k\u2208N SBU(k). We give a polynomial sufficient condition fora system to be in SBU. The class SBU contains (strictly) several classes of systems which were already known to inverse preserve recognizability: the inverse left-basic semi-Thue systems (viewed as unary term rewriting systems), the linear growing term rewriting systems, the inverse Linear-Finite-Path-Ordering systems. new decidable call-by-need [12] class, decidability results for confluence, accessibility, joinability. Also, recently, this notion has been used to prove termination of systems for which none of the already known termination techniques work [18]. Such a preservation property is also a tool for studying the recognizable/rational subsets of various monoids which are defined by a presentation X, R , where X is a finite alphabet and Ra Thue system (see for example [25,26]). Consequently, the seek of new decidable classes of systems which preserve (or inverse preserve) recognizability is worthwile.Many such classes defined so far have been defined by imposing syntactical restrictions on the rewrite rules. For instance, in growing systems ([21, 27]) variables at depth strictly greater than 1 in the left-handside of a rule cannot appear in the corresponding right-handside. Finite-path Overlapping systems [35] are also defined by syntactic restrictions on the system. The class of Finite-path Overlapping systems contains the class of growing systems [27]. Previous works on semi-Thue systems also prove recognizability preservation, under syntactic restrictions: cancellation systems [2], monadic systems [4], basic systems [1], and left-basic systems [30] (see [32] fora survey).Other works establish that some strategies i.e. restrictions on the derivations rather than on the rules, ensure preservation of recognizability. Various such strategies were studied in [16], [29], [33].We rather follow here this second approach: we define anew rewriting strategy which we call bottom-up rewriting for linear term rewriting systems. The bottom-up derivations are, intuitively, those derivations in which the rules are applied, roughly speaking, from the bottom of the term towards the top (this set of derivations contains strictly the bottom-up derivations of [29] and the one-pass leaf-started derivations of [16]). An important feature of this strategy, as opposed to the ones quoted above, is that it allows overlaps between successive applications of rules. A class of systems is naturally associated with this strategy: it consists of the systems R for which the binary relation \u2192 * R coincides with its restriction to the bottom-up strategy. We call \"bottom-up\" such systems and denote by BU the set of all bottom-up systems.Overview of the paper. Most of the results proved in this paper were announced in [13], which can thus be considered as a medium-scale overview of this paper. Let us give here a large-scale overview, section by section, of the contents of the paper. the subject of tree-automata and to [22] for term rewriting.2.1. Sets, binary relations Abstract rewriting. Given a set E, we denote by P(E) its powerset i.e.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Voice-controlled Debugging of Spreadsheets Voice-controlled Debugging of Spreadsheets Computing are putting pressure on the software industry to research new modes of interaction that do not rely on the traditional keyboard and mouse combination. Computer users suffering from Repetitive Strain Injury also seek an alternative to keyboard and mouse devices to reduce suffering in wrist and finger joints. Voice-control is an alternative approach to spreadsheet development and debugging that has been researched and used successfully in other domains. While voice-control technology for spreadsheets is available its effectiveness has not been investigated. This study is the first to compare the performance of a set of expert spreadsheet developers that debugged a spreadsheet using voice-control technology and another set that debugged the same spreadsheet using keyboard and mouse. The study showed that voice, despite its advantages, proved to be slower and less accurate. However, it also revealed ways in which the technology might be improved to redress this imbalance.  Table 4: Average time to enter numbers in seconds Group",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "STOCHASTIC VARIATIONAL VIDEO PREDICTION Predicting the future in real-world settings, particularly from raw sensory observations such as images, is exceptionally challenging. Real-world events can be stochastic and unpredictable, and the high dimensionality and complexity of natural images require the predictive model to build an intricate understanding of the natural world. Many existing methods tackle this problem by making simplifying assumptions about the environment. One common assumption is that the outcome is deterministic and there is only one plausible future. This can lead to low-quality predictions in real-world settings with stochastic dynamics. In this paper, we develop a stochastic variational video prediction (SV2P) method that predicts a different possible future for each sample of its latent variables. To the best of our knowledge, our model is the first to provide effective stochastic multi-frame prediction for real-world videos. We demonstrate the capability of the proposed method in predicting detailed future frames of videos on multiple real-world datasets, both action-free and action-conditioned. We find that our proposed method produces substantially improved video predictions when compared to the same model without stochasticity, and to other stochastic video prediction methods. Our SV2P implementation will be open sourced upon publication. To highlight the importance of stochasticity in video prediction, we created a toy video dataset with intentionally stochastic motion Each video in this dataset is four frames long To evaluate SV2P, we test it on three real-world video datasets by comparing it to the CDNA model, as a deterministic baseline, as well as a baseline that outputs the last seen frame as the prediction We quantitatively and qualitatively evaluate SV2P on following real-world datasets: \u2022 BAIR robot pushing dataset: This dataset contains action-conditioned videos collected by a Sawyer robotic arm pushing a variety of objects All of the videos in this datasets have similar tabletop settings with static background An interesting property of this dataset is the fact that the arm movements are quite unpredictable in the absence of actions (compared to the robot pushing dataset which the arm moves to the center of the bin) For this dataset, we train Table 1: Hyper-parameters used for experiments. Inference Network 50000 Optimization model type Generative Network ADAM 200000 # of masks 16 batch size CDNA # of iterations 10",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Video Generation\", \"Dataset\": \"BAIR Robot Pushing\", \"Metric\": \"FVD score\", \"Score\": \"262.5\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Video Generation\", \"Dataset\": \"BAIR Robot Pushing\", \"Metric\": \"FVD score\", \"Score\": \"90.9\"}} ]"
    },
    {
        "Context": "Unprocessing Images for Learned Raw Denoising Machine learning techniques work best when the data used for training resembles the data used for evaluation. This holds true for learned single-image denoising algorithms, which are applied to real raw camera sensor readings but, due to practical constraints, are often trained on synthetic image data. Though it is understood that generalizing from synthetic to real images requires careful consideration of the noise properties of camera sensors, the other aspects of an image processing pipeline (such as gain, color correction, and tone mapping) are often overlooked, despite their significant effect on how raw measurements are transformed into finished images. To address this, we present a technique to \"unprocess\" images by inverting each step of an image processing pipeline, thereby allowing us to synthesize realistic raw sensor measurements from commonly available Internet photos. We additionally model the relevant components of an image processing pipeline when evaluating our loss function, which allows training to be aware of all relevant photometric processing that will occur after denoising. By unprocessing and processing training data and model outputs in this way, we are able to train a simple convolutional neural network that has 14%-38% lower error rates and is 9\u00d7-18\u00d7 faster than the previous state of the art on the Darmstadt Noise Dataset [30], and generalizes to sensors outside of that dataset as well.  Table 1. Performance of our model and its ablations on the Darmstadt Noise Dataset Ablations of \" Our Model ( sRGB ) \" ( 62 . 5% ) ( 28 . 5% ) ( 52 . 5% ) ( 51 . 5% ) ( 0 . 0% ) ( 29 . 7% ) Raw ( 59 . 1% ) ( 67 . 9% ) ( 0 . 3% ) ( 41 . 7% ) ( 39 . 9% ) ( 49 . 0% ) PSNR SSIM ( 49 . 2% ) ( 53 . 2% ) ( 9 . 8% ) ( 4 . 8% ) ( 18 . 6% ) sRGB ( 49 . 6% ) ( 44 . 2% ) ( 38 . 0% ) ( 31 . 2% )",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Color Image Denoising\", \"Dataset\": \"Darmstadt Noise Dataset\", \"Metric\": \"PSNR (Raw)\", \"Score\": \"48.88\"}}, {\"LEADERBOARD\": {\"Task\": \"Color Image Denoising\", \"Dataset\": \"Darmstadt Noise Dataset\", \"Metric\": \"PSNR (sRGB)\", \"Score\": \"40.35\"}}, {\"LEADERBOARD\": {\"Task\": \"Color Image Denoising\", \"Dataset\": \"Darmstadt Noise Dataset\", \"Metric\": \"SSIM (Raw)\", \"Score\": \"0.9821\"}}, {\"LEADERBOARD\": {\"Task\": \"Color Image Denoising\", \"Dataset\": \"Darmstadt Noise Dataset\", \"Metric\": \"SSIM (sRGB)\", \"Score\": \"0.9641\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Denoising\", \"Dataset\": \"SIDD\", \"Metric\": \"PSNR (sRGB)\", \"Score\": \"38.49\"}} ]"
    },
    {
        "Context": "This paper proposes a novel remote user authentication scheme using smart cards which allows both the authentication server (AS) and the user to verify each other's authenticity. Our scheme is efficient enough to resist the known attacks that could be launched against remote user authentication process.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Fast-SCNN: Fast Semantic Segmentation Network The encoder-decoder framework is state-of-the-art for offline semantic image segmentation. Since the rise in autonomous systems, real-time computation is increasingly desirable. In this paper, we introduce fast segmentation convolutional neural network (Fast-SCNN), an above realtime semantic segmentation model on high resolution image data (1024 \u00d7 2048px) suited to efficient computation on embedded devices with low memory. Building on existing two-branch methods for fast segmentation, we introduce our 'learning to downsample' module which computes lowlevel features for multiple resolution branches simultaneously. Our network combines spatial detail at high resolution with deep features extracted at lower resolution, yielding an accuracy of 68.0% mean intersection over union at 123.5 frames per second on Cityscapes. We also show that large scale pre-training is unnecessary. We thoroughly validate our metric in experiments with ImageNet pre-training and the coarse labeled data of Cityscapes. Finally, we show even faster computation with competitive results on subsampled inputs, without any network modifications. We evaluated our proposed fast segmentation convolutional neural network (Fast-SCNN) on the validation set of the Cityscapes dataset, and report its performance on the Cityscapes test set, i.e We evaluate our proposed Fast-SCNN on Cityscapes, the largest publicly available dataset on urban roads This dataset contains a diverse set of high resolution images (1024\u00d72048px) captured from 50 different cities in Europe The label for the training set and validation set are available and test results can be evaluated on the evaluation server Cityscapes provides 30 class labels, while only 19 classes are used for evaluation We evaluate overall performance on the withheld test set of Cityscapes Table 1. Fast-SCNN uses standard convolution (Conv2D), depth- wise separable convolution (DSConv), inverted residual bottle- neck blocks (bottleneck), a pyramid pooling module (PPM) and a feature fusion module (FFM) block. Parameters t, c, n and s rep- resent expansion factor of the bottleneck block, number of output channels, number of times block is repeated and stride parameter which is applied to first sequence of the repeating block. The hori- zontal lines separate the modules: learning to down-sample, global feature extractor, feature fusion and classifier (top to bottom). s t Block - - n Table 4. Class and category mIoU of the proposed Fast-SCNN compared to other state-of-the-art semantic segmentation methods on the Cityscapes test set. Number of parameters is listed in mil- lions. 44 . - lions . Category 1024 \u00d7 2048 Params Class 256 \u00d7 512 - 512 \u00d7 1024 Table 5. Runtime (fps) on Nvidia Titan X (Maxwell, 3,072",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"68%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"mIoU\", \"Score\": \"69.19%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"67.5%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\":\"mIoU\", \"Score\": \"18.0%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\":\"mIoU\", \"Score\": \"74.7%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\":\"mIoU\", \"Score\": \"73.0%\"}} ]"
    },
    {
        "Context": "Published as a conference paper at ICLR 2020 ATOMNAS: FINE-GRAINED END-TO-END NEURAL ARCHITECTURE SEARCH Search space design is very critical to neural architecture search (NAS) algorithms. We propose a fine-grained search space comprised of atomic blocks, a minimal search unit that is much smaller than the ones used in recent NAS algorithms. This search space allows a mix of operations by composing different types of atomic blocks, while the search space in previous methods only allows homogeneous operations. Based on this search space, we propose a resource-aware architecture search framework which automatically assigns the computational resources (e.g., output channel numbers) for each operation by jointly considering the performance and the computational cost. In addition, to accelerate the search process, we propose a dynamic network shrinkage technique which prunes the atomic blocks with negligible influence on outputs on the fly. Instead of a searchand-retrain two-stage paradigm, our method simultaneously searches and trains the target architecture. Our method achieves state-of-the-art performance under several FLOPs configurations on ImageNet with a small searching cost. We open our entire codebase at: https://github.com/meijieru/AtomNAS. * This work was done during the internship program at Bytedance.  Table 1: Comparision with state-of-the-arts on ImageNet under the mobile setting. \u2020 denotes meth- ods using extra network modules such as Swish activation and Squeeze-and-Excitation module. \u2021 de- notes using extra data augmentation such as MixUp and AutoAugment. * denotes models searched and trained simultaneously. - Parameters Top - 1 ( % ) Top - 5 ( % ) Table 2: Influence of awareness of resource metric. The upper block uses equal penalties for all atomic blocks. The lower part uses our resource-aware atomic block selection. Our dynamic network shrinkage algorithm speedups the search and train process significantly . For Top - 1 ( % ) \u03bb COST OF DYNAMIC NETWORK SHRINKAGE Table 3: Influence of BN recalibration. w / o Recalibration w / Recalibration Model Table 4: Comparision with baseline backbones on COCO object detection and instance segmenta- tion. Cls denotes the ImageNet top-1 accuracy; detect-mAP and seg-mAP denotes mean",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Accuracy\", \"Score\": \"77.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"MACs\", \"Score\": \"363M\"}}, {\"LEADERBOARD\": {\"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Params\", \"Score\": \"5.9M\"}}, {\"LEADERBOARD\": {\"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top-1 Error Rate\", \"Score\": \"22.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Accuracy\", \"Score\": \"77.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"MACs\", \"Score\": \"329M\"}}, {\"LEADERBOARD\": {\"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Params\", \"Score\": \"5.5M\"}}, {\"LEADERBOARD\": {\"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top-1 Error Rate\", \"Score\": \"22.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Accuracy\", \"Score\": \"76.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"MACs\", \"Score\": \"260M\"}}, {\"LEADERBOARD\": {\"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Params\", \"Score\": \"4.7M\"}}, {\"LEADERBOARD\": {\"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top-1 Error Rate\", \"Score\": \"23.7\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Accuracy\", \"Score\": \"77.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"MACs\", \"Score\": \"325M\"}}, { \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top-1 Error Rate\", \"Score\": \"22.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Accuracy\", \"Score\": \"76.0\"}}, { \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"MACs\", \"Score\": \"238M\"}}, { \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top-1 Error Rate\", \"Score\": \"24.0\"}}, { \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Accuracy\", \"Score\": \"73.2\"}}, { \"LEADERBOARD\": { \"Task\": "
    },
    {
        "Context": "Domain Adaptation for sEMG-based Gesture Recognition with Recurrent Neural Networks Surface Electromyography (sEMG/EMG) is to record muscles' electrical activity from a restricted area of the skin by using electrodes. The sEMG-based gesture recognition is extremely sensitive of inter-session and inter-subject variances. We propose a model and a deep-learning-based domain adaptation method to approximate the domain shift for recognition accuracy enhancement. Analysis performed on sparse and High-Density (HD) sEMG public datasets validate that our approach outperforms state-of-the-art methods. We have tested the approach on HD sEMG and sparse sEMG datasets: 1) CapgMyo dataset: includes HD-sEMG data for 128 channels acquired from 23 intact subjects 2) NinaPro dataset: a) DB1: The NinaPro sub-database 1 (DB-1) is for the development of hand prostheses, and contains sparse multi-channel sEMG recordings The domain adaptation layer has the M \u2208 Rf \u00d7f where f is 128 in case of the CapgMyo dataset and 10 in case of the NinaPro",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Gesture Recognition\", \"Dataset\": \"CapgMyo DB-b\", \"Metric\": \"Accuracy\", \"Score\": \"97.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Gesture Recognition\", \"Dataset\": \"CapgMyo DB-a\", \"Metric\": \"Accuracy\", \"Score\": \"97.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Gesture Recognition\", \"Dataset\": \"Ninapro DB-1 12 gestures\", \"Metric\": \"Accuracy\", \"Score\": \"84.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Gesture Recognition\", \"Dataset\": \"Ninapro DB-1 8 gestures\", \"Metric\": \"Accuracy\", \"Score\": \"90.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Gesture Recognition\", \"Dataset\": \"CapgMyo DB-c\", \"Metric\": \"Accuracy\", \"Score\": \"96.8\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Hand Gesture Recognition\", \"Dataset\": \"Banglas\", \"Metric\": \"Accuracy\", \"Score\": \"87.75\"}} ]"
    },
    {
        "Context": "New Binomial Bent Function over the Finite Fields of Odd Characteristic * The p-ary function f (x) mapping GF(p 4k ) to GF(p) given by f (x) = Tr 4k`x p 3k +p 2k \u2212p k +1 + x 2\u00b4i s proven to be a weakly regular bent function and the exact values of its Walsh transform coefficients are found. The proof is based on a few new results in the area of exponential sums and polynomials over finite fields that may also be interesting as independent problems.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Large Area Roller Embossing of Multilayered Ceramic Green Composites This paper presents our latest achievements in developing large area patterning of multilayered ceramic green composites using micro roller embossing. The aim of this research is to develop large area pattern technique for ceramic green substrates using a modified roller laminator, which is compatible with screen printing apparatus, for integration of micro embossing and printing in the future stage. A thin film nickel mold was developed via photolithography, nickel electroplating and photoresist strip-off. The mold had an effective panel size of 150 mm\u00d7 \u00d7 \u00d7 \u00d7 150 mm with the height of protrusive micro patterns being about 40 \u00b5m. Formation of micro patterns was successfully demonstrated over the whole panel area using roller embossing on laminated ceramic green tapes (HL2000 from Heraeus). Micro patterns for inductors, capacitors as well as interconnection with 50 \u00b5m line width were embossed on ceramic green substrates. With the optimized process parameters (including feeding speed, roller temperature and applied pressure), we have demonstrated that micro roller embossing is a promising method for large area patterning of ceramic green substrates.I.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Factor Graph Attention Dialog is an effective way to exchange information, but subtle details and nuances are extremely important. While significant progress has paved a path to address visual dialog with algorithms, details and nuances remain a challenge. Attention mechanisms have demonstrated compelling results to extract details in visual question answering and also provide a convincing framework for visual dialog due to their interpretability and effectiveness. However, the many data utilities that accompany visual dialog challenge existing attention techniques. We address this issue and develop a general attention mechanism for visual dialog which operates on any number of data utilities. To this end, we design a factor graph based attention mechanism which combines any number of utility representations. We illustrate the applicability of the proposed approach on the challenging and recently introduced VisDial datasets, outperforming recent state-of-the-art methods by 1.1% for VisDial0.9 and by 2% for VisDial1.0 on MRR. Our ensemble model improved the MRR score on VisDial1.0 by more than 6%. Evaluation metrics: Evaluating dialog systems, or any other generative tasks is challenging We follow and evaluate each individual response at each of the T = 10 rounds in a multiple-choice setup The model is hence evaluated on retrieval metrics: Recall@k is the percentage of questions where the human response was part of the top k predicted answers In contrast, our general attention mechanism allows to attend to the entire set of cues in the dataset, letting the model automatically choose the more relevant cues Their introduced question prediction dataset is based on VisDial v0.9, along with a collected set of 100 question candidates Other Datasets: When we replace the attention unit of other methods with our FGA unit we observe improvements in visual question answering (VQA) and audio-visual scene aware dialog (AVSD) Lastly, in, we evaluate question generation and let the model interact with the answer predictor Table 1: Performance of discriminative models on VisDial v0.9. Higher is better for MRR and recall@k, while lower is better for mean rank. (*) denotes use of external knowledge. Mean R@1 R@5 R@10 MRR Table 2: Performance on the question generation task. Higher is better for MRR and recall@k, while lower is better for mean rank. Model MRR R@1 R@5 R@10 Mean SF-QIH-se-2 [21] 0.4060 26.76 55.17 70.39 9.32 FGA 0.4138 27.42 56.33 71.32 9.1 better for MRR and recall@k , while lower is better for mean rank . Mean R@1 Table 2 : Performance on the question generation task . Higher is R@5 R@10 MRR Table 3: Performance of discriminative models on VisDial v1.0 test- std. Higher is better for MRR and recall@k, while lower is better for mean rank and NDCG. (*) denotes use of external knowledge. Mean R@1 R@5 R@10 MRR NDCG Table 4: Attention-related ablation analysis. Mean",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"MRR\", \"Score\": \"68.92\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"Mean Rank\", \"Score\": \"3.39\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"R@1\", \"Score\": \"55.16\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"R@10\", \"Score\": \"92.95\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"R@5\", \"Score\": \"86.26\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"Visual Dialog v1.0 test-std\", \"Metric\": \"MRR (x 100)\", \"Score\": \"69.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"Visual Dialog v1.0 test-std\", \"Metric\": \"Mean\", \"Score\": \"3.14\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"Visual Dialog v1.0 test-std\", \"Metric\": \"NDCG (x 100)\", \"Score\": \"57.20\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"Visual Dialog v1.0 test-std\", \"Metric\": \"R@1\", \"Score\": \"55.65\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"Visual Dialog v1.0 test-std\", \"Metric\": \"R@10\", \"Score\": \"94.05\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"Visual Dialog v1.0 test-std\", \"Metric\": \"R@5\", \"Score\": \"86.73\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"MRR\", \"Score\": \"63.98\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"Mean Rank\", \"Score\": \"4.47\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"R@1\", \"Score\": \"50.29\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"R@10\", \"Score\": \"88.81\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"R@5\", \"Score\": \"80.71\"}} ]"
    },
    {
        "Context": "Minimum Manhattan network problem in normed planes with polygonal balls: a factor 2.5 approximation algorithm Let B be a centrally symmetric convex polygon of R 2 and ||p \u2212 q|| be the distance between two points p, q \u2208 R 2 in the normed plane whose unit ball is B. For a set T of n points (terminals) in R 2 , a B-Manhattan network on T is a network N (T ) = (V, E) with the property that its edges are parallel to the directions of B and for every pair of terminals ti and t j , the network N (T ) contains a shortest B-path between them, i.e., a path of length ||t i \u2212 t j ||. A minimum B-Manhattan network on T is a B-Manhattan network of minimum possible length. The problem of finding minimum B-Manhattan networks has been introduced by Gudmundsson, Levcopoulos, and Narasimhan (APPROX'99) in the case when the unit ball B is a square (and hence the distance ||p \u2212 q|| is the l 1 or the l \u221e -distance between p and q) and it has been shown recently by Chin, Guo, and Sun [6] to be strongly NP-complete. Several approximation algorithms (with factors 8,4,3, and 2) for the minimum Manhattan problem are known. In this paper, we propose a factor 2.5 approximation algorithm for the minimum B-Manhattan network problem. The algorithm employs a simplified version of the strip-staircase decomposition proposed in our paper [5] and subsequently used in other factor 2 approximation algorithms for the minimum Manhattan problem.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "On the Protocol Composition Logic PCL * Manuscript A recent development informal security protocol analysis is the Protocol Composition Logic (PCL). We identify a number of problems with this logic as well as with extensions of the logic, as defined in [DDMP05,HSD + 05,He05,Dat05,Der06,DDMR07]. The identified problems imply strong restrictions on the scope of PCL, and imply that some currently claimed PCL proofs cannot be proven within the logic, or make use of unsound axioms. Where possible, we propose solutions for these problems. * This work was supported by the Hasler Foundation, ManCom project 2071.In this paper, we identify a number of problems with PCL as defined in [DDMP05, Dat05, Der06, DDMR07, HSD + 05, He05]. They have implications for the scope of PCL, a number of claimed formal proofs, and several extensions to the base model. In particular, we show that in contrast with the claims in e.g. the introduction of [DDMR07], PCL as defined in [DDMP05, Dat05, Der06, DDMR07] cannot be used to prove common authentication properties of protocols that do not include signatures. We show that a number of claimed proofs in PCL cannot be correct because (a) there is noway to establish preceding actions in a thread, and (b) there is noway to express type restrictions in PCL. With respect to existing PCL extensions, we identify two problems: the Diffie-Hellman extension from [DDMP05, Dat05, Der06, DDMR07] does not correctly capture the algebraic behaviour of Diffie-Hellman-like protocols, and the extension for hash functions from [HSD + 05, He05] is not sound. Some of these problems can be resolved by minor modifications to PCL, but other problems require further investigation. Our observations suggest that it is at least required to make changes to existing axioms, to introduce new axioms, and to add a mechanism fora type system.The purpose of this paper is to identify some of the challenges that need to be addressed in order to make a logic like PCL work. We hope it will contribute to the improvement of PCL, and will lead to a better understanding of some of the pitfalls of designing a compact and usable formal logic for security protocols.The scope of this paper. The presentation of this paper is inherently difficult, not least because there area number of different papers on PCL, which vary in notation and technical details. Many ideas were already present in precursors of PCL, e.g. [DMP01, DMP03], but these variants use different concepts than later versions of PCL. These early variants in [DMP01, DMP03] have no notion of thread (a.k.a. process, run, or role instance), and events are bound to agents. More recent versions of PCL bind events to threads of agents, and therefore distinguish between several threads of the same agent. PCL versions of the latter type can be found in [DDMP03b,DDMP03a,DDMP04b,DDMP04a]. Subsequently, [DDMP03b, DDMP03a, DDMP04b, DDMP04a] have been claimed to be either subsumed, or revised and extended, by more recent works [DDMP05, Dat05, Der06, DDMR07]. Hence we choose hereto focus on [DDMP05, Dat05, Der06, DDMR07], which contain similar descriptions of PCL. Throughout this paper we write basic PCL to refer to [DDMP05, Dat05, Der06, DDMR07]. The publications on basic PCL describe the fundamental part of PCL that focusses on authentication. In general, the comments in this paper apply to basic PCL. The comments in Section 4.2 apply only to the extensions found in [HSD + 05, He05]. Our comments here do not cover the recent extensions to basic PCL for the analysis of secrecy, as found in [RDD + 06], nor the computational variants of PCL, as found in e.g. [DDMW06].Syntax and page references. In order to pinpoint our observations to specific formulas, we select a specific version of PCL to refer to. We have chosen the most recent description of PCL from 2007 as found in [DDMR07]. In particular, we will use [DDMR07] as a reference for the syntax of PCL formulas, and to provide specific page references. Hence we use [DDMR07] as the reference paper to present the problems with basic PCL from the papers [DDMP05,Dat05,Der06,DDMR07].For the technical details, in particular the formulas, we assume the reader is at least somewhat familiar with one of the papers from [DDMP05, Dat05, Der06, DDMR07] or [He05, HSD + 05]. However, the main points should be clear to readers familiar with formal security protocol analysis.The remainder of the paper is structured in the following way. We start off by recalling some PCL notation and concepts in Section 2. Then, in Section 3 we discuss problems with the basic definition of PCL. In Section 4 we identify two problems with existing PCL extensions. We conclude in Section 5.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Combinatorial Approximation Algorithms for MaxCut using Random Walks We give the first combinatorial approximation algorithm for MaxCut that beats the trivial 0.5 factor by a constant. The main partitioning procedure is very intuitive, natural, and easily described. It essentially performs a number of random walks and aggregates the information to provide the partition. We can control the running time to get an approximation factor-running time tradeoff. We show that for any constant b > 1.5, there is an O(n b ) algorithm that outputs a (0.5 + \u03b4)-approximation for MaxCut, where \u03b4 = \u03b4(b) is some positive constant.One of the components of our algorithm is a weak local graph partitioning procedure that maybe of independent interest. Given a starting vertex i and a conductance parameter \u03c6, unless a random walk of length = O(log n) starting from i mixes rapidly (in terms of \u03c6 and ), we can find a cut of conductance at most \u03c6 close to the vertex. The work done per vertex found in the cut is sublinear inn. arXiv:1008.3938v1 [cs.DS] 23 Aug 2010 2. Even though the core of our algorithm is completely combinatorial, relying only on simple random walks and integer operations, the analysis of the algorithm is based on spectral methods. We obtain a combinatorial version of Trevisan's algorithm by showing two key facts: (a) the \"flipping signs\" random walks we use corresponds to running the power method on the graph Laplacian, and (b) a random starting vertex yields a good starting vector for the power method with constant probability. These two facts replace numerical matrix computations with the combinatorial problem of estimating certain probabilities, which can be done effectively by sampling and concentration bounds. This also allows improved running times since we can selectively find portions of the graph and classify them.3. A direct application of the partitioning procedure yields an algorithm whose running time is O(n 2+\u00b5 ). To design the sub-quadratic time algorithm, we have to ensure that the random walks in the algorithm mix rapidly. To do this, we design a sort of a local graph partitioning algorithm of independent interest based on simple random walks of logarithmic length. Given a starting vertex i, either it finds a low conductance cut or certifies that the random walk from i has somewhat mixed, in the sense that the ratio of the probability of hitting any vertex j to its probability in the stationary distribution is bounded. The work done per vertex output in the cut is sublinear inn. The precise statement is given in Theorem 4.1. Previous local partitioning algorithms [ST04, ACL06, AL08] are more efficient than our procedure, but can only output a low conductance cut, if the actual conductance of some set containing i is O(1/ log n). In this paper, we need to be able to find low conductance cuts in more general settings, even if there is no cut of conductance of O(1/ log n), and hence the previous algorithms are unsuitable for our purposes.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "graph2vec: Learning Distributed Representations of Graphs Recent works on representation learning for graph structured data predominantly focus on learning distributed representations of graph substructures such as nodes and subgraphs. However, many graph analytics tasks such as graph classification and clustering require representing entire graphs as fixed length feature vectors. While the aforementioned approaches are naturally unequipped to learn such representations, graph kernels remain as the most effective way of obtaining them. However, these graph kernels use handcrafted features (e.g., shortest paths, graphlets, etc.) and hence are hampered by problems such as poor generalization. To address this limitation, in this work, we propose a neural embedding framework named graph2vec to learn data-driven distributed representations of arbitrary sized graphs. graph2vec's embeddings are learnt in an unsupervised manner and are task agnostic. Hence, they could be used for any downstream task such as graph classification, clustering and even seeding supervised representation learning approaches. Our experiments on several benchmark and large real-world datasets show that graph2vec achieves significant improvements in classification and clustering accuracies over substructure representation learning approaches and are competitive with state-of-the-art graph kernels. We evaluate graph2vec's accuracy and efficiency both in graph classification and clustering tasks Besides experimenting with benchmark datasets, we also evaluate our approach on two real-world graph analytics tasks from the field of program analysis, namely, malware detection and malware familial clustering on large malware datasets Specifically, we intend to address the following research questions: (1) How does graph2vec compare to state-of-the-art substructure representation learning approaches and graph kernels for graph classification tasks in terms of accuracy and efficiency on benchmark datasets, (2) How does graph2vec compare to the aforementioned state-of-the-art approaches on a realworld graph classification task, namely, malware detection detection, and (3) How does graph2vec compare to the aforementioned state-of-the-art approaches on a real-world graph clustering task, namely, malware familial clustering Deep WL 4 see for the explanations on obtaining kernel matrix with substructure embedding approaches Datasets Five benchmark graph classification datasets namely MUTAG, PTC, PROTEINS, NCI1 and NCI109 Table 1: Benchmark dataset statistics 3 37 on a server with 36 CPU cores ( Intel E5 - 2699 2 . 30GHz pro - 7 19 # nodes # samples # distinct ( avg . ) node labels Table 1 : Benchmark dataset statistics Thus , the ker - Table 2: Average Accuracy (\u00b1 std dev.) for graph2vec and state-of-the-art graph kernels on benchmark graph classification datasets MUTAG NCI1 NCI109 PROTEINS PTC Table 2 : Average Accuracy ( \u00b1 std dev . ) for graph2vec and state - of - the - art graph kernels on benchmark graph classification datasets Table 3: Large real-world datasets used in graph classifications and clustering tasks 4271 and clustering tasks # nodes ( avg . ) Table 3 : Large real - world datasets used in graph classifications # edges source Table 4: Malware Detection -Results 4271 and clustering tasks # nodes ( avg .",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Malware Detection\", \"Dataset\": \"Android Malware Dataset\", \"Metric\": \"Accuracy\", \"Score\": \"99.03\"}}, {\"LEADERBOARD\": {\"Task\": \"Graph Classification\", \"Dataset\": \"NCI1\", \"Metric\": \"Accuracy\", \"Score\": \"73.22%\"}}, {\"LEADERBOARD\": {\"Task\": \"Graph Classification\", \"Dataset\": \"PTC\", \"Metric\": \"Accuracy\", \"Score\": \"60.17%\"}}, {\"LEADERBOARD\": {\"Task\": \"Graph Classification\", \"Dataset\": \"NCI109\", \"Metric\": \"Accuracy\", \"Score\": \"74.26\"}}, {\"LEADERBOARD\": {\"Task\": \"Graph Classification\", \"Dataset\": \"PROTEINS\", \"Metric\": \"Accuracy\", \"Score\": \"73.3%\"}}, {\"LEADERBOARD\": {\"Task\": \"Graph Classification\", \"Dataset\": \"MUTAG\", \"Metric\": \"Accuracy\", \"Score\": \"83.15%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Movie Classification\", \"Dataset\": \"MovieLens 20M\", \"Metric\": \"Accuracy\", \"Score\": \"95.36%\"}}, { \"LEADERBOARD\": { \"Task\": \"Muade Classification\", \"Dataset\": \"MovieLens 20M\", \"Metric\": \"Accuracy\", \"Score\": \"92.82%\"}}, { \"LEADERBOARD\": { \"Task\": \"Muade Classification\", \"Dataset\": \"MovieLens 20M\", \"Metric\": \"Accuracy\", \"Score\": \"82.82%\"}}, { \"LEADERBOARD\": { \"Task\": \"Muade Classification\", \"Dataset\": \"MovieLens 20M\", \"Metric\": \"Accuracy\", \"Score\": \"80.42%\"}}, { \"LEADERBOARD\": { \"Task\": \"Muade Classification\", \"Dataset\": \"MovieLens 20M\", \"Metric\": \"Accuracy\", \"Score\": \"80.42%\"}}, { \"LEADERBOARD\": { \"Task\": \"Muade Classification\", \"Dataset\": \"MovieLens 20M\", \"Metric\": \"Accuracy\", \"Score\": \"80.42%\"}}, { \"LEADERBOARD\": { \"Task\": \"Muade Classification\", \"Dataset\": \"Text Classification"
    },
    {
        "Context": "Sub-frame Appearance and 6D Pose Estimation of Fast Moving Objects TbD-NC 3D position & angular velocity 6-DOF Trajectory Input Video 2D trajectory Object snapshots shape & appearance Piece-wise Deblatting : Estimation of appearance, shape and 6D pose (3D position and rotation) of fast moving objects. The input video and 2D trajectories estimated by Non-Causal Tracking by Deblatting, , are processed by the proposed piecewise deblatting that generates, with sub-frame temporal resolution, the object appearance and shape (snapshots), from which the complete 6-DOF trajectory is estimated.We propose a novel method that tracks fast moving objects, mainly non-uniform spherical, in full 6 degrees of freedom, estimating simultaneously their 3D motion trajectory, 3D pose and object appearance changes with a time step that is a fraction of the video frame exposure time. The sub-frame object localization and appearance estimation allows realistic temporal super-resolution and precise shape estimation. The method, called TbD-3D (Tracking by Deblatting in 3D) relies on a novel reconstruction algorithm which solves a piece-wise deblurring and matting problem. The 3D rotation is estimated by minimizing the reprojection error. As a second contribution, we present anew challenging dataset with fast moving objects that change their appearance and distance to the camera. High speed camera recordings with zero lag between frame exposures were used to generate videos with different frame rates annotated with ground-truth trajectory and pose. We created anew annotated dataset containing fast moving objects All previous datasets with FMOs, such as FMO dataset and TbD dataset, included only objects moving in a 2D plane parallel to the camera plane and their appearance was close to static The introduced dataset is the first dataset with nonnegligible 3D object motion and with changing appearance of non-uniform fast moving objects The dataset is called TbD-3D and it contains nine sequences with annotated object location, pose, and size from a high-speed camera In contrast to previous datasets, the perceived size of objects in TbD-3D dataset varies throughout the whole sequence due to depth of the scene, as shown in The dataset sequences were generated by averaging 2, 4 and 8 frames, which corresponds to real videos captured at 30, 60, 120 fps, respectively The proposed method is evaluated on the TbD-3D dataset for all three frame-rate settings Table 1: TbD-3D dataset -comparison of TbD \u2022 # TbD - NC Radius Error [ pixels ] Axis Error [ TbD TIoU - 3D TbD - 3D Angle Error [ TbD - 3D - O ]",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Video Super-Resolution\", \"Dataset\": \"TbD-3D\", \"Metric\": \"PSNR\", \"Score\": \"23.13\"}}, {\"LEADERBOARD\": {\"Task\": \"Video Super-Resolution\", \"Dataset\": \"TbD-3D\", \"Metric\": \"SSIM\", \"Score\": \"0.651\"}}, {\"LEADERBOARD\": {\"Task\": \"Video Super-Resolution\", \"Dataset\": \"TbD-3D\", \"Metric\": \"TIoU\", \"Score\": \"0.598\"}}, {\"LEADERBOARD\": {\"Task\": \"Video Super-Resolution\", \"Dataset\": \"TbD\", \"Metric\": \"PSNR\", \"Score\": \"25.21\"}}, {\"LEADERBOARD\": {\"Task\": \"Video Super-Resolution\", \"Dataset\": \"TbD\", \"Metric\": \"SSIM\", \"Score\": \"0.674\"}}, {\"LEADERBOARD\": {\"Task\": \"Video Super-Resolution\", \"Dataset\": \"TbD\", \"Metric\": \"TIoU\", \"Score\": \"0.542\"}}, {\"LEADERBOARD\": {\"Task\": \"Video Super-Resolution\", \"Dataset\": \"Falling Objects\", \"Metric\": \"PSNR\", \"Score\": \"23.42\"}}, {\"LEADERBOARD\": {\"Task\": \"Video Super-Resolution\", \"Dataset\": \"Falling Objects\", \"Metric\": \"SSIM\", \"Score\": \"0.671\"}}, {\"LEADERBOARD\": {\"Task\": \"Video Super-Resolution\", \"Dataset\": \"Falling Objects\", \"Metric\": \"TIoU\", \"Score\": \"0.539\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Object Reconstruction\", \"Dataset\": \"TbD-3D\", \"Metric\": \"Avg F1\", \"Score\": \"78.3\"}} ]"
    },
    {
        "Context": "Named Entity Recognition with Bidirectional LSTM-CNNs Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance.In this paper, we present a novel neural network architecture that automatically detects word-and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information. Evaluation was performed on the well-established CoNLL-2003 NER shared task dataset (Tjong Kim and the much larger but less-studied OntoNotes 5.0 dataset gives an overview of these two different datasets For all datasets, we performed the following preprocessing: \u2022 All digit sequences are replaced by a single \"0\" In addition, for the OntoNotes dataset, in order to handle the Date, Time, Money, Percent, Quantity, Ordinal, and Cardinal named entity tags, we split tokens before and after every digit The CoNLL-2003 dataset (Tjong Kim consists of newswire from the Reuters RCV1 corpus tagged with four types of named entities: location, organization, person, and miscellaneous As the dataset is small compared to OntoNotes, we trained the model on both the training and development sets after performing hyperparameter optimization on the development set compiled a core portion of the OntoNotes 5.0 dataset for the CoNLL-2012 shared task and described a standard train/dev/test split, which Table 1: Number of entries for each category in the SENNA lexicon and our DBpedia lexicon. Table 2: Dataset sizes in number of tokens (entities) Dataset CoNLL - 2003 ( 23 , 499 ) Table 3: Hyper-parameter search space and final values used for all experiments [ 15 , 100 ] [ 100 , 400 ] - Table 3 : Hyper - parameter search space and final values used for all experiments [ 3 , 7 ] Hyper - parameter LSTM layers [ 1 , 4 ] Mini - batch size Range [ 2 , 4 ] 1 2 3 9 Final [ 3 , 9 ] Convolution width Table 5: Results of our models, with various feature sets, compared to other published results. The three sections are, in order, our models, published neural network models, and published non-neural network models. For the features, emb = Collobert word embeddings, caps =",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Named Entity Recognition\", \"Dataset\": \"CoNLL 2003 (English)\", \"Metric\": \"F1\", \"Score\": \"91.62\"}}, {\"LEADERBOARD\": {\"Task\": \"Named Entity Recognition\", \"Dataset\": \"Ontonotes v5 (English)\", \"Metric\": \"F1\", \"Score\": \"86.19\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Named Entity Recognition\", \"Dataset\": \"Ontonotes v5 (English)\", \"Metric\": \"F1\", \"Score\": \"84.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Named Entity Recognition\", \"Dataset\": \"Ontonotes v5 (English)\", \"Metric\": \"F1\", \"Score\": \"84.8\"}} ]"
    },
    {
        "Context": "Model Based Ceramic tile inspection using Discrete Wavelet Transform and Euclidean Distance Visual inspection of industrial products is used to determine the control quality for these products. This paper deals with the problem of visual inspection of ceramic tiles industry using Wavelet Transform. The third level the coefficients of two dimensions Haar Discrete Wavelet Transform (HDWT) is used in this paper to process the images and feature extraction. The proposed algorithm consists of two main phases. The first phase is to compute the wavelet transform for an image free of defects which known as reference image, and the image to be inspected which known as test image. The second phase is used to decide whether the tested image is defected or not using the Euclidean distance similarity measure. The experimentation results of the proposed algorithm give 97% for correct detection of ceramic defects.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Study of Mechanical Response in Embossing of Ceramic Green Substrate by Micro-Indentation Micro-indentation test with a micro flat-end cone indenter was employed to simulate micro embossing process and investigate the thermo-mechanical response of ceramic green substrates. The laminated low temperature co-fired ceramic green tapes were used as the testing material; the correlations of indentation depth versus applied force and applied stress at the temperatures of 25 \u00b0C and 75\u00b0C were studied. The results showed that permanent indentation cavities could be formed at temperatures ranging from 25 \u00b0C to 75 \u00b0C, and the depth of cavities created was applied force, temperature and dwell time dependent. Creep occurred and made a larger contribution to the plastic deformation at elevated temperatures and high peak loads. There was instantaneous recovery during the unloading and retarded recovery in the first day after indentation. There was no significant pile-up due to material flow observed under compression at the temperature up to 75 \u00b0C. The plastic deformation was the main cause for formation of cavity on the ceramic green substrate under compression. The results can be used as a guideline for embossing ceramic green substrates.I.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Disp R-CNN: Stereo 3D Object Detection via Shape Prior Guided Instance Disparity Estimation In this paper, we propose a novel system named Disp R-CNN for 3D object detection from stereo images. Many recent works solve this problem by first recovering a point cloud with disparity estimation and then apply a 3D detector. The disparity map is computed for the entire image, which is costly and fails to leverage category-specific prior. In contrast, we design an instance disparity estimation network (iDispNet) that predicts disparity only for pixels on objects of interest and learns a category-specific shape prior for more accurate disparity estimation. To address the challenge from scarcity of disparity annotation in training, we propose to use a statistical shape model to generate dense disparity pseudo-ground-truth without the need of LiDAR point clouds, which makes our system more widely applicable. Experiments on the KITTI dataset show that, even when LiDAR ground-truth is not available at training time, Disp R-CNN achieves competitive performance and outperforms previous state-of-the-art methods by 20% in terms of average precision. The code will be available at https://github.com/zju3dv/disprcnn. We evaluate the proposed approach on the 3D object detection benchmark of KITTI dataset Table 1. 3D object detection results on the KITTI object validation set. We report average precision of bird's eye view (AP bev ) and 3D boxes (AP 3d ) for the car category. LiDAR supervision indicates if the method uses the sparse LiDAR point cloud as a supervision signal during training. We report the reproduced result for PL (AVOD) since [30] didn't provide full results on experiments without LiDAR supervision. Besides published state-of-the-art methods, we also present the results of concurrent works (grey background) for comparison. Method bev ( IoU=0 . 7 ) Mod . Easy bev ( IoU=0 . 5 ) AP bev ( IoU=0 . 7 ) LiDAR 3D object detection results on the KITTI object validation set . Hard AP 3d ( IoU=0 . 7 ) AP 3d ( IoU=0 . 7 ) Supervision 3d ( IoU=0 . 5 ) Table 2. 3D object detection results on the",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Vehicle Pose Estimation\", \"Dataset\": \"KITTI Cars Hard\", \"Metric\": \"Average Orientation Similarity\", \"Score\": \"67.16\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Moderate val\", \"Metric\": \"AP\", \"Score\": \"75.54%\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Moderate val\", \"Metric\": \"AP\", \"Score\": \"84.83\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Easy val\", \"Metric\": \"AP\", \"Score\": \"92.57\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Easy\", \"Metric\": \"AP\", \"Score\": \"90.14%\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Moderate val\", \"Metric\": \"AP\", \"Score\": \"82.69\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Moderate val\", \"Metric\": \"AP\", \"Score\": \"81.94%\"}} ]"
    },
    {
        "Context": "Integrated Modeling and Verification of Real-Time Systems through Multiple Paradigms Complex systems typically have many different parts and facets, with different characteristics. Ina multi-paradigm approach to modeling, formalisms with different natures are used in combination to describe complementary parts and aspects of the system. This can have a beneficial impact on the modeling activity, as different paradigms can be better suited to describe different aspects of the system. While each paradigm provides a different view on the many facets of the system, it is of paramount importance that a coherent comprehensive model emerges from the combination of the various partial descriptions. In this paper we present a technique to model different aspects of the same system with different formalisms, while keeping the various models tightly integrated with one another. In addition, our approach leverages the flexibility provided by a bounded satisfiability checker to encode the verification problem of the integrated model in the propositional satisfiability (SAT) problem; this allows users to carryout formal verification activities both on the whole model and on parts thereof. The effectiveness of the approach is illustrated through the example of a monitoring system.  Table 2: Checking properties of the data monitoring system. \u2212 + T SAT ( hrs . ) PRE ( min . ) # CL\u00b710 K CNF ( hrs . ) T2 T3",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Joint Parsing and Generation for Abstractive Summarization Sentences produced by abstractive summarization systems can be ungrammatical and fail to preserve the original meanings, despite being locally fluent. In this paper we propose to remedy this problem by jointly generating a sentence and its syntactic dependency parse while performing abstraction. If generating a word can introduce an erroneous relation to the summary, the behavior must be discouraged. The proposed method thus holds promise for producing grammatical sentences and encouraging the summary to stay true-to-original. Our contributions of this work are twofold. First, we present a novel neural architecture for abstractive summarization that combines a sequential decoder with a tree-based decoder in a synchronized manner to generate a summary sentence and its syntactic parse. Secondly, we describe a novel human evaluation protocol to assess if, and to what extent, a summary remains true to its original meanings. We evaluate our method on a number of summarization datasets and demonstrate competitive results against strong baselines. We present our datasets, settings, baselines, qualitative and quantitative evaluation of our proposed method Summarization We present summarization results on all datasets Evaluation is performed using the automatic metric of ROUGE, which measures the n-gram overlap between system and reference summaries, as well as human evaluation of grammaticality and preservation of meanings In we present summarization results on the NEWSROOM, CNN/DM-R, and WEBMERGE datasets Instead, we train the pointer-generator networks with coverage mechanism (PointerGen;, one of the best performed neural abstractive summarizers, on the train split of each dataset, then report results on the test split; we apply a similar process to our GenParse systems We observe that the GenParse-FULL model consistently outperforms strong baselines across all datasets 5 PointerGen: Summarization results on Newsroom, CNN/DM-R, and WebMerge datasets NEWSROOM dataset, respectively Overall, we notice that the GenParse-FULL method performs exceptionally well on retaining relations on the CNN/DM-R dataset We proceed by Table 3: Statistics of our datasets. |y| is number of words. 4 , 020 , 581 199 , 341 472 , 872 |y| Table 4: Summarization results on Gigaword dataset. Our GenParse systems perform on par with or superior to state- of-the-art systems on the standard test set. R - 1 Gigaword Test Set R - 2 R - L Table 5: Summarization results on Newsroom, CNN/DM-R, and WebMerge datasets. Our GenParse-FULL method jointly decodes a summary and its dependency structure using a novel architecture that performs competitively against strong baselines. It outperforms both pointer-generator networks and the ablated model GenParse-BASE without using the tree-decoder. \u2022 55 45 \u2022 35 25 15 Newsroom JtParseSumm \u2212 Base JtParseSumm \u2212 Full PointerGen Source CNN / DM ( % ) Summary 60 \u2022 JtParseSumm \u2212 Base 50 40 30 20 Table 7: Human assessment of grammaticality and seman- tic accuracy of various summaries. Our",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Text Summarization\", \"Dataset\": \"GigaWord\", \"Metric\": \"ROUGE-1\", \"Score\": \"36.61\"}}, {\"LEADERBOARD\": {\"Task\": \"Text Summarization\", \"Dataset\": \"GigaWord\", \"Metric\": \"ROUGE-2\", \"Score\": \"18.85\"}}, {\"LEADERBOARD\": {\"Task\": \"Text Summarization\", \"Dataset\": \"GigaWord\", \"Metric\": \"ROUGE-L\", \"Score\": \"34.33\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Text Summarization\", \"Dataset\": \"WebNLG\", \"Metric\": \"ROUGE-1\", \"Score\": \"28.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Text Summarization\", \"Dataset\": \"NYT\", \"Metric\": \"ROUGE-1\", \"Score\": \"42.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Text Summarization\", \"Dataset\": \"WebNLI\", \"Metric\": \"ROUGE-1\", \"Score\": \"42.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Text Summarization\", \"Dataset\": \"NYT\", \"Metric\": \"ROUGE-2\", \"Score\": \"22.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Text Summarization\", \"Dataset\": \"NYT\", \"Metric\": \"ROUGE-L\", \"Score\": \"41.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Text Summarization\", \"Dataset\": \"arXimetric Thermal Face Dataset\", \"Metric\": \"ROUGE-1\", \"Score\": \"41.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Text Summarization\", \"Dataset\": \"arXimetric Thermal Face Dataset\", \"Metric\": \"ROUGE-2\", \"Score\": \"31.2\"}}]"
    },
    {
        "Context": "WELL-CENTERED TRIANGULATION \u00b6 Meshes composed of well-centered simplices have nice orthogonal dual meshes (the dual Voronoi diagram). This is useful for certain numerical algorithms that prefer such primal-dual mesh pairs. We prove that well-centered meshes also have optimality properties and relationships to Delaunay and minmax angle triangulations. We present an iterative algorithm that seeks to transform a given triangulation in two or three dimensions into a well-centered one by minimizing a cost function and moving the interior vertices while keeping the mesh connectivity and boundary vertices fixed. The cost function is a direct result of anew characterization of well-centeredness in arbitrary dimensions that we present. Ours is the first optimization-based heuristic for wellcenteredness, and the first one that applies in both two and three dimensions. We show the results of applying our algorithm to small and large two-dimensional meshes, some with a complex boundary, and obtain a well-centered tetrahedralization of the cube. We also show numerical evidence that our algorithm preserves gradation and that it improves the maximum and minimum angles of acute triangulations created by the best known previous method.AMS subject classifications. 65N50, 65M50, 65D18, 51M04 \u00b6 Preliminary results for the 2-dimensional problem of well-centered planar triangulations appeared previously in the",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "INFOGRAPH: UNSUPERVISED AND SEMI-SUPERVISED GRAPH-LEVEL REPRESENTATION LEARNING VIA MU- TUAL INFORMATION MAXIMIZATION This paper studies learning the representations of whole graphs in both unsupervised and semi-supervised scenarios. Graph-level representations are critical in a variety of real-world applications such as predicting the properties of molecules and community analysis in social networks. Traditional graph kernel based methods are simple, yet effective for obtaining fixed-length representations for graphs but they suffer from poor generalization due to hand-crafted designs. There are also some recent methods based on language models (e.g. graph2vec) but they tend to only consider certain substructures (e.g. subtrees) as graph representatives. Inspired by recent progress of unsupervised representation learning, in this paper we proposed a novel method called InfoGraph for learning graph-level representations. We maximize the mutual information between the graph-level representation and the representations of substructures of different scales (e.g., nodes, edges, triangles). By doing so, the graph-level representations encode aspects of the data that are shared across different scales of substructures. Furthermore, we further propose InfoGraph*, an extension of InfoGraph for semi-supervised scenarios. InfoGraph* maximizes the mutual information between unsupervised graph representations learned by InfoGraph and the representations learned by existing supervised methods. As a result, the supervised encoder learns from unlabeled data while preserving the latent semantic space favored by the current supervised task. Experimental results on the tasks of graph classification and molecular property prediction show that InfoGraph is superior to state-of-the-art baselines and InfoGraph* can achieve performance competitive with state-of-the-art semi-supervised models. We evaluate the effectiveness of the graph-level representation learned by InfoGraph on downstream graph classification tasks and on semi-supervised molecular property prediction tasks For graph classification, we conduct experiments on 6 well-known benchmark datasets: MUTAG, PTC, REDDIT-BINARY, REDDIT-MULTI-5K, IMDB-BINARY, and IMDB-MULTI) For semi-supervised learning tasks, we use the publicly available QM9 dataset Additional details of the datasets can be found in Appendix B (2017) but we did not run it on all datasets as the implementation details are not clear in the paper The QM9 dataset has 130462 molecules in it We minimize the mean squared error between the model output and the target, although we evaluate mean absolute error C.1 GRAPH CLASSIFICATION DATASETS MUTAG contains 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 different discrete labels PTC is a dataset of 344 different chemical compounds that have been tested for carcinogenicity in male and female rats This dataset has Table 1: Classification accuracy on 6 datasets. The result in bold indicates the best reported classification accuracy. The top half of the table compares results with various graph kernel approaches while bottom half compares results with other state-of-the-art unsupervised graph representation learning methods. '> 1 day' represents that the computation exceeds 24 hours. 'OMR' is out of memory error. Graph Kernels Other Unsupervised Methods IMDB - B RDT - B 1000 2000 IMDB - M - PTC - MR 188 2 344 1500 3 5 MUTAG RDT - M5K 4999 Table 2: Results of semi-supervised experiments on QM9 dataset. The result in bold indicates the best performance. The top half of the table shows the mean absolute error (MAE) of the supervised model. The bottom half shows the error ratio (with respect to supervised result) of the semi-supervised models using the same underlying model. Lower scores are better and values less",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Graph Classification\", \"Dataset\": \"PTC\", \"Metric\": \"Accuracy\", \"Score\": \"61.65\"}}, {\"LEADERBOARD\": {\"Task\": \"Graph Classification\", \"Dataset\": \"IMDb-M\", \"Metric\": \"Accuracy\", \"Score\": \"49.69%\"}}, {\"LEADERBOARD\": {\"Task\": \"Graph Classification\", \"Dataset\": \"MUTAG\", \"Metric\": \"Accuracy\", \"Score\": \"89.01%\"}}, {\"LEADERBOARD\": {\"Task\": \"Graph Classification\", \"Dataset\": \"IMDb-B\", \"Metric\": \"Accuracy\", \"Score\": \"73.03%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Citeseer\", \"Metric\": \"Accuracy\", \"Score\": \"73.50%\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Pubmed\", \"Metric\": \"Accuracy\", \"Score\": \"89.35%\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Cora\", \"Metric\": \"Accuracy\", \"Score\": \"93.14%\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"CiteSeer with Public Split: fixed 20 nodes per class\", \"Metric\": \"Accuracy\", \"Score\": \"72.88%\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"PubMed with Public Split: fixed 20 nodes per class\", \"Metric\": \"Accuracy\", \"Score\": \"80.06%\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"PubMed with Public Split: fixed 20 nodes per class\", \"Metric\": \"Accuracy\", \"Score\": \"82.84%\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Cora with Public Split: fixed 20 nodes per"
    },
    {
        "Context": "Adversarial training for multi-context joint entity and relation extraction Adversarial training (AT) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data. We show how to use AT for the tasks of entity recognition and relation extraction. In particular, we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations, allows improving the state-of-the-art effectiveness on several datasets in different contexts (i.e., news, biomedical, and real estate data) and for different languages (English and Dutch). We evaluate our models on four datasets, using the code as available from our github codebase 1 Specifically, we follow the 5-fold crossvalidation defined by for the ACE04 dataset We also evaluate our models on the NER task similar to in the same dataset using 10-fold cross validation For the Dutch Real Estate Classifieds, DREC (Bekoulis et al., 2017) dataset, we use train-test splits as in We use three types of evaluation, namely: (i) S(trict): we score an entity as correct if both the entity boundaries and the entity type are correct (ACE04, ADE, CoNLL04, DREC), (ii) B(oundaries): we score an entity as correct if only the entity boundaries are correct while the entity type is not taken into account (DREC) and (iii) R(elaxed): a multi-token entity is considered correct if at least one correct type is assigned to the tokens comprising the entity, assuming that the: Comparison of our method Table 1: Comparison of our method with the state- of-the-art in terms of F 1 score. The proposed mod- els are: (i) baseline, (ii) baseline EC (predicts only entity classes) and (iii) baseline (EC) + AT (reg- ularized by AT). The and symbols indicate whether the models rely on external NLP tools. We include different evaluation types (S, R and B). Entity Overall Relation",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Relation Extraction\", \"Dataset\": \"CoNLL04\", \"Metric\": \"NER Macro F1\", \"Score\": \"83.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Relation Extraction\", \"Dataset\": \"CoNLL04\", \"Metric\": \"RE+ Macro F1\", \"Score\": \"61.95\"}}, {\"LEADERBOARD\": {\"Task\": \"Relation Extraction\", \"Dataset\": \"ADE Corpus\", \"Metric\": \"NER Macro F1\", \"Score\": \"86.73\"}}, {\"LEADERBOARD\": {\"Task\": \"Relation Extraction\", \"Dataset\": \"ADE Corpus\", \"Metric\": \"RE+ Macro F1\", \"Score\": \"75.52\"}}, {\"LEADERBOARD\": {\"Task\": \"Relation Extraction\", \"Dataset\": \"ACE 2004\", \"Metric\": \"NER Micro F1\", \"Score\": \"81.64\"}}, {\"LEADERBOARD\": {\"Task\": \"Relation Extraction\", \"Dataset\": \"ACE 2004\", \"Metric\": \"RE+ Micro F1\", \"Score\": \"47.45\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"CoNLL04\", \"Metric\": \"NER Macro F1\", \"Score\": \"83.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"CoNLL04\", \"Metric\": \"RE+ Macro F1\", \"Score\": \"62.04\"}}, { \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"ADE Corpus\", \"Metric\": \"NER Macro F1\", \"Score\": \"86.40\"}}, { \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"ADE Corpus\", \"Metric\": \"RE+ Macro F1\", \"Score\": \"74.58\"}}, { \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"ACE 2004\", \"Metric\": \"NER Micro F1\", \"Score\": \"81.16\"}}, { \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"ACE 2004\", \"Metric\": \"RE+ Micro F1\", \"Score\": \"47.14\"}} ]"
    },
    {
        "Context": "Negative Augmentation with Language Model for Reading Comprehension of Abstract Meaning This paper presents our systems for the three Subtasks of SemEval Task4: Reading Comprehension of Abstract Meaning (ReCAM). We explain the algorithms used to learn our models and the process of tuning the algorithms and selecting the best model. Inspired by the similarity of the ReCAM task and the language pre-training, we propose a simple yet effective technology, namely, negative augmentation with language model. Evaluation results demonstrate the effectiveness of our proposed approach. Our models achieve the 4th rank on both official test sets of Subtask 1 and Subtask 2 with an accuracy of 87.9% and an accuracy of 92.8%, respectively 1 . We further conduct comprehensive model analysis and observe interesting error cases, which may promote future researches.  Table 2: Statistics of the SemEval 2021 Task 4 dataset. 3 , 227 2 , 017 1 , 000 Subtask 2 2 , 025 Subtask 1 3 , 318 Table 3: Results (Accuracy) on Subtask 1. Ours Baseline - Dev Test Table 4: Results (Accuracy) on Subtask 2. Ours Baseline - Dev Test",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Reading Comprehension\", \"Dataset\": \"ReCAM\", \"Metric\": \"Accuracy\", \"Score\": \"87.9/92.8\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"SemEval-Based Task 4 Sub Task 2\", \"Dataset\": \"SemEval-R Task 20\", \"Metric\": \"Accuracy\", \"Score\": \"87.9%\"}} ]"
    },
    {
        "Context": "Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade We propose a novel deep layer cascade (LC) method to improve the accuracy and speed of semantic segmentation. Unlike the conventional model cascade (MC) that is composed of multiple independent models, LC treats a single deep model as a cascade of several sub-models. Earlier sub-models are trained to handle easy and confident regions, and they progressively feed-forward harder regions to the next sub-model for processing. Convolutions are only calculated on these regions to reduce computations. The proposed method possesses several advantages. First, LC classifies most of the easy regions in the shallow stage and makes deeper stage focuses on a few hard regions. Such an adaptive and 'difficulty-aware' learning improves segmentation performance. Second, LC accelerates both training and testing of deep network thanks to early decisions in the shallow stage. Third, in comparison to MC, LC is an endto-end trainable framework, allowing joint learning of all sub-models. We evaluate our method on PASCAL VOC and Cityscapes datasets, achieving state-of-the-art performance and fast speed. We evaluate our method on the PASCAL VOC 2012 (VOC12) and Cityscapes datasets VOC12 dataset is a generic object segmentation benchmark with 21 classes Cityscapes dataset, on the other hand, focuses on street scenes segmentation and contains 19 categories We adopt mean intersection over union (mIoU) to evaluate the performance of different methods Table 1: Ablation study on probability thresholds \u03c1. Table 1 : Ablation study on probability thresholds Table 2: Comparisons with related methods. mIoU ( % ) Table 3: A comparison of performance and speed of Layer Cascade (LC) against existing methods. mIoU ms FPS Table 4: Per-class results on VOC12 test set. Approaches pre-trained on COCO [20] are marked with \u2020 . bus sofa tv chair mbike person plant cow bottle boat bike horse mIoU car areo bird cat dog table sheep train Table 5: Per-class results on Cityscapes test set. \"sub\" denotes whether the method used subsampling images for training. bus truck sign veg . person rider mbike bike mIoU road car swalk build . wall tlight terrain sky fence pole train Table 6: Comparisons with state-of-the-art methods on VOC12 test set. '-' indicates the corresponding information was not disclosed in the previous papers. IRNet - LC improves FPS of",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL VOC 2012 test\", \"Metric\": \"Mean IoU\", \"Score\": \"82.7%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"81.4%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL VOC 2012 test\", \"Metric\": \"Mean IoU\", \"Score\": \"86.9%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\":\"mIoU\", \"Score\": \"78.5%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL VOC 2012 val\", \"Metric\":\"mIoU\", \"Score\": \"82.70%\"}} ]"
    },
    {
        "Context": "Unraveling BitTorrent's File Unavailability: Measurements, Analysis and Solution Exploration BitTorrent suffers from one fundamental problem: the long-term availability of content. This occurs on a massivescale with 38% of torrents becoming unavailable within the first month. In this paper we explore this problem by performing two large-scale measurement studies including 46K torrents and 29M users. The studies go significantly beyond any previous work by combining per-node, per-torrent and system-wide observations to ascertain the causes, characteristics and repercussions of file unavailability. The study confirms the conclusion from previous works that seeders have a significant impact on both performance and availability. However, we also present some crucial new findings: (i) the presence of seeders is not the sole factor involved in file availability, (ii) 23.5% of nodes that operate in seedless torrents can finish their downloads, and (iii) BitTorrent availability is discontinuous, operating in cycles of temporary unavailability. Due to our new findings, we consider it is important to revisit the solution space; to this end, we perform large-scale trace-based simulations to explore the potential of two abstract approaches. To evaluate the two possible solutions approaches, the BitTorrent simulator of Bharambe et al Table 1. Characteristics of resilient torrents (those that maintain availability in seedless state) and susceptible torrents (those that cannot reconstruct the file). 1 affected ( micros - 2 ) 1 hour 24 hours Time before seedless state Susceptible Resilient 6 hours Time after torrent ' s birth Table 2. Overview about system-level results. ( in KBps ) S D S F Metric ( in % ) Avg . seeding time ( in hours )",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Refinement and Verification of Real-Time Systems \u2020 This paper discusses highly general mechanisms for specifying the refinement of a real-time system as a collection of lower level parallel components that preserve the timing and functional requirements of the upper level specification. These mechanisms are discussed in the context of ASTRAL, which is a formal specification language for real-time systems. Refinement is accomplished by mapping all of the elements of an upper level specification into lower level elements that maybe split among several parallel components. In addition, actions that can occur in the upper level are mapped to actions of components operating at the lower level. This allows several types of implementation strategies to be specified in a natural way, while the price for generality (in terms of complexity) is paid only when necessary. The refinement mechanisms are first illustrated using a simple digital circuit; then, through a highly complex phone system; finally, design guidelines gleaned from these specifications are presented.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances Emotion is intrinsic to humans and consequently emotion understanding is a key part of human-like artificial intelligence (AI). Emotion recognition in conversation (ERC) is becoming increasingly popular as anew research frontier in natural language processing (NLP) due to its ability to mine opinions from the plethora of publicly available conversational data in platforms such as Facebook, Youtube, Reddit, Twitter, and others. Moreover, it has potential applications in health-care systems (as a tool for psychological analysis), education (understanding student frustration) and more. Additionally, ERC is also extremely important for generating emotion-aware dialogues that require an understanding of the user's emotions. Catering to these needs calls for effective and scalable conversational emotion-recognition algorithms. However, it is a strenuous problem to solve because of several research challenges. In this paper, we discuss these challenges and shed light on the recent research in this field. We also describe the drawbacks of these approaches and discuss the reasons why they fail to successfully overcome the research challenges in ERC. In this section, we discuss the publicly available ERC datasets as well as the shortcomings of these datasets There area few publicly available datasets for ERC -IEMO-CAP, SEMAINE, Emotionlines, MELD, DailyDialog and EmoContext A detailed comparison of these datasets is drawn in Out of these five datasets, IEMOCAP, SEMAINE and MELD are multimodal (containing acoustic, visual and textual information) and the remaining two are textual Apart from SEMAINE dataset, rest of the 255) and power ([0, \u221e)) We also show the emotion label distribution of these datasets in In EmoContext dataset, an emotion label is assigned to only the last utterance of each dialogue None of these datasets can be used for emotion reasoning as they lack necessary annotation details required for the reasoning task Readers should also note that, all these datasets do not contain fine-grained and topic level emotion annotation",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Emotion Recognition in Conversation\", \"Dataset\": \"EC\", \"Metric\": \"Micro-F1\", \"Score\": \"0.758\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Emotion Recognition in Conversation\", \"Dataset\": \"MELD\", \"Metric\": \"Weighted Macro-F1\", \"Score\": \"65.21\"}}, { \"LEADERBOARD\": { \"Task\": \"Emotion Recognition in Conversation\", \"Dataset\": \"IEMOCAP\", \"Metric\": \"Accuracy\", \"Score\": \"65.28\"}}, { \"LEADERBOARD\": { \"Task\": \"Emotion Recognition in Conversation\", \"Dataset\": \"EmoryNLP\", \"Metric\": \"Weighted Macro-F1\", \"Score\": \"34.73\"}}, { \"LEADERBOARD\": { \"Task\": \"Emotion Recognition in Conversation\", \"Dataset\": \"DailyDialog\", \"Metric\": \"Weighted Macro-F1\", \"Score\": \"54.93\"}}, { \"LEADERBOARD\": { \"Task\": \"Emotion Recognition in Conversation\", \"Dataset\": \"DailyDialog\", \"Metric\": \"Weighted Macro-F1\", \"Score\": \"58.84\"}}, { \"LEADERBOARD\": { \"Task\": \"Emotion Recognition in Conversation\", \"Dataset\": \"DailyDialog\", \"Metric\": \"Weakly Supervised Macro-F1\", \"Score\": \"85.64\"}}, { \"LEADERBOARD\": { \"Task\": \"Emotion Recognition in Conversation\", \"Dataset\": \"Da"
    },
    {
        "Context": "MultiFiT: Efficient Multi-lingual Language Model Fine-tuning Pretrained language models are promising particularly for low-resource languages as they only require unlabelled data. However, training existing models requires huge amounts of compute, while pretrained cross-lingual models often underperform on low-resource languages. We propose Multi-lingual language model Fine-Tuning (MultiFiT) to enable practitioners to train and fine-tune language models efficiently in their own language. In addition, we propose a zero-shot method using an existing pretrained cross-lingual model. We evaluate our methods on two widely used cross-lingual classification datasets where they outperform models pretrained on orders of magnitude more data and compute. We release all models and code 1 . Data We evaluate our models on the Multilingual Document Classification Corpus (MLDoc; product reviews in four languages We provide an overview of the datasets in In the supervised setting, every model is fine-tuned and evaluated on examples from the target language In the zero-shot setting, every model is fine-tuned on source language examples and evaluated on target language examples Baselines We compare against the state-ofthe-art cross-lingual embedding models LASER, which uses a large parallel corpus, multilingual BERT (MultiB- ERT) 10 , and monolingual BERT 11 We also compare against the best models on each dataset, Mul-tiCCA, a cross-lingual word embedding model, and BiDRL, which translates source and target data Our methods We evaluate our monolingual LMs in the supervised setting (MultiFit) and our LMs fine-tuned with pseudo labels from LASER in the zero-shot setting (pseudo forms the comparison methods as the shared embedding space between many languages is overly restrictive For Table 1: The domain, languages, and number of train- ing, development, and test examples in each dataset. Supervised ( 100 target language examples ) Supervised ( 1 , 000 target language examples ) Zero - shot ( 1 , 000 source language examples ) DE RU JA IT FR ES ZH Table 2: Comparison of zero-shot and supervised meth- ods on MLDoc. Supervised ( 100 target language examples ) Supervised ( 1 , 000 target language examples ) Zero - shot ( 1 , 000 source language examples ) DE RU JA IT FR ES ZH Table 3: Comparison of zero-shot, translation-based and supervised methods (with 2k training examples) on all domains of CLS. MT-BOW and CL-SCL results are from (Zhou et al., 2016). DE DVD Music JA Books FR Table 4: Comparison of LSTM and QRNN per-batch training speed on a Tesla V100 (in ms) in MultiFiT. 71 LSTM QRNN",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Cross-Lingual Document Classification\", \"Dataset\": \"MLDoc Zero-Shot English-to-Japanese\", \"Metric\": \"Accuracy\", \"Score\": \"69.57\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-Lingual Document Classification\", \"Dataset\": \"MLDoc Zero-Shot English-to-French\", \"Metric\": \"Accuracy\", \"Score\": \"89.42\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-Lingual Document Classification\", \"Dataset\": \"MLDoc Zero-Shot English-to-Russian\", \"Metric\": \"Accuracy\", \"Score\": \"67.83\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-Lingual Document Classification\", \"Dataset\": \"MLDoc Zero-Shot English-to-Chinese\", \"Metric\": \"Accuracy\", \"Score\": \"82.48\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-Lingual Document Classification\", \"Dataset\": \"MLDoc Zero-Shot English-to-Spanish\", \"Metric\": \"Accuracy\", \"Score\": \"79.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-Lingual Document Classification\", \"Dataset\": \"MLDoc Zero-Shot English-to-Italian\", \"Metric\": \"Accuracy\", \"Score\": \"76.02\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-Lingual Document Classification\", \"Dataset\": \"MLDoc Zero-Shot English-to-German\", \"Metric\": \"Accuracy\", \"Score\": \"91.62%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Multi-lingualDocument Classification\", \"Dataset\": \"Multi-lingualDomain\", \"Metric\": \"Accuracy\", \"Score\": \"74.0\"}}, { \"LEADERBOARD\": { \"Task\": \"Multi-lingualDocument Classification\", \"Dataset\": \"Multi-lingualDomain\", \"Metric\": \"Accuracy\", \"Score\": \"74.0\"}}, { \"LEADERBOARD\": { \"Task\": \"Multi-lingualDocument Classification\", \"Dataset\": \"Multi-lingualDomain\", \"Metric\": \"Accuracy\", \"Score\": \"77.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Multi-lingualDocument Classification\", \"Dataset\": \"Multi-lingualDomain\", \"Metric\": \"Accuracy\", \"Score\": \"76.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Multi-lingualDocument Classification\", \"Dataset\": \"Multi-lingualDomain\", \"Metric\": \"Accuracy\", \"Score\": \"95.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Multi-lingualDocument Classification\", \"Dataset\": \"Multi-lingualDomain\", \"Metric\": \"Accuracy\", \"Score\": \"96.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Multi-lingualDocument Classification\", \"Dataset\": \"Multi-lingualDomain\", 'Metric'"
    },
    {
        "Context": "Monotony in Service Orchestrations Centre de recherche INRIA Rennes -Bretagne Atlantique IRISA, Campus universitaire de Beaulieu, 35042 Rennes Cedex Monotony in Service Orchestrations * Th\u00e8me COM -Syst\u00e8mes communicant\u015b Equipes-Projets Distribcom Rapport de recherche n 6528 - Monotonie dans les orchestrations de web services Web Service orchestrations are compositions of different Web Services to form anew service. The services called during the orchestration guarantee a given performance to the orchestrater, usually in the form of contracts. These contracts can be used by the orchestrater to deduce the contract it can offer to its own clients, by performing contract composition. An implicit assumption in contract based QoS management is: \"the better the component services perform, the better the orchestration's performance will be\". Thus, contract based QoS management for Web services orchestrations implicitly assumes monotony.In some orchestrations, however, monotony can be violated, i.e., the performance of the orchestration improves when the performance of a component service degrades. This is highly undesirable since it can render the process of contract composition inconsistent.In this paper we define monotony for orchestrations modelled by Colored Occurrence Nets (CO-nets) and we characterize the classes of monotonic orchestrations. We show that few orchestrations are indeed monotonic, mostly since latency can be traded for quality of data. We also propose a sound refinement of monotony, called conditional monotony, which forbids this kind of cheating and show that conditional monotony is widely satisfied by orchestrations. This finding leads to reconsidering the way SLAs should be formulated.R\u00e9sum\u00e9 : Les orchestrations de services web sont des compositions de services\u00e9l\u00e9mentaires. Ces services, fournissent un 'contrat'\u00e0 l'orchestrateur, ce qui garantit une certaine performance de leur service. Ces contrats sont utilis\u00e9s par l'orchestrateur pour proposer un contrat\u00e0 un client pour son propre service. Cela se fait par la 'compostion de contrats'. Du point vue de la performance, la composition de contrats suppose implicitement que \"L'am\u00e9lioration de la performance d'un service va rendre l'orchestration plus performante\". La composition de contrats suppose ainsi que les orchestrations sont \"monotones\".Dans quelques orchestrations, cependant, la monotonie peut ne pas\u00eatre respect\u00e9e. Lorsque la performance d'un service s'am\u00e9liore, la performance de l'orchestration se d\u00e9grade. Ceci est tr\u00e8s g\u00eanant car cela rend le processus de composition de contrats invalide. Dans ce rapport, nous d\u00e9finissons la monotonie pour les orchestrations mod\u00e9lis\u00e9es par des r\u00e9seaux d'occurrence color\u00e9s (CO-nets) et nous caract\u00e9risons la classe des orchestrations monotones. Nous d\u00e9montrons que tr\u00e8s peu d'orchestrations sont monotone en pratique, ce qui est largement d\u00fb\u00e0 la possibilit\u00e9 d'am\u00e9liorer la latence en d\u00e9gradant la qualit\u00e9 de la r\u00e9ponse donn\u00e9. Nous proposons ensuite un raffinement de la monotonie, la \"monotonie conditionnelle\", qui interdit ce type de 'triche'. Nous montrons que la monotonie conditionnelle est tr\u00e8s g\u00e9n\u00e9ralement satisfaite par les orchestrations. Cette\u00e9tude nous m\u00e8ne\u00e0 reconsid\u00e9rer la formulation des contrats dans le cadre des orchestrations de services web.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Stochastic Power Grid Analysis Considering Process Variations In this paper, we investigate the impact of interconnect and device process variations on voltage fluctuations in power grids. We consider random variations in the power grid's electrical parameters as spatial stochastic processes and propose anew and efficient method to compute the stochastic voltage response of the power grid. Our approach provides an explicit analytical representation of the stochastic voltage response using orthogonal polynomials in a Hilbert space. The approach has been implemented in a prototype software called OPERA (Orthogonal Polynomial Expansions for Response Analysis). Use of OPERA on industrial power grids demonstrated speed-ups of up to two orders of magnitude. The results also show a significant variation of about \u00b1 35% in the nominal voltage drops at various nodes of the power grids and demonstrate the need for variation-aware power grid analysis. If there are n random variables, the response expansion obtained by limiting the order top would be The error due to truncation is given by Once we have the truncated expansion from Equation, we need to evaluate the best deterministic coefficients {a i } that result in the best minimization of the truncation error Table 1. Results for grids from OPERA and Monte Carlo simulations for order 2 expansion 101 124 % MC 0 ) OPERA Max . % Error ( % of nominal Speedup CPU time Monte ( sec ) ( # nodes ) \u00b1 3\u03c3 variation in \u03c3 in \u00b5 OPERA ( sec ) Ave . % Error",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "A DYNAMIC REDUCTION NETWORK FOR POINT CLOUDS A PREPRINT Classifying whole images is a classic problem in machine learning, and graph neural networks area powerful methodology to learn highly irregular geometries. It is often the case that certain parts of a point cloud are more important than others when determining overall classification. On graph structures this started by pooling information at the end of convolutional filters, and has evolved to a variety of staged pooling techniques on static graphs. In this paper, a dynamic graph formulation of pooling is introduced that removes the need for predetermined graph structure. It achieves this by dynamically learning the most important relationships between data via an intermediate clustering.The network architecture yields interesting results considering representation size and efficiency. It also adapts easily to a large number of tasks from image classification to energy regression in high energy particle physics. 12",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Superpixel Image Classification\", \"Dataset\": \"75 Superpixel MNIST\", \"Metric\": \"Classification Error\", \"Score\": \"0.95\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-100\", \"Metric\": \"Percentage correct\", \"Score\": \"82.95\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Percentage correct\", \"Score\": \"96.71\"}} ]"
    },
    {
        "Context": "Voxel R-CNN: Towards High Performance Voxel-based 3D Object Detection Recent advances on 3D object detection heavily rely on how the 3D data are represented, i.e., voxel-based or point-based representation. Many existing high performance 3D detectors are point-based because this structure can better retain precise point positions. Nevertheless, point-level features lead to high computation overheads due to unordered storage. In contrast, the voxel-based structure is better suited for feature extraction but often yields lower accuracy because the input data are divided into grids. In this paper, we take a slightly different viewpoint -we find that precise positioning of raw points is not essential for high performance 3D object detection and that the coarse voxel granularity can also offer sufficient detection accuracy. Bearing this view in mind, we devise a simple but effective voxel-based framework, named Voxel R-CNN. By taking full advantage of voxel features in a two stage approach, our method achieves comparable detection accuracy with state-of-the-art point-based models, but at a fraction of the computation cost. Voxel R-CNN consists of a 3D backbone network, a 2D bird-eye-view (BEV) Region Proposal Network and a detect head. A voxel RoI pooling is devised to extract RoI features directly from voxel features for further refinement. Extensive experiments are conducted on the widely used KITTI Dataset and the more recent Waymo Open Dataset. Our results show that compared to existing voxel-based methods, Voxel R-CNN delivers a higher detection accuracy while maintaining a realtime frame processing rate, i.e., at a speed of 25 FPS on an NVIDIA RTX 2080 Ti GPU. The code is available at https://github.com/djiajunustc/Voxel-R-CNN. We evaluate our Voxel R-CNN on KITTI Dataset following the common protocol to report the average precision (AP) of class Car with the 0.7 (IoU) threshold And the results evaluated by the test server utilize AP setting of recall 40 positions 1 The AP for 3D object detection and BEV object detection of our Voxel R-CNN on KITTI Dataset is presented in We also conduct experiments on the larger Waymo Open Dataset to further validate the effectiveness of our proposed Voxel R-CNN The objects on the Waymo Open Dataset are split into two levels based on the number of points of a single object, where the LEVEL 1 objects have more than 5 points while the LEVEL 2 objects have 1\u223c5 points We evaluate our Voxel R-CNN on both LEVEL 1 and LEVEL 2 objects and compare with several top-performing methods on the Waymo Open Dataset Specifically, with the commonly used Table 1: Performance comparison for adding BEV detect head on the top of SECOND. These results are evaluated on the KITTI val set with average precision calculated by 11 recall positions for car class. recall positions for car class . Methods Mod . Easy 2D backbone VSA Hard 3D backbone Detect head AP3D ( % ) Table 2: Running time comparison for each component in PV-RCNN. This result is calculated by the average over the 3,769 samples in the KITTI val set. recall positions for car class . Methods Mod . Easy 2D backbone VSA Hard 3D backbone Detect head AP3D ( % ) Table 4: Performance of Voxel R-CNN on the KITTI val set with AP calculated by 40 recall positions for car class LEVEL 2 BEV mAP ( IoU=0 . 7 ) : 2 the methods . By taking full advantage of voxel - based rep - LEVEL 2",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Hard\", \"Metric\": \"AP\", \"Score\": \"77.06%\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Moderate val\", \"Metric\": \"AP\", \"Score\": \"84.52\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Easy val\", \"Metric\": \"AP\", \"Score\": \"89.41\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Easy\", \"Metric\": \"AP\", \"Score\": \"90.9%\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Hard val\", \"Metric\": \"AP\", \"Score\": \"78.93\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Moderate\", \"Metric\": \"AP\", \"Score\": \"81.62%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Moderate val\", \"Metric\": \"AP\", \"Score\": \"76.82%\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Easy val\", \"Metric\": \"AP\", \"Score\": \"90.25%\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cyclists Easy val\", \"Metric\": \"AP\", \"Score\": \"57.44%\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Pedestrians Moderate val\", \"Metric\": \"AP\", \"Score\": \"44.24%\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cars Moderate val\", \"Metric\": \"AP\", \"Score\": \"82.69\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"KITTI Cyclists Easy val\", \"Metric\": \"AP\", \"Score\": \"84.32%\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Object Detection"
    },
    {
        "Context": "Th \" emes COM et COG et SYM et NUM et BIO Computing a Finite Size Representation of the Set of Approximate Solutions of an MOP Computing a Finite Size Representation of the Set of Approximate Solutions of an MOP * Th\u00e8mes COM et COG et SYM et NUM et BIO -Syst\u00e8mes communicants et Syst\u00e8mes cognitifs et Syst\u00e8mes symboliques et Syst\u00e8mes num\u00e9riques et Syst\u00e8mes biologiques Projets Apics et Op\u00e9ra Computing a Finite Size Representation of the Set of Approximate Solutions of an MOP Recently, a framework for the approximation of the entire set of \u01eb-efficient solutions (denote by E \u01eb ) of a multi-objective optimization problem with stochastic search algorithms has been proposed. It was proven that such an algorithm produces -under mild assumptions on the process to generate new candidate solutions -a sequence of archives which converges to E \u01eb in the limit and in the probabilistic sense. The result, though satisfactory for most discrete MOPs, is at least from the practical viewpoint not sufficient for continuous models: in this case, the set of approximate solutions typically forms an n-dimensional object, where n denotes the dimension of the parameter space, and thus, it may come to perfomance problems since in practise one has to cope with a finite archive. Here we focus on obtaining finite and tight approximations of E \u01eb , the latter measured by the Hausdorff distance. We propose and investigate a novel archiving strategy theoretically and empirically. For this, we analyze the convergence behavior of the algorithm, yielding bounds on the obtained approximation quality as well as on the cardinality of the resulting approximation, and present some numerical results.R\u00e9sum\u00e9 : Dans des travaux pr\u00e9c\u00e9dent, nous avons propos\u00e9 un environnement (\"framework\") pour l\"approximation de l'int\u00e9gralit\u00e9 de l'ensemble des solutions \u01eb-efficaces (not\u00e9 E \u01eb ) d'un probl\u00e8me d'optimisation multi-objectifs\u00e0 l'aide d'une recherche stochastique. Il a\u00e9t\u00e9 prouv\u00e9 que suivant certaines hypoth\u00e8ses relatives au processus de g\u00e9n\u00e9ration de nouvelles solutions candidates, un tel algorithme produit une s\u00e9quence d'archives qui converge asymptotiquement vers E \u01eb , au sens probabiliste du terme. Le r\u00e9sultat, s'il est satisfaisant pour la plupart des MOP discrets, ne l'est pas d'un point de vue pratique pour les probl\u00e8mes continus. Dans ce dernier cas, l'ensemble des solutions approxim\u00e9es forme un objet\u00e0 n dimentions, o\u00f9 nest la dimension de l'espace des param\u00e8tres. Ceci peut amener\u00e0 des probl\u00e8mes de performances puisqu'en pratique la taille de l'archive est finie.Dans le travail pr\u00e9sent\u00e9, nous nous concentrons sur l'obtention d'approximations finies et pr\u00e9cises de E \u01eb qui est mesur\u00e9 par la distance de Hausdorff. Nous proposons et nous\u00e9tudions une nouvelle strat\u00e9gie d'archivage des points de vue th\u00e9orique et pratique. Pour ce faire, nous analysons le comportement asymptotique de l'algorithme, en fournissant les limites de qualit\u00e9 de l'approximation obtenue, aussi bien que la cardinalit\u00e9 de l'approximation et nous pr\u00e9sentons\u00e9galement quelques r\u00e9sultats num\u00e9riques.optimisation multi-objectif, convergence, solutions \u01eb-efficaces, solutions approwim\u00e9es, algorithmes de recherche stochastique.Representing the \u01eb-Efficient Set of an MOP  Table 1: Comparison of the magnitudes of the final archive (|A f inal |, rounded) and the corresponding update times (T , in seconds) for MOP (38) and for different values of \u2206. We have taken the average result of 100 test runs. \u2206 |A f inal | T 0 3836 32.98 0.01 827 6.22 0.05 68 1.80 have taken the average result of 100 test runs . T \u2206 Comparison of the magnitudes of the final archive ( |A Table 2: .. x",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Bringing Alive Blurred Moments We present a solution for the goal of extracting a video from a single motion blurred image to sequentially reconstruct the clear views of a scene as beheld by the camera during the time of exposure. We first learn motion representation from sharp videos in an unsupervised manner through training of a convolutional recurrent video autoencoder network that performs a surrogate task of video reconstruction. Once trained, it is employed for guided training of a motion encoder for blurred images. This network extracts embedded motion information from the blurred image to generate a sharp video in conjunction with the trained recurrent video decoder. As an intermediate step, we also design an efficient architecture that enables real-time single image deblurring and outperforms competing methods across all factors: accuracy, speed, and compactness. Experiments on real scenes and standard datasets demonstrate the superiority of our framework over the state-of-the-art and its ability to generate a plausible sequence of temporally consistent sharp frames.  Table 1. Performance comparison of our deblurring network with existing methods on the benchmark dataset [ 23 ] [ 34 ] [ 33 ] Ours [ 42 ] [ 18 ] [ 39 ] [ 7 ]",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Deblurring\", \"Dataset\": \"GoPro\", \"Metric\": \"PSNR\", \"Score\": \"30.58\"}}, {\"LEADERBOARD\": {\"Task\": \"Deblurring\", \"Dataset\": \"GoPro\", \"Metric\": \"SSIM\", \"Score\": \"0.941\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Deblurring\", \"Dataset\": \"GoPro\", \"Metric\": \"PSNR\", \"Score\": \"31.79\"}}, { \"LEADERBOARD\": { \"Task\": \"Deblurring\", \"Dataset\": \"GoPro\", \"Metric\": \"SSIM\", \"Score\": \"0.949\"}} ]"
    },
    {
        "Context": "Fast Neural Architecture Search of Compact Semantic Segmentation Models via Auxiliary Cells Automated design of neural network architectures tailored fora specific task is an extremely promising, albeit inherently difficult, avenue to explore. While most results in this domain have been achieved on image classification and language modelling problems, here we concentrate on dense per-pixel tasks, in particular, semantic image segmentation using fully convolutional networks. In contrast to the aforementioned areas, the design choices of a fully convolutional network require several changes, ranging from the sort of operations that need to be used-e.g., dilated convolutions-to a solving of a more difficult optimisation problem. In this work, we are particularly interested in searching for high-performance compact segmentation architectures, able to run in real-time using limited resources. To achieve that, we intentionally over-parameterise the architecture during the training time via a set of auxiliary cells that provide an intermediate supervisory signal and can be omitted during the evaluation phase. The design of the auxiliary cell is emitted by a controller, a neural network with the fixed structure trained using reinforcement learning. More crucially, we demonstrate how to efficiently search for these architectures within limited time and computational budgets. In particular, we rely on a progressive strategy that terminates non-promising architectures from being further trained, and on Polyak averaging coupled with knowledge distillation to speed-up the convergence. Quantitatively, in 8 GPU-days our approach discovers a set of architectures performing on-par with stateof-the-art among compact models on the semantic segmentation, pose estimation and depth prediction tasks. Code will be made available here: https://github.com/ drsleep/nas-segm-pytorch  Table 3. Among other compact real-time networks, we achieve significantly better results across all the metrics without any additional tricks. Note also that the work in \u2212 arch2 CReaM [ 40 ] Ours arch0 arch1 RF - LW [ 27 ]",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Monocular Depth Estimation\", \"Dataset\": \"NYU-Depth V2\", \"Metric\": \"RMSE\", \"Score\": \"0.523\"}}, {\"LEADERBOARD\": {\"Task\": \"Monocular Depth Estimation\", \"Dataset\": \"NYU-Depth V2\", \"Metric\": \"RMSE\", \"Score\": \"0.525\"}}, {\"LEADERBOARD\": {\"Task\": \"Monocular Depth Estimation\", \"Dataset\": \"NYU-Depth V2\", \"Metric\": \"RMSE\", \"Score\": \"0.526\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL VOC 2012 val\", \"Metric\": \"mIoU\", \"Score\": \"78.0%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL VOC 2012 val\", \"Metric\": \"mIoU\", \"Score\": \"77.3%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL VOC 2012 val\", \"Metric\": \"mIoU\", \"Score\": \"77.1%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"Tityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"82.6%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL Context\", \"Metric\":\"mIoU\", \"Score\": \"61.7\"}} ]"
    },
    {
        "Context": "POLY-GAN: MULTI-CONDITIONED GAN FOR FASHION SYNTHESIS A PREPRINT We present Poly-GAN, a novel conditional GAN architecture that is motivated by Fashion Synthesis, an application where garments are automatically placed on images of human models at an arbitrary pose. Poly-GAN allows conditioning on multiple inputs and is suitable for many tasks, including image alignment, image stitching and inpainting. Existing methods have a similar pipeline where three different networks are used to first align garments with the human pose, then perform stitching of the aligned garment and finally refine the results. Poly-GAN is the first instance where a common architecture is used to perform all three tasks. Our novel architecture enforces the conditions at all layers of the encoder and utilizes skip connections from the coarse layers of the encoder to the respective layers of the decoder. Poly-GAN is able to perform a spatial transformation of the garment based on the RGB skeleton of the model at an arbitrary pose. Additionally, Poly-GAN can perform image stitching, regardless of the garment orientation, and inpainting on the garment mask when it contains irregular holes. Our system achieves state-of-the-art quantitative results on Structural Similarity Index metric and Inception Score metric using the DeepFashion dataset. * Use footnote for providing further information about author (webpage, alternative address)-not for acknowledging funding agencies. while creating our training and testing datasets from the publicly available DeepFashion dataset Liu et al. We have 14,221 training samples, as in the VITON dataset, and 900 paired samples for testing our method which are widely accepted metrics for evaluation of images generated by GANs Since the code for CP-VTON is publicly released WangB et al., we are able to obtain results on our dataset for comparison We perform our evaluation on test data that consist of 900 paired images This limitation can be overcome by using a larger and more diverse dataset for training For qualitative evaluation, we also present images that were randomly generated with StyleGAN in Table 1: Fashion Synthesis Quantitative Results. Bold numbers indicate best performance. SSIM Metric IS",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Virtual Try-on\", \"Dataset\": \"Deep-Fashion\", \"Metric\": \"IS\", \"Score\": \"2.7904\"}}, {\"LEADERBOARD\": {\"Task\": \"Virtual Try-on\", \"Dataset\": \"Deep-Fashion\", \"Metric\": \"SSIM\", \"Score\": \"0.7251\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Inpainting\", \"Dataset\": \"RealBl\", \"Metric\": \"FID\", \"Score\": \"7.73\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Inpainting\", \"Dataset\": \"RealBl\", \"Metric\": \"Inception score\", \"Score\": \"8.12\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Inpainting\", \"Dataset\": \"RealBl\", \"Metric\": \"SSIM\", \"Score\": \"0.59\"}} ]"
    },
    {
        "Context": "Sparse and noisy LiDAR completion with RGB guidance and uncertainty This work proposes anew method to accurately complete sparse LiDAR maps guided by RGB images. For autonomous vehicles and robotics the use of LiDAR is indispensable in order to achieve precise depth predictions. A multitude of applications depend on the awareness of their surroundings, and use depth cues to reason and react accordingly. On the one hand, monocular depth prediction methods fail to generate absolute and precise depth maps. On the other hand, stereoscopic approaches are still significantly outperformed by LiDAR based approaches. The goal of the depth completion task is to generate dense depth predictions from sparse and irregular point clouds which are mapped to a 2D plane. We propose anew framework which extracts both global and local information in order to produce proper depth maps. We argue that simple depth completion does not require a deep network. However, we additionally propose a fusion method with RGB guidance from a monocular camera in order to leverage object information and to correct mistakes in the sparse input. This improves the accuracy significantly. Moreover, confidence masks are exploited in order to take into account the uncertainty in the depth predictions from each modality. This fusion method outperforms the state-of-the-art and ranks first on the KITTI depth completion benchmark [21]. Our code with visualizations is available at https: // github. com/ wvangansbeke/ Sparse-Depth-Completion . We evaluate our framework by computing the loss on all pixels of the ground truth since not all input pixels of the Li-DAR are correct The KITTI dataset provides 85898 frames for training, 1000 frames for evaluation and 1000 frames for testing Table 1. Hourglass network. Filters Table 2. Ablation study on KITTI's selected val- idation set. 3223 1473 RMSE [ mm ] MAE [ mm ] Table 3. Comparison with state-of-the-art on the testset based on RMSE[mm], MAE[mm] and t[s]. MAE t RMSE",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Depth Completion\", \"Dataset\": \"KITTI Depth Completion\", \"Metric\": \"MAE\", \"Score\": \"215.02\"}}, {\"LEADERBOARD\": {\"Task\": \"Depth Completion\", \"Dataset\": \"KITTI Depth Completion\", \"Metric\": \"RMSE\", \"Score\": \"772.87\"}}, {\"LEADERBOARD\": {\"Task\": \"Depth Completion\", \"Dataset\": \"KITTI Depth Completion\", \"Metric\": \"Runtime [ms]\", \"Score\": \"20\"}}, {\"LEADERBOARD\": {\"Task\": \"Depth Completion\", \"Dataset\": \"KITTI Depth Completion\", \"Metric\": \"iMAE\", \"Score\": \"0.93\"}}, {\"LEADERBOARD\": {\"Task\": \"Depth Completion\", \"Dataset\": \"KITTI Depth Completion\", \"Metric\": \"iRMSE\", \"Score\": \"2.19\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Depth Completion\", \"Dataset\": \"KITTI Depth Completion\", \"Metric\": \"MAE\", \"Score\": \"749.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Depth Completion\", \"Dataset\": \"KITTI Depth Completion\", \"Metric\": \"RMSE\", \"Score\": \"8.2\"}} ]"
    },
    {
        "Context": "An Efficient Admission Control Algorithm for Load Balancing In Hierarchical Mobile IPv6 Networks Anchor Point (MAP) may become a single point of bottleneck as it handles more and more mobile nodes (MNs). A number of schemes have been proposed to achieve load balancing among different MAPs. However, signaling reduction is still imperfect because these schemes also avoid the effect of the number of CN's. Also only the balancing of MN is performed, but not the balancing of the actual traffic load, since CN of each MN maybe different. This paper proposes an efficient admission control algorithm along with a replacement mechanism for HMIPv6 networks. The admission control algorithm is based on the number of serving CNs and achieves actual load balancing among MAPs. Moreover, a replacement mechanism is introduced to decrease the new MN blocking probability and the handoff MN dropping probability. By simulation results, we show that, the handoff delay and packet loss are reduced in our scheme, when compared with the standard HMIPv6 based handoff.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Rotation Equivariant CNNs for Digital Pathology We propose anew model for digital pathology segmentation, based on the observation that histopathology images are inherently symmetric under rotation and reflection. Utilizing recent findings on rotation equivariant CNNs, the proposed model leverages these symmetries in a principled manner. We present a visual analysis showing improved stability on predictions, and demonstrate that exploiting rotation equivariance significantly improves tumor detection performance on a challenging lymph node metastases dataset. We further present a novel derived dataset to enable principled comparison of machine learning models, in combination with an initial benchmark. Through this dataset, the task of histopathology diagnosis becomes accessible as a challenging benchmark for fundamental machine learning research. * Equal contribution. To evaluate the proposed model, we use Camelyon16 and PCam (1) The Camelyon16 dataset contains 400 H&E stained WSIs of sentinel lymph node sections split into 270 slides with pixel-level annotations for training and 130 unlabeled slides for testing In the Camelyon16 challenge, model performance is evaluated using the FROC curve for tumor localization (2) The PCam dataset contains 327,680 patches extracted from Camelyon16 at a size of 96 \u00d7 96 pixels @ 10\u00d7 magnification, with a 75/12.5/12.5% train/validate/test split, selected using a hard-negative mining regime 1 (3) The BreakHis dataset contains 7909 H&E stained microscopy images at a size of 700 \u00d7 460 pixels We limit our evaluation to the images at 4\u00d7 magnification, for which previous approaches have reported the highest accuracy For the evaluation on the WSI-level Camelyon16 benchmarks, we largely follow the pipeline proposed in, uniformly sampling WSIs and drawing tumor/nontumor patches with equal probability We focus Table 1: Performance on PCam, measured by negative log-likelihood, accuracy and AUC. Experiments with additional data augmentation with 90 \u2022 rotations and reflections are marked by +. M indicates matching number of Z 2 maps, #W number of weights, K number of Z 2 maps per layer. Acc NLL AUC DenseNet 90 80 70 60 50 40 FROC 100 70 Data P4M - DenseNet",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Breast Tumour Classification\", \"Dataset\": \"PCam\", \"Metric\": \"AUC\", \"Score\": \"0.963\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Medical Image Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"PQ\", \"Score\": \"47.0\"}}, { \"LEADERBOARD\": { \"Task\": \"Medical Image Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"PQst\", \"Score\": \"11.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Medical Image Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"PQth\", \"Score\": \"0.56\"}} ]"
    },
    {
        "Context": "Continual Occlusions and Optical Flow Estimation Two optical flow estimation problems are addressed: i) occlusion estimation and handling, and ii) estimation from image sequences longer than two frames. The proposed ContinualFlow method estimates occlusions before flow, avoiding the use of flow corrupted by occlusions for their estimation. We show that providing occlusion masks as an additional input to flow estimation improves the standard performance metric by more than 25% on both KITTI and Sintel. As a second contribution, a novel method for incorporating information from past frames into flow estimation is introduced. The previous frame flow serves as an input to occlusion estimation and as a prior in occluded regions, i.e. those without visual correspondences. By continually using the previous frame flow, ContinualFlow performance improves further by 18% on KITTI and 7% on Sintel, achieving top performance on KITTI and Sintel. The ContinualFlow network is trained using a curriculum learning approach starting from a dataset with less complex motions and increasing gradually the task complexity First, we train on FlyingChairs dataset using the training parameters introduced in and following the learning rate schedule from Since the FlyingChairs dataset contains only two frames sequences and no occlusion ground truth, we cannot train the full ContinualFlow model with temporal connections and the occlusion map estimation Next, the all parts of the ContinualFlow network are trained on the Fly-ingThings dataset Since occlusion maps were not available for this dataset, we computed them using the available backward and forward ground truth flows and the object segmentation masks Finally, the ContinualFlow is trained on data from six datasets: Driving, KITTI'15, VirtualKITTI, Sintel, HD1K and the FlyingChairs small motions dataset These datasets, except for FlyingChairs, contain sequences longer than two frames and are suitable for the training of Table 1: Ablation study of ContinualFlow. The leftmost column codes the experiment configurations: occlusion estimator (+OC); refinement network (+R); temporal connection with forward warping (W f ), backward warping (W b ) and both warping methods (W bf ); previous flow input in the refinement (RW x ); and two pass (2pass) initialisation of the first frame of the sequence N frames long. Performance measure are the KITTI 3-pixel error metric (column Fl) and the end-point error (in pixels, all other columns) for background (bg), fore- ground (fg), occluded (occ), non-occluded (noc) and all (all) pixels. The best performance in bold. All models trained on FlyingChairs and fine-tuned on Fly- ingThings. See section 3 for details. all Sintel Clean Influence of coordinate warping methods noc Occlusion map learning common : baseline The specialised refinement block Number of refinement blocks occ - bg occ - fg noc - bg noc - fg",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Optical Flow Estimation\", \"Dataset\": \"Sintel-final\", \"Metric\": \"Average End-Point Error\", \"Score\": \"4.52\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Optical Flow Estimation\", \"Dataset\": \"Sintel-clean\", \"Metric\": \"Average End-Point Error\", \"Score\": \"3.52\"}}, { \"LEADERBOARD\": { \"Task\": \"Optical Flow Estimation\", \"Dataset\": \"Sintel-final\", \"Metric\": \"Average End-Point Error\", \"Score\": \"4.01\"}} ]"
    },
    {
        "Context": "Iterative Context-Aware Graph Inference for Visual Dialog Visual dialog is a challenging task that requires the comprehension of the semantic dependencies among implicit visual and textual contexts. This task can refer to the relation inference in a graphical model with sparse contexts and unknown graph structure (relation descriptor), and how to model the underlying context-aware relation inference is critical. To this end, we propose a novel Context-Aware Graph (CAG) neural network. Each node in the graph corresponds to a joint semantic feature, including both objectbased (visual) and history-related (textual) context representations. The graph structure (relations in dialog) is iteratively updated using an adaptive top-K message passing mechanism. Specifically, in every message passing step, each node selects the most K relevant nodes, and only receives messages from them. Then, after the update, we impose graph attention on all the nodes to get the final graph embedding and infer the answer. In CAG, each node has dynamic relations in the graph (different related K neighbor nodes), and only the most relevant nodes are attributive to the context-aware relational graph inference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG outperforms comparative methods. Visualization results further validate the interpretability of our method. Datasets Table 1. Ablation studies of different iterative steps T and the main components on VisDial val v0.9. features . Our model with VGG features is denoted as CAG - VGG . As shown in Table 1 , compared with CAG , CAG w / o relational reasoning . Learning the implicit relations among Graph - based Models CAG Attention - based Models s , CAG removes the whole dynamic directed - graph inference MRR\u2191 R@1 Mean\u2193 R@10\u2191 R@5 Top8 Top16 Top36 R@1\u2191 R@10 MRR R@5\u2191 Main Component Comparison . A few variants are pro - the nodes is helpful to predict the final answer . CAG w / o Infer denotes that of T = 3 already performs well . In the following experi - Top2 Top1 posed for ablation study . CAG w / o Q - att , which replaces ques - Table 2. Performance comparison on VisDial val",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"MRR\", \"Score\": \"0.6756\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"Mean Rank\", \"Score\": \"3.75\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"R@1\", \"Score\": \"54.64\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"R@10\", \"Score\": \"91.48\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"R@5\", \"Score\": \"83.72\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"Visual Dialog v1.0 test-std\", \"Metric\": \"MRR (x 100)\", \"Score\": \"63.49\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"Visual Dialog v1.0 test-std\", \"Metric\": \"Mean\", \"Score\": \"4.11\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"Visual Dialog v1.0 test-std\", \"Metric\": \"NDCG (x 100)\", \"Score\": \"56.64\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"Visual Dialog v1.0 test-std\", \"Metric\": \"R@1\", \"Score\": \"49.85\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"Visual Dialog v1.0 test-std\", \"Metric\": \"R@10\", \"Score\": \"90.15\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Dialog\", \"Dataset\": \"Visual Dialog v1.0 test-std\", \"Metric\": \"R@5\", \"Score\": \"80.63\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"MRR\", \"Score\": \"0.6285\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"Mean Rank\", \"Score\": \"4.57\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"R@1\", \"Score\": \"48.95\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"R@10\", \"Score\": \"88.36\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Dialog\", \"Dataset\": \"VisDial v0.9 val\", \"Metric\": \"R@5\", \"Score\": \"79.65\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Dialog\", \"Dataset\": \"Visual Dialog v1.0 test-std\", \"Metric\": \"MRR (x 100)\", \"Score\": \"61.37\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Dialog\", \"Dataset\": \"Visual Dialog v"
    },
    {
        "Context": "Open Question Answering with Weakly Supervised Embedding Models Building computers able to answer questions on any subject is along standing goal of artificial intelligence. Promising progress has recently been achieved by methods that learn to map questions to logical forms or database queries. Such approaches can be effective but at the cost of either large amounts of human-labeled data or by defining lexicons and grammars tailored by practitioners. In this paper, we instead take the radical approach of learning to map questions to vectorial feature representations. By mapping answers into the same space one can query any knowledge base independent of its schema, without requiring any grammar or lexicon. Our method is trained with anew optimization procedure combining stochastic gradient descent followed by a fine-tuning step using the weak supervision provided by blending automatically and collaboratively generated resources. We empirically demonstrate that our model can capture meaningful signals from its noisy supervision leading to major improvements over paralex, the only existing method able to be trained on similar weakly labeled data. In this last experimental section, we tend to evaluate how generic our learned system is To this end, we propose to ask our model to answer questions coming from another dataset from the literature, but without retraining it with labeled data, just by directly using the parameters learned on WikiAnswers+ReVerb Still, fora system trained with almost no manual annotation nor prior information on another dataset, with another -very noisy-KB, the results can be seen as particularly promising Besides, evaluation is broad since, in ReVerb, most entities actually appear many times under different names as explained in Section 3 Hence, there might be higher ranked answers but they are missed by our evaluation script Table 3. Performance of variants of our embedding models and Paralex [10] for rerank- ing question-answer pairs from the WikiAnswers+ReVerb test set. F1 Prec Recall MAP Table 4. Performance of our embedding model for retrieving answers for questions from the WikiAnswers+ReVerb test set, among the whole ReVerb KB (14M candidates). F1 Table 6. Performance of our embedding model for retrieving answers for questions from the WebQuestions test set, among the whole ReVerb KB (14M candidates). Top - 1 Top - 10 F1",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Question Answering\", \"Dataset\": \"WebQuestions\", \"Metric\": \"F1\", \"Score\": \"29.7%\"}}, {\"LEADERBOARD\": {\"Task\": \"Question Answering\", \"Dataset\": \"Reverb\", \"Metric\": \"Accuracy\", \"Score\": \"73%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Question Answering\", \"Dataset\": \"Weakly-supervised\", \"Metric\": \"MAP\", \"Score\": \"76.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Question Answering\", \"Dataset\": \"Weakly-supervised\", \"Metric\": \"F1\", \"Score\": \"80.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Question Answering\", \"Dataset\": \"Weakly-supervised\", \"Metric\": \"MAP\", \"Score\": \"74.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Question Answering\", \"Dataset\": \"Weakly-supervised\", \"Metric\": \"F1\", \"Score\": \"80.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Question Answering\", \"Dataset\": \"Weakly-supervised\", \"Metric\": \"MAP\", \"Score\": \"68.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Question Answering\", \"Dataset\": \"Weakly-supervised\", \"Metric\": \"F1\", \"Score\": \"76.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Question Answering\", \"Dataset\": \"Weakly-supervised\", \"Metric\": \"MAP\", \"Score\": \"75.4\"}}, { 'LEADERBOARD'"
    },
    {
        "Context": "MAGNN: Metapath Aggregated Graph Neural Network for Heterogeneous Graph Embedding A large number of real-world graphs or networks are inherently heterogeneous, involving a diversity of node types and relation types. Heterogeneous graph embedding is to embed rich structural and semantic information of a heterogeneous graph into low-dimensional node representations. Existing models usually define multiple metapaths in a heterogeneous graph to capture the composite relations and guide neighbor selection. However, these models either omit node content features, discard intermediate nodes along the metapath, or only consider one metapath. To address these three limitations, we propose anew model named Metapath Aggregated Graph Neural Network (MAGNN) to boost the final performance. Specifically, MAGNN employs three major components, i.e., the node content transformation to encapsulate input node attributes, the intra-metapath aggregation to incorporate intermediate semantic nodes, and the inter-metapath aggregation to combine messages from multiple metapaths. Extensive experiments on three real-world heterogeneous graph datasets for node classification, node clustering, and link prediction show that MAGNN achieves more accurate prediction results than state-of-the-art baselines. How do we understand the representation capability of different graph embedding methods? We adopt three widely used heterogeneous graph datasets from different domains to evaluate the performance of MAGNN as compared to state-of-the-art baselines Specifically, the IMDb and DBLP datasets are used in the experiments for node classification and node clustering The Last.fm dataset is used in the experiments for link prediction Simple statistics of the three datasets are summarized in, and network schemas are illustrated in We assign one-hot id vectors to nodes with no attributes as their dummy input features.: Network schemas of the three heterogeneous graph datasets used in this paper We adopt a dataset released by HetRec 2011, consisting of 1892 users, 17632 artists, and 1088 artist tags after data preprocessing This dataset is used for the link prediction task, and no label or feature is included in this dataset Table 3: Experiment results (%) on the IMDb and DBLP datasets for the node classification task. Train % ESim Semi - supervised GCN HERec HAN node2vec LINE MAGNN Unsupervised metapath2vec GAT Table 4: Experiment results (%) on the IMDb and DBLP datasets for the node clustering task. Metrics ESim Semi - supervised GCN HERec HAN node2vec LINE MAGNN Unsupervised metapath2vec GAT Table 5: Experiment results (%) on the Last.fm dataset for the link prediction task. ESim GCN HERec HAN node2vec GATNE LINE MAGNN metapath2vec GAT Table 6: Quantitative results (%) for ablation study. Variant IMDb Macro - F1 N / A ARI Last . fm NMI DBLP Micro - F1 AUC AP",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"Last.FM\", \"Metric\": \"AP\", \"Score\": \"98.93\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"Last.FM\", \"Metric\": \"AUC\", \"Score\": \"98.91\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"DBLP\", \"Metric\": \"Accuracy\", \"Score\": \"84.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"DBLP\", \"Metric\": \"Accuracy\", \"Score\": \"95.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"DBLP\", \"Metric\": \"Macro F1\", \"Score\": \"83.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"DBLP\", \"Metric\": \"Accuracy\", \"Score\": \"81.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"DBLP\", \"Metric\": \"Macro F1\", \"Score\": \"69.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"DBLP\", \"Metric\": \"Macro F1\", \"Score\": \"73.6\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"DBLP\", \"Metric\": \"Macro F1\", \"Score\": \"75.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction"
    },
    {
        "Context": "End-to-End Object Detection with Transformers We present anew method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, area set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr. We show that DETR achieves competitive results compared to Faster R-CNN in quantitative evaluation on COCO We perform experiments on COCO 2017 detection and panoptic segmentation datasets, containing 118k training images and 5k validation images Table 1: Comparison with Faster R-CNN with a ResNet-50 and ResNet-101 backbones on the COCO validation set. The top section shows results for Faster R-CNN models in Detectron2 [50], the middle section shows results for Faster R-CNN models with GIoU [38], random crops train-time augmentation, and the long 9x training schedule. DETR models achieve comparable results to heavily tuned Faster R-CNN baselines, having lower APS but greatly improved APL. We use torchscript Faster R-CNN and DETR models to measure FLOPS and FPS. Results without R101 in the name corre- spond to ResNet-50. longer schedule used to compare with Faster R - CNN we train for 500 epochs AP50 AP75 APS APM APL AP Table 2: Effect of encoder size. Each row corresponds to a model with varied number of encoder layers and fixed number of decoder layers. Performance gradually improves with more encoder layers. AP50 APS #params APL APM AP Table",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"AP\", \"Score\": \"33\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"PQ\", \"Score\": \"45.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"PQst\", \"Score\": \"37\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"PQth\", \"Score\": \"50.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"RQ\", \"Score\": \"55.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"RQst\", \"Score\": \"46\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"RQth\", \"Score\": \"61.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"SQ\", \"Score\": \"79.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"SQst\", \"Score\": \"78.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"SQth\", \"Score\": \"80.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"AP\", \"Score\": \"39.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"PQ\", \"Score\": \"44.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"PQst\", \"Score\": \"33.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"PQth\", \"Score\": \"51.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"RQ\", \"Score\": \"53.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"RQst\", \"Score\": \"42.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"RQth\", \"Score\": \"60.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"SQ\", \"Score\": \"79.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"SQst\", \"Score\": \"74.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO panoptic\", \"Metric\": \"SQth\", \"Score\": \"83.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"AP50\", \"Score\": \"64.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"AP75\", \"Score\": \"47.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APL\", \"Score\": \"62.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APM\", \"Score\": \"49.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APS\", \"Score\": \"23.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"box AP\", \"Score\": \"44.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"AP50\", \"Score\": \"63.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"AP75\", \"Score\": \"47.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APL\", \"Score\": \"56\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APM\", \"Score\": \"48.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APS\", \"Score\": \"27.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"box AP\", \"Score\": \"44\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"PQ\", \"Score\": \"48.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"PQst\", \"Score\": \"38.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"PQth\", \"Score\": \"52.0\"}} ]"
    },
    {
        "Context": "Parallelizing Legendre Memory Unit Training Recently, anew recurrent neural network (RNN) named the Legendre Memory Unit (LMU) was proposed and shown to achieve state-of-the-art performance on several benchmark datasets. Here we leverage the linear time-invariant (LTI) memory component of the LMU to construct a simplified variant that can be parallelized during training (and yet executed as an RNN during inference), thus overcoming a well known limitation of training RNNs on GPUs. We show that this reformulation that aids parallelizing, which can be applied generally to any deep network whose recurrent components are linear, makes training up to 200 times faster. Second, to validate its utility, we compare its performance against the original LMU and a variety of published LSTM and transformer networks on seven benchmarks, ranging from psMNIST to sentiment analysis to machine translation. We demonstrate that our models exhibit superior performance on all datasets, often using fewer parameters. For instance, our LMU sets anew state-of-the-art result on psMNIST, and uses half the parameters while outperforming Dis-tilBERT and LSTM models on IMDB sentiment analysis.  Table 2. psMNIST results. The first three rows are from Voelker et al. (2019), and the fourth row is from Gu et al. (2020). results . Accuracy Table 3 . Mackey - Glass Table 3. Mackey-Glass results. NRMSE Table 4. IMDB, QQP and SNLI results. IMDB result is from 86 . 95 / 85 . 36 82 . 58 / 81 . 4 LSTM 1201 ) . Param . Acc . Our Model Table 5. IMDB results with pre-training. First row is from Radford et al. (2017), and the second row is from Sanh et al. (2019). Accuracy Table 6. Language modelling and translation results. text8 score is from Zhang et al. (2016), and IWSLT score is from Luong & Manning (2015). a (case sensitive), b (case insensitive). text8 IWSLT ' 15",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Sentiment Analysis\", \"Dataset\": \"IMDb\", \"Metric\": \"Accuracy\", \"Score\": \"93.20\"}}, {\"LEADERBOARD\": {\"Task\": \"Sequential Image Classification\", \"Dataset\": \"Sequential MNIST\", \"Metric\": \"Permuted Accuracy\", \"Score\": \"98.49%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"IWSLT2015 German-English\", \"Metric\": \"BLEU score\", \"Score\": \"36.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-German\", \"Metric\": \"BLEU score\", \"Score\": \"28.9\"}} ]"
    },
    {
        "Context": "3D Building Model Fitting Using A New Kinetic Framework We describe anew approach to fit the polyhedron describing a 3D building model to the point cloud of a Digital Elevation Model (DEM). We introduce anew kinetic framework that hides to its user the combinatorial complexity of determining or maintaining the polyhedron topology, allowing the design of a simple variational optimization.This new kinetic framework allows the manipulation of a bounded polyhedron with simple faces by specifying the target plane equations of each of its faces. It proceeds by evolving continuously from the polyhedron defined by its initial topology and its initial plane equations to a polyhedron that is as topologically close as possible to the initial polyhedron but with the new plane equations. This kinetic framework handles internally the necessary topological changes that maybe required to keep the faces simple and the polyhedron bounded. For each intermediate configurations where the polyhedron looses the simplicity of its faces or its boundedness, the simplest topological modification that is able to reestablish the simplicity and the boundedness is performed.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes Exploiting synthetic data to learn deep models has attracted increasing attention in recent years. However, the intrinsic domain difference between synthetic and real images usually causes a significant performance drop when applying the learned model to real world scenarios. This is mainly due to two reasons: 1) the model overfits to synthetic images, making the convolutional filters incompetent to extract informative representation for real images; 2) there is a distribution difference between synthetic and real data, which is also known as the domain adaptation problem. To this end, we propose anew reality oriented adaptation approach for urban scene semantic segmentation by learning from synthetic data. First, we propose a target guided distillation approach to learn the real image style, which is achieved by training the segmentation model to imitate a pretrained real style model using real images. Second, we further take advantage of the intrinsic spatial structure presented in urban scene images, and propose a spatialaware adaptation scheme to effectively align the distribution of two domains. These two modules can be readily integrated with existing state-of-the-art semantic segmentation networks to improve their generalizability when adapting from synthetic to real urban scenes. We evaluate the proposed method on Cityscapes dataset by adapting from GTAV and SYNTHIA datasets, where the results demonstrate the effectiveness of our method.\u2022 real distribution orientation: To deal with the dis- Following previous works, the experimental validation are conducted on the GTAV dataset and Cityscapes dataset We use GTAV dataset as our source domain, and we have access to the pixel-level annotation, and we use Cityscapes dataset as the target domain We briefly introduce the datasets used in our experiment in below: Cityscapes is a dataset focused on autonomous driving, which consists of 2, 975 images in training set, and 500 images for validation GTAV is a dataset recently proposed for learning from synthetic data The resolution of images is around 2000 \u00d7 1000 pixels which is similar with Cityscapes, the semantic categories are also compatible between the two datasets Table 1. The segmentation results on the Cityscapes dataset by using the GTAV dataset as the source domain. DeepLab and PSPNet are sidewalk sky motorbike bus bicycle truck sign vegetation pole rider building IoU light road car person mean wall fence terrain traffic train Table 2. Comparison with state-of-the-arts methods for semantic segmentation on Cityscapes using synthetic datasets as the training data. Top: adapting from GTAV, Bottom: adapting from SYNTHIA. Results of state-of-the-art methods are from their papers. We use VGG-16 as the backbone network for fair comparison. The best results are denoted in bold. sidewalk sky motorbike bus bicycle truck sign vegetation pole rider building IoU light road car person mean wall fence terrain traffic train",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Synthetic-to-Real Translation\", \"Dataset\": \"GTAV-to-Cityscapes Labels\", \"Metric\": \"mIoU\", \"Score\": \"39.4\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"82.6%\"}} ]"
    },
    {
        "Context": "A Peer to Peer Protocol for Online Dispute Resolution over Storage Consumption In bilateral accounting of resource consumption both the consumer and provider independently measure the amount of resources consumed by the consumer. The problem here is that potential disparities between the provider's and consumer's accountings, might lead to conflicts between the two parties that need to be resolved. We argue that with the proper mechanisms available, most of these conflicts can be solved online, as opposite to in court resolution; the design of such mechanisms is still a research topic; to help cover the gap, in this paper we propose a peer-to-peer protocol for online dispute resolution over storage consumption. The protocol is peer-to-peer and takes into consideration the possible causes (e.g, transmission delays, unsynchronized metric collectors, etc.) of the disparity between the provider's and consumer's accountings to make, if possible, the two results converge.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Defense Strategies Against Modern Botnets Botnets are networks of compromised computers with malicious code which are remotely controlled and which are used for starting distributed denial of service (DDoS) attacks, sending enormous number of e-mails (SPAM) and other sorts of attacks. Defense against modern Botnets is areal challenge. This paper offers several strategies for defense against Botnets with a list and description of measures and activities which should be carried out in order to establish successful defense. The paper also offers parallel preview of the strategies with their advantages and disadvantages considered in accordance with various criteria.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Hierarchical Transformer Network for Utterance-level Emotion Recognition While there have been significant advances in detecting emotions in text, in the field of utterancelevel emotion recognition (ULER), there are still many problems to be solved. In this paper, we address some challenges in ULER in dialog systems.(1) The same utterance can deliver different emotions when it is in different contexts or from different speakers.(2) Long-range contextual information is hard to effectively capture. (3) Unlike the traditional text classification problem, this task is supported by a limited number of datasets, among which most contain inadequate conversations or speech. To address these problems, we propose a hierarchical transformer framework (apart from the description of other studies, the \"transformer\" in this paper usually refers to the encoder part of the transformer) with a lower-level transformer to model the word-level input and an upper-level transformer to capture the context of utterance-level embeddings. We use a pretrained language model bidirectional encoder representations from transformers (BERT) as the lower-level transformer, which is equivalent to introducing external data into the model and solve the problem of data shortage to some extent. In addition, we add speaker embeddings to the model for the first time, which enables our model to capture the interaction between speakers. Experiments on three dialog emotion datasets, Friends, Emotion-Push, and EmoryNLP, demonstrate that our proposed hierarchical transformer network models achieve 1.98%, 2.83%, and 3.94% improvement, respectively, over the state-of-the-art methods on each dataset in terms of macro-F1. In this section, we present the datasets, evaluation metrics, baselines and experimental results of our model Friends: The dataset is annotated from the Friends TV Scripts, and each dialog in the dataset consists of a scene of multiple speakers EmotionPush: The dataset consists of private conversations between friends on Facebook include 1,000 dialogs, which are split into 720, 80, and 200 dialogs for training, validation and testing, respectively Each utterance is tagged with an emotion in a set of emotions as in the Friends dataset EmoryNLP: The dataset is annotated from the Friends TV Scripts as well For the first two datasets, we follow previous works to consider only four emotion classes, i.e., anger, joy, sadness, and neutral, and consider all the emotion classes for EmoryNLP Following, which achieved the best performance on several ULER datasets, we choose macro-averaged F1-score as the primary metric for evaluating the performance of our models Table 1: Detailed descriptions of Friends and EmotionPush Others Hap / Joy Neu Table 2: Detailed descriptions of EmoryNLP Dataset #Dialog ( #Utterance ) Neutral Mad Joyful Peaceful Scared Table 1 : Detailed descriptions of Friends and EmotionPush Emotion Powerful Table 3: Testing results on Friend, EmotionPush, and EmoryNLP Model - Macro - F1 EmoryNLP Friends WA - UWA EmotionPush",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Emotion Recognition in Conversation\", \"Dataset\": \"EmotionPush\", \"Metric\": \"Unweighted Accuracy\", \"Score\": \"63.03\"}}, {\"LEADERBOARD\": {\"Task\": \"Emotion Recognition in Conversation\", \"Dataset\": \"EmotionPush\", \"Metric\": \"Weighted Accuracy\", \"Score\": \"86.92\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Emotion Recognition in Conversation\", \"Dataset\": \"EC\", \"Metric\": \"Micro-F1\", \"Score\": \"83.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Emotion Recognition in Conversation\", \"Dataset\": \"EC\", \"Metric\": \"Micro-F1\", \"Score\": \"83.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Emotion Recognition in Conversation\", \"Dataset\": \"NYT\", \"Metric\": \"Micro-F1\", \"Score\": \"73.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Emotion Recognition in Conversation\", \"Dataset\": \"NYT\", \"Metric\": \"Micro-F1\", \"Score\": \"71\"}} ]"
    },
    {
        "Context": "Spectral Sparsification of Graphs * We introduce anew notion of graph sparsification based on spectral similarity of graph Laplacians: spectral sparsification requires that the Laplacian quadratic form of the sparsifier approximate that of the original. This is equivalent to saying that the Laplacian of the sparsifier is a good preconditioner for the Laplacian of the original.We prove that every graph has a spectral sparsifier of nearly-linear size. Moreover, we present an algorithm that produces spectral sparsifiers in time O (m log c m), where m is the number of edges in the original graph and c is some absolute constant. This construction is a key component of a nearly-linear time algorithm for solving linear equations in diagonallydominant matrices.Our sparsification algorithm makes use of a nearly-linear time algorithm for graph partitioning that satisfies a strong guarantee: if the partition it outputs is very unbalanced, then the larger part is contained in a subgraph of high conductance. * This paper is the second in a sequence of three papers expanding on material that appeared first under the title \"Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems\" [ST04]. The first paper, \"A Local Clustering Algorithm for Massive Graphs and its Application to Nearly-Linear Time Graph Partitioning\" [ST08a] contains graph partitioning algorithms that are used to construct the sparsifiers in this paper. The third paper, \"Nearly-Linear Time Algorithms for Preconditioning and Solving Symmetric, Diagonally Dominant Linear Systems\" [ST08b] contains the results on solving linear equations and approximating eigenvalues and eigenvectors.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts Prevalent models based on artificial neural network (ANN) for sentence classification often classify sentences in isolation without considering the context in which sentences appear. This hampers the traditional sentence classification approaches to the problem of sequential sentence classification, where structured prediction is needed for better overall classification performance. In this work, we present a hierarchical sequential labeling network to make use of the contextual information within surrounding sentences to help classify the current sentence. Our model outperforms the state-of-the-art results by 2%-3% on two benchmarking datasets for sequential sentence classification in medical scientific abstracts. We evaluate our model on two sources of benchmarking datasets on medical scientific abstracts, where each sentence of the abstract is annotated with one label that is associated with the rhetorical structure summarizes the statistics of the two datasets NICTA-PIBOSO This dataset 2 was shared from the ALTA 2012 Shared Task, the goal of which is to build automatic sentence classifiers that can map the sentences from biomedical abstracts into a set of pre-defined categories for Evidence-Based Medicine (EBM) PubMed RCT This new dataset was curated by 3 and is currently the largest dataset for sequential sentence classification Table 1: Datasets statistics. |C| denotes the number of labels, |V | represents the vocabulary size. For the train, validation, and test sets, we indicate the number of abstracts followed by the number of sentences in parentheses. Validation 200 ( 2 . 2k ) Test 80 ( 0 . 9k ) Table 3: Hyperparameter settings. d hs : hidden size of the sentence-level RNN layer (single direction); d hd : hidden size of the abstract-level bi-LSTM layer (single direction); d a : dimension of the context vector u s ; r: number of context vectors; \u03b2: coefficient of the dropout regularization added to the total loss; dr: dropout. c Parameter hd - 75 1 CNN RNN 15 4 5 NICTA PubMed Table 4: Comparison of F1 scores (weighted average by support (the number of true instances for each la- bel)) between our model and the best published meth- ods. The presented",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Sentence Classification\", \"Dataset\": \"PubMed 20k RCT\", \"Metric\": \"F1\", \"Score\": \"92.60\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"IMDb\", \"Metric\": \"Accuracy\", \"Score\": \"83.24\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"IMDb\", \"Metric\": \"Accuracy\", \"Score\": \"61. Intervention\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"IMDb\", \"Metric\": \"Accuracy\", \"Score\": \"89.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"IMDb\", \"Metric\": \"Accuracy\", \"Score\": \"83.38\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"IMDb\", \"Metric\": \"Accuracy\", \"Score\": \"82.81\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"IMDb\", \"Metric\": \"Accuracy\", \"Score\": \"82.44\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"IMDb\", \"Metric\": \"Accuracy\", \"Score\": \"81.42\"}}, { 'LEADER"
    },
    {
        "Context": "TVQA: Localized, Compositional Video Question Answering Recent years have witnessed an increasing interest in image-based question-answering (QA) tasks. However, due to data limitations, there has been much less work on video-based QA. In this paper, we present TVQA, a largescale video QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairs from 21,793 clips, spanning over 460 hours of video. Questions are designed to be compositional in nature, requiring systems to jointly localize relevant moments within a clip, comprehend subtitle-based dialogue, and recognize relevant visual concepts. We provide analyses of this new dataset as well as several baselines and a multi-stream end-to-end trainable neural network framework for the TVQA task. The dataset is publicly available at We collected our dataset on 6 long-running TV shows from 3 genres: 1) sitcoms: The Big Bang Theory, How I Met Your Mother, Friends, 2) medical dramas: Grey's Anatomy, House, 3) crime drama: Castle On average, our questions contain 13.5 words, which is fairly long compared to other datasets The prompt of \"when\", \"after\", \"before\" account for 60.03%, 30.19% and 9.78% respectively of our dataset Differences among our 6 TV Shows: The videos used in our dataset are from 6 different TV shows.: Human accuracy on test-public set based on different sources Comparison with Other Datasets: presents a comparison of our dataset to some recently proposed video question answering datasets MovieQA is most similar to our dataset, with both multiple choice questions and timestamp annotation Human Evaluation on Usefulness of Video and Subtitle in Dataset: To gain a better understand-ing of the roles of videos and subtitles in the our dataset, Table 1: Statistics for different question types based on first question word. Q = question, CA = correct answer, WA = wrong answer. Length is defined as the number of words in the sentence. Person ( who ) 10% Reasoning ( why ) 6% 15% #QA Q . Len . WA . Len . CA . Len . Method ( how ) Reasoning ( why ) Table 2: Data Statistics for each TV show. BBT = The Big Bang Theory, HIMYM = How I Met You Mother, Grey = Grey's Anatomy, House = House M.D., Epi = Episode, Sea. = Season #Epi . #QA 58 29 , 384 4 , 198 72 4 , 621 4 , 698 Table 5: Comparison of TVQA to various existing video QA datasets. OE = open-ended, MC = multiple-choices. Q. Src. = Question Sources, it indicates where the questions are raised from. TVQA dataset is",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Video Question Answering\", \"Dataset\": \"SUTD-TrafficQA\", \"Metric\": \"1/2\", \"Score\": \"63.15\"}}, {\"LEADERBOARD\": {\"Task\": \"Video Question Answering\", \"Dataset\": \"SUTD-TrafficQA\", \"Metric\": \"1/4\", \"Score\": \"35.16\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Video Question Answering\", \"Dataset\": \"TVQA\", \"Metric\": \"Accuracy\", \"Score\": \"76.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Video Question Answering\", \"Dataset\": \"TVQA\", \"Metric\": \"Accuracy\", \"Score\": \"69.34\"}}, { \"LEADERBOARD\": { \"Task\": \"Video Question Answering\", \"Dataset\": \"TVQA\", \"Metric\": \"Accuracy\", \"Score\": \"62.55\"}}, { \"LEADERBOARD\": { \"Task\": \"Video Question Answering\", \"Dataset\": \"TVQA\", \"Metric\": \"Accuracy\", \"Score\": \"63.48\"}} ]"
    },
    {
        "Context": "Building Language Models for Text with Named Entities Text in many domains involves a significant amount of named entities. Predicting the entity names is often challenging fora language model as they appear less frequent on the training corpus. In this paper, we propose a novel and effective approach to building a discriminative language model which can learn the entity names by leveraging their entity type information. We also introduce two benchmark datasets based on recipes and Java programming codes, on which we evaluate the proposed model. Experimental results show that our model achieves 52.2% better perplexity in recipe generation and 22.06% on code generation than the stateof-the-art language models. 7 http://dbpedia.org/page/Lionel Messi We evaluate our proposed model on two different language generation tasks where there exist a lot of entity names in the text In this paper, we release all the codes and datasets For this task, we analyze a cooking recipe corpus Each instance in this corpus is an individual recipe and consists of many ingredi-6 While calculating the final probability distribution overall candidate words, with our joint inference schema, a strong state-of-art language model, without the type information, itself can work sufficiently well and replace the entity composite model We construct a Java code corpus where each instance is a Java method (i.e., function) We use the same dimensional word embedding (400 for recipe corpus, 300 for code corpus) to represent both of the entity name (e.g., \"apple\") and their entity type (e.g., \"fruits\") in all the models Accordingly, for the entity composite model which takes the concatenation of the entity Table 1: Comparing the performance of recipe gen- type model with type feature Size Perplexity Table 2: Comparing the performance of code genera- Size Perplexity Table 3: Performance of fill in the blank task. Accuracy Free - Form AWD LSTM MCQ Our",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Code Generation\", \"Dataset\": \"Android Repos\", \"Metric\": \"Perplexity\", \"Score\": \"2.65\"}}, {\"LEADERBOARD\": {\"Task\": \"Recipe Generation\", \"Dataset\": \"Now You're Cooking!\", \"Metric\": \"Perplexity\", \"Score\": \"9.67\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Recipe Generation\", \"Dataset\": \"Recipe\", \"Metric\": \"Accuracy\", \"Score\": \"0.275\"}}, { \"LEADERBOARD\": { \"Task\": \"Recipe Generation\", \"Dataset\": \"Recipe\", \"Metric\": \"F1\", \"Score\": \"0.695\"}}, { \"LEADERBOARD\": { \"Task\": \"Recipe Generation\", \"Dataset\": \"Recipe\", \"Metric\": \"Handbags\", \"Score\": \"0.295\"}}, { \"LEADERBOARD\": { \"Task\": \"Recipe Generation\", \"Dataset\": \"Recipe\", \"Metric\": \"Distribution\", \"Score\": \"0.151\"}}, { \"LEADERBOARD\": { \"Task\": \"Recipe Generation\", \"Dataset\": \"Recipe\", \"Metric\": \"Open-Domain Question Answering\", \"Score\": \"0.175\"}}, { \"LEADERBOARD\": { \"Task\": \"Recipe Generation\", \"Dataset\": \"Recipe\", \"Metric\": \"Handbags\", \"Score\": \"0.374\"}}, { \"LEADERBOARD\": { \"Task\": \"Recipe Generation\", \"Dataset\": \"Recipe\", \"Metric\": \"Dismatched\", \"Score\": \"0.468\"}}, { \"LEADERBOARD\": { '"
    },
    {
        "Context": "Competitive Multi-scale Convolution In this paper, we introduce anew deep convolutional neural network (ConvNet) module that promotes competition among a set of multi-scale convolutional filters. This new module is inspired by the inception module, where we replace the original collaborative pooling stage (consisting of a concatenation of the multi-scale filter outputs) by a competitive pooling represented by a maxout activation unit. This extension has the following two objectives: 1) the selection of the maximum response among the multi-scale filters prevents filter co-adaptation and allows the formation of multiple sub-networks within the same model, which has been shown to facilitate the training of complex learning problems; and 2) the maxout unit reduces the dimensionality of the outputs from the multi-scale filters. We show that the use of our proposed module in typical deep ConvNets produces classification results that are either better than or comparable to the state of the art on the following benchmark datasets: MNIST, CIFAR-10, CIFAR-100 and SVHN. We quantitatively measure the performance of our proposed models Competitive Multi-scale Convolution and Competitive Inception on four computer vision/machine learning benchmark datasets: CIFAR-10, CIFAR-100, MNIST and SVHN Finally, we compare the performance of the proposed Competitive Multi-scale Convolution and Competitive Inception with respect to the current state of the art in the four benchmark datasets mentioned above The CIFAR-10 dataset contains 60000 images of 10 commonly seen object categories (e.g., animals, vehicles, etc.), where 50000 images are used for training and the rest 10000 for testing, and all 10 categories have equal volume of training and test images The CIFAR-100 dataset extends CIFAR-10 by increasing the number of categories to 100, whereas the total number of images remains the same, so the CIFAR-100 dataset is considered as a harder classification problem than CIFAR-10 since it contains 10 times less images per class and 10 times more categories The well-known MNIST dataset Table 1. Results on CIFAR-10 of the proposed models, in addi- tion to the Competitive Single-scale Convolution and Competitive DropConnect Single-scale Convolution that test our research ques- tions posed in Sec. 3.1. Convolution DropConnect Single - scale Convolution that test our research ques - ( mean \u00b1 std dev ) Test Error No . of Params Train Time ( h ) Test Time ( ms ) Table 2. Results on MNIST of the proposed models, in addition to the Competitive Single-scale Convolution and Competitive Drop- Connect Single-scale Convolution that test our research questions posed in Sec. 3.1. Convolution ( mean \u00b1 std dev ) Test Error No . of Params Train Time ( h ) Test Time ( ms )",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"MNIST\", \"Metric\": \"Percentage error\", \"Score\": \"0.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-100\", \"Metric\": \"Percentage correct\", \"Score\": \"72.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"SVHN\", \"Metric\": \"Percentage error\", \"Score\": \"1.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Percentage correct\", \"Score\": \"93.1\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"MNIST\", \"Metric\": \"Percentage error\", \"Score\": \"0.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-100\", \"Metric\": \"Percentage correct\", \"Score\": \"73.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Percentage correct\", \"Score\": \"91.7\"}} ]"
    },
    {
        "Context": "Spoken Language Identification using ConvNets Language Identification (LI) is an important first step in several speech processing systems. With a growing number of voice-based assistants, speech LI has emerged as a widely researched field. To approach the problem of identifying languages, we can either adopt an implicit approach where only the speech fora language is present or an explicit one where text is available with its corresponding transcript. This paper focuses on an implicit approach due to the absence of transcriptive data. This paper benchmarks existing models and proposes anew attention based model for language identification which uses log-Mel spectrogram images as input. We also present the effectiveness of raw waveforms as features to neural network models for LI tasks. For training and evaluation of models, we classified six languages (English, French, German, Spanish, Russian and Italian) with an accuracy of 95.4% and four languages (English, French, German, Spanish) with an accuracy of 96.3% obtained from the VoxForge dataset. This approach can further be scaled to incorporate more languages. We classified six languages (English, French, German, Spanish, Russian and Italian) from the VoxForge dataset VoxForge is an open-source speech corpus which primarily consists of samples recorded and submitted by users using their own microphone Our dataset consists of 1,500 samples for each of six languages Out of 1,500 samples for each language, 1,200 were randomly selected as training dataset for that language and rest 300 as validation dataset using k-fold cross-validation Table 2: Architecture of the 1D-ConvNet model ( Convolutional Block 1 ) ( 128 , 26664 ) 384 Table 2 : Architecture of the 1D - ConvNet model # of parameters Table 3: Architecture of the 2D-ConvNet model Table 4: Results of the two models and all its variations 2D ConvNet log - Mel Spectra 2D ConvNet Table 4 : Results of the two models and all its variations Mixup Accuracy log - Mel Spectra",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Spoken language identification\", \"Dataset\": \"VoxForge European\", \"Metric\": \"Accuracy (%)\", \"Score\": \"96.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Spoken language identification\", \"Dataset\": \"VoxForge European\", \"Metric\": \"Accuracy (%)\", \"Score\": \"96.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Spoken language identification\", \"Dataset\": \"VoxForge European\", \"Metric\": \"Accuracy (%)\", \"Score\": \"94.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Spoken language identification\", \"Dataset\": \"VoxForge European\", \"Metric\": \"Accuracy (%)\", \"Score\": \"94.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Spoken language identification\", \"Dataset\": \"VoxForge European\", \"Metric\": \"Accuracy (%)\", \"Score\": \"93.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Spoken language identification\", \"Dataset\": \"VoxForge Commonwealth\", \"Metric\": \"Accuracy (%)\", \"Score\": \"95.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Spoken language identification\", \"Dataset\": \"VoxForge Commonwealth\", \"Metric\": \"Accuracy (%)\", \"Score\": \"95.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Spoken language identification\", \"Dataset\": \"VoxForge Commonwealth\", \"Metric\": \"Accuracy (%)\", \"Score\": \"94.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Spoken language identification\", \"Dataset\": \"VoxForge Commonwealth\", \"Metric\": \"Accuracy (%)\", \"Score\": \"93.7\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Speech Recognition\", \"Dataset\": \"VoxFornch\", \"Metric\": \"Word Error Rate (WER)\", \"Score\": \"6.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Speech Recognition\", \"Dataset\": \"VoxFornch\", \"Metric\": \"Word Error Rate (WER)\", \"Score\": \"16.5\"}} ]"
    },
    {
        "Context": "GLOBAL-TO-LOCAL MEMORY POINTER NETWORKS FOR TASK-ORIENTED DIALOGUE End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-theart models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation. We use two public multi-turn task-oriented dialogue datasets to evaluate our model: the bAbI dialogue and Stanford multi-domain dialogue (SMD) On the other hand, SMD is a human-human, multi-domain dialogue dataset The key difference between these two datasets is, the former has longer dialogue turns but the regular user and system behaviors, the latter has few conversational turns but variant responses, and the KB information is much more complicated Table 3: In SMD dataset, our model achieves highest BLEU score and entity F1 score over baselines, including previous state-of-the-art result from Madotto et al. (2018). (Models with * are reported from Eric et al. (2017), where the problem is simplified to the canonicalized forms.) Human Evaluation Automatic Evaluation Human GLMP GLMP K3 GLMP K6 100 ( 100 ) including previous state - of - the - art result from Madotto et al . ( 2018 ) . ( Models with * are reported GLMP K1 Mem2Seq S2S S2S + Attn KVR * Ptr - Unk Rule - Based * Table 5: Selected hyper-parameters in each dataset for different hops. The values is the embedding dimension and the GRU hidden size, and the values between parenthesis is the dropout rate. For all the models we used learning rate equal to 0.001, with a decay rate of 0.5. 128 ( 0 .",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Task-Oriented Dialogue Systems\", \"Dataset\": \"KVRET\", \"Metric\": \"BLEU\", \"Score\": \"14.79\"}}, {\"LEADERBOARD\": {\"Task\": \"Task-Oriented Dialogue Systems\", \"Dataset\": \"KVRET\", \"Metric\": \"Entity F1\", \"Score\": \"59.97\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Task-Oriented Dialogue Systems\", \"Dataset\": \"KVRET\", \"Metric\": \"BLEU\", \"Score\": \"12.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Task-Oriented Dialogue Systems\", \"Dataset\": \"KVRET\", \"Metric\": \"Entity F1\", \"Score\": \"51.9\"}} ]"
    },
    {
        "Context": "Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification Sentiment Classification (ATSC) is a subtask of Aspect-Based Sentiment Analysis (ABSA), which has many applications e.g. in e-commerce, where data and insights from reviews can be leveraged to create value for businesses and customers. Recently, deep transfer-learning methods have been applied successfully to a myriad of Natural Language Processing (NLP) tasks, including ATSC. Building on top of the prominent BERT language model, we approach ATSC using a two-step procedure: selfsupervised domain-specific BERT language model finetuning, followed by supervised task-specific finetuning. Our findings on how to best exploit domain-specific language model finetuning enable us to produce new state-of-the-art performance on the SemEval 2014 Task 4 restaurants dataset. In addition, to explore the real-world robustness of our models, we perform cross-domain evaluation. We show that a cross-domain adapted BERT language model performs significantly better than strong baseline models like vanilla BERT-base and XLNet-base. Finally, we conduct a case study to interpret model prediction errors. In our experiments we aim to answer the following research questions (RQs): RQ1: How does the number of training iterations in the BERT language model finetuning stage influence the ATSC end-task performance? At what point does performance start to improve, when does it converge? RQ2: If trained in-domain, what ATSC endtask performance can be reached through fully exploited finetuning of the BERT language model? RQ3: If trained cross-domain in the special case of domain adaptation, what ATSC end-task performance can be reached if BERT language model finetuning is fully exploited? We conduct experiments using the two SemEval 2014 Task 4 Subtask 2 datasets 1 for the laptops and the restaurants domain The two datasets contain sentences with one or multiple marked aspect-targets that each have a 3level sentiment polarity (positive, neutral or negative) associated In the original dataset the conflict class is also present Detailed statistics for both datasets are shown Table 1. Dataset Neutral 15 10 , 000 , 000 3 Negative Positive Test 2 , 007 , 213 Sentences 1 , 007 , 209 Finetuning Epochs Train 30 Table 1: Top: Detailed statistics of the corpora for BERT language model finetuning. Bottom: Number of labels for each category of the SemEval 2014 Task 4 Subtask 2 laptop and restaurant datasets for Aspect- Target Sentiment Classification. Dataset Neutral 15 10 , 000 , 000 3 Negative Positive Test 2 , 007 , 213 Sentences 1 , 007 , 209 Finetuning Epochs Train 30 Table 2: Summary of results for Aspect-Target Sentiment Classification for in-domain, cross-domain, and joint- domain training on SemEval 2014 Task 4 Subtask 2 datasets. The cells with gray background correspond to the cross-domain adaptation case, where the language model is finetuned on the target domain. As evaluation metrics accuracy (Acc) and Macro-F1 (MF1) are used. Ours Baselines Acc",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Aspect-Based Sentiment Analysis\", \"Dataset\": \"SemEval 2014 Task 4 Sub Task 2\", \"Metric\": \"Laptop (Acc)\", \"Score\": \"80.23\"}}, {\"LEADERBOARD\": {\"Task\": \"Aspect-Based Sentiment Analysis\", \"Dataset\": \"SemEval 2014 Task 4 Sub Task 2\", \"Metric\": \"Mean Acc (Restaurant + Laptop)\", \"Score\": \"84.06\"}}, {\"LEADERBOARD\": {\"Task\": \"Aspect-Based Sentiment Analysis\", \"Dataset\": \"SemEval 2014 Task 4 Sub Task 2\", \"Metric\": \"Restaurant (Acc)\", \"Score\": \"87.89\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Aspect-Based Sentiment Analysis\", \"Dataset\": \"SemEval 2014 Task 4 Sub Task 2\", \"Metric\": \"Laptop (Acc)\", \"Score\": \"70.06\"}}, { \"LEADERBOARD\": { \"Task\": \"Aspect-Based Sentiment Analysis\", \"Dataset\": \"SemEval 2014 Task 4 Sub Task 2\", \"Metric\": \"Mean Acc (Restaurant + Laptop)\", \"Score\": \"74.63\"}}, { \"LEADERBOARD\": { \"Task\": \"Aspect-Based Sentiment Analysis\", \"Dataset\": \"SemEval 2014 Task 4 Sub Task 2\", \"Metric\": \"Restaurant (Acc)\", \"Score\": \"79.20\"}} ]"
    },
    {
        "Context": "Rewrite based Verification of XML Updates Rewrite based Verification of XML Updates Th\u00e8me : Knowledge and Data Representation and Management Equipes-Projets DAHU and CASSIS Rewrite based Verification of XML Updates We consider problems of access control for update of XML document. In the context of XML programming, types can be viewed as hedge automata, and static type checking amounts to verify that a program always converts valid source documents into also valid output documents. Given a set of update operations we are particularly interested by checking safety properties such as preservation of document types along any sequence of updates. We are also interested by the related policy consistency problem, that is detecting whether a sequence of authorized operations can simulate a forbidden one. We reduce these questions to type checking problems, solved by computing variants of hedge automata characterizing the set of ancestors and descendants of the initial document type for the closure of parameterized rewrite rules.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map Most of the existing deep learning-based methods for 3D hand and human pose estimation from a single depth map are based on a common framework that takes a 2D depth map and directly regresses the 3D coordinates of keypoints, such as hand or human body joints, via 2D convolutional neural networks (CNNs). The first weakness of this approach is the presence of perspective distortion in the 2D depth map. While the depth map is intrinsically 3D data, many previous methods treat depth maps as 2D images that can distort the shape of the actual object through projection from 3D to 2D space. This compels the network to perform perspective distortion-invariant estimation. The second weakness of the conventional approach is that directly regressing 3D coordinates from a 2D image is a highly nonlinear mapping, which causes difficulty in the learning procedure. To overcome these weaknesses, we firstly cast the 3D hand and human pose estimation problem from a single depth map into a voxel-to-voxel prediction that uses a 3D voxelized grid and estimates the per-voxel likelihood for each keypoint. We design our model as a 3D CNN that provides accurate estimates while running in real-time. Our system outperforms previous methods in almost all publicly available 3D hand and human pose estimation datasets and placed first in the HANDS 2017 frame-based 3D hand pose estimation challenge. The code is available in 1 . ICVL Hand Posture Dataset The ICVL dataset consists of 330K training and 1.6K testing depth images NYU Hand Pose Dataset The NYU dataset consists of 72K training and 8.2K testing depth images Most of the previous works only used frames from the frontal view and 14 out of 36 joints in the evaluation, and we also followed them MSRA Hand Pose Dataset The MSRA dataset [39] contains 9 subjects with 17 gestures for each subject For evaluation, the leave-one-subject-out cross-validation strategy is utilized HANDS 2017 Frame-based 3D Hand Pose Estimation Challenge Dataset The HANDS 2017 frame-based 3D hand pose estimation challenge dataset consists of 957K training and 295K testing depth images that are sampled from BigHand2.2M and First-Person Hand Action datasets The ground-truth of this dataset is the 3D coordinates of We used 3D distance error and percentage of success frame metrics for 3D hand pose estimation following Table 3: Comparison of the proposed method (V2V-PoseNet) with state-of-the-art methods on the three 3D hand pose datasets. Mean error indicates the average 3D distance error. ( c ) MSRA middle : NYU dataset , right : MSRA dataset . Top row : the percentage of success frames over different error thresholds . Left : ICVL dataset , Mean error ( mm ) Bottom row : 3D distance errors per hand keypoints . Table 5: Comparison of the proposed method (V2V-PoseNet) with state-of-the-art methods on the front and top views of the ITOP dataset. VI ( Ours ) RF RTW V2V - PoseNet mAP ( front - view ) mAP ( top - view ) IEF REN - 9x6x6",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Hand Pose Estimation\", \"Dataset\": \"NYU Hands\", \"Metric\": \"Average 3D Error\", \"Score\": \"8.42\"}}, {\"LEADERBOARD\": {\"Task\": \"Hand Pose Estimation\", \"Dataset\": \"MSRA Hands\", \"Metric\": \"Average 3D Error\", \"Score\": \"7.49\"}}, {\"LEADERBOARD\": {\"Task\": \"Hand Pose Estimation\", \"Dataset\": \"HANDS 2017\", \"Metric\": \"Average 3D Error\", \"Score\": \"9.95\"}}, {\"LEADERBOARD\": {\"Task\": \"Hand Pose Estimation\", \"Dataset\": \"ICVL Hands\", \"Metric\": \"Average 3D Error\", \"Score\": \"6.28\"}}, {\"LEADERBOARD\": {\"Task\": \"Pose Estimation\", \"Dataset\": \"ITOP top-view\", \"Metric\": \"Mean mAP\", \"Score\": \"83.44\"}}, {\"LEADERBOARD\": {\"Task\": \"Pose Estimation\", \"Dataset\": \"ITOP front-view\", \"Metric\": \"Mean mAP\", \"Score\": \"88.74\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Hand Pose Estimation\", \"Dataset\": \"NYU Hands\", \"Metric\": \"Average 3D Error\", \"Score\": \"11.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Hand Pose Estimation\", \"Dataset\": \"MSRA Hands\", \"Metric\": \"Average 3D Error\", \"Score\": \"8.6\"}}, { \"LEADERBOARD\": { \"Task\": \"Hand Pose Estimation\", \"Dataset\": \"HANDS 2017\", \"Metric\": \"Average 3D Error\", \"Score\": \"11.70\"}}, { \"LEADERBOARD\": { \"Task\": \"Hand Pose Estimation\", \"Dataset\": \"ICVL Hands\", \"Metric\": \"Average 3D Error\", \"Score\": \"6.8\"}} ]"
    },
    {
        "Context": "IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 PAC-Bayesian Meta-learning with Implicit Prior and Posterior We introduce anew and rigorously-formulated PAC-Bayes few-shot meta-learning algorithm that implicitly learns a prior distribution of the model of interest. Our proposed method extends the PAC-Bayes framework from a single task setting to the few-shot learning setting to upper-bound generalisation errors on unseen tasks and samples. We also propose a generative-based approach to model the shared prior and the posterior of task-specific model parameters more expressively compared to the usual diagonal Gaussian assumption. We show that the models trained with our proposed meta-learning algorithm are well calibrated and accurate, with state-of-the-art calibration and classification results on few-shot classification (mini-ImageNet and tiered-ImageNet) and regression (multi-modal task-distribution regression) benchmarks. We evaluate SImPa on few-shot regression and classification problems Mini - ImageNet [ 37 ] - non - standard network Tiered - ImageNet [ 38 ] Omniglot [ 21 ] - standard 4 - block CNN Mini - ImageNet [ 37 ] - standard 4 - block CNN 5 - SHOT 1 - SHOT \u2212 5 \u2212 4 \u2212 2 Non - standard 2 1024 Notation network",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"Tiered ImageNet 5-way (1-shot)\", \"Metric\": \"Accuracy\", \"Score\": \"70.82\"}}, {\"LEADERBOARD\": {\"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"Tiered ImageNet 5-way (5-shot)\", \"Metric\": \"Accuracy\", \"Score\": \"81.84\"}}, {\"LEADERBOARD\": {\"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"Mini-Imagenet 5-way (1-shot)\", \"Metric\": \"Accuracy\", \"Score\": \"52.11\"}}, {\"LEADERBOARD\": {\"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"Mini-Imagenet 5-way (5-shot)\", \"Metric\": \"Accuracy\", \"Score\": \"63.87\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"Mini-Imagenet 5-way (1-shot)\", \"Metric\": \"Accuracy\", \"Score\": \"71.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"Mini-Imagenet 5-way (5-shot)\", \"Metric\": \"Accuracy\", \"Score\": \"82.2\"}} ]"
    },
    {
        "Context": "CU-Net: Coupled U-Nets We design anew connectivity pattern for the U-Net architecture. Given several stacked U-Nets, we couple each U-Net pair through the connections of their semantic blocks, resulting in the coupled U-Nets (CU-Net). The coupling connections could make the information flow more efficiently across U-Nets. The feature reuse across U-Nets makes each U-Net very parameter efficient. We evaluate the coupled U-Nets on two benchmark datasets of human pose estimation. Both the accuracy and model parameter number are compared. The CU-Net obtains comparable accuracy as state-of-the-art methods. However, it only has at least 60% fewer parameters than other approaches. Datasets For human pose estimation, we use benchmark datasets: MPII Human Pose and Leeds Sports Pose (LSP) Table 1: Comparison of different hyper-parameters m and n measured by the model parameter number and the PCKh on the MPII validation set. The PCKh increase becomes less from the left to the right while the parameter number growly consistently. A good trade-off between the PCKh and parameter number is m=128 and n=32. m 64 128 128 128 192 192 n 16 16 24 32 24 32 # Parameters 0.5M 1.0M 1.4M 1.9M 2.4M 2.9M PCKh@0.5 (%) 81.6 84.2 85.6 86.0 86.3 86.6 the PCKh and parameter number is m=128 and n=32 . . CU - Net - 2 24 16 CU - Net - 4 Table 1 : Comparison of different hyper - parameters m and n measured by the model parameter 192 PCKh of CU - Net - 2 . m n 1 2 3 4 128 However , it improves the PCKh of deeper networks CU - Net",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Pose Estimation\", \"Dataset\": \"MPII Human Pose\", \"Metric\": \"PCKh-0.5\", \"Score\": \"89.4%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Pose Estimation\", \"Dataset\": \"MPII Human Pose\", \"Metric\": \"PCKh-0.5\", \"Score\": \"91.9\"}} ]"
    },
    {
        "Context": "Deeply-Learned Part-Aligned Representations for Person Re-Identification In this paper, we address the problem of person reidentification, which refers to associating the persons captured from different cameras. We propose a simple yet effective human part-aligned representation for handling the body part misalignment problem. Our approach decomposes the human body into regions (parts) which are discriminative for person matching, accordingly computes the representations over the regions, and aggregates the similarities computed between the corresponding regions of a pair of probe and gallery images as the overall matching score. Our formulation, inspired by attention models, is a deep neural network modeling the three steps together, which is learnt through minimizing the triplet loss function without requiring body part labeling information. Unlike most existing deep learning algorithms that learn a global or spatial partition-based local representation, our approach performs human body partition, and thus is more robust to pose changes and various human spatial distributions in the person bounding box. Our approach shows state-of-the-art results over standard datasets, Market-1501, CUHK03, CUHK01 and VIPeR. 1 This dataset is one of the largest benchmark datasets for person re-identification This dataset consists of 13, 164 images of 1, 360 persons, captured by six cameras This dataset contains 971 identities captured from two camera views in the same campus with CUHK03 This dataset contains two views of 632 persons We adopt the widely-used evaluation protocol The performances are evaluated by the cumulated matching characteristics (CMC) curves, which is an estimate of the expectation of finding the correct match in the top n matches Table 1. The performance (%) of our approach and spatial partition based methods (stripe and grid) over Market-1501 and CUHK03. rank - 1 rank - 10 rank - 20 rank - 5 Table 2. The performance of our approach, and separate part seg- mentation over Market-1501 and CUHK-03. rank - 1 rank - 10 rank - 20 rank - 5 Table 3. The validation performance with different numbers (K) of parts over CUHK03. The model is trained over a random half of the training data, and the performance is reported over the re- maining half (as the validation set). The best results are in bold. #parts rank-1 rank-5 rank-10 rank-20 maining half ( as the validation set ) . The best results are in bold . tation over Market - 1501 and CUHK03 . rank - 1 rank - 10 rank - 20 rank - 5 Table 3 . The validation",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Person Re-Identification\", \"Dataset\": \"Market-1501\", \"Metric\": \"MAP\", \"Score\": \"63.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Person Re-Identification\", \"Dataset\": \"Market-1501\", \"Metric\": \"Rank-1\", \"Score\": \"81.0\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Person Re-Identification\", \"Dataset\": \"Market-1501\", \"Metric\": \"MAP\", \"Score\": \"83.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Person Re-Identification\", \"Dataset\": \"Market-1501\", \"Metric\": \"Rank-1\", \"Score\": \"93.6\"}} ]"
    },
    {
        "Context": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning We introduce Bootstrap Your Own Latent (BYOL), anew approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves anew state of the art without them. BYOL reaches 74.3% top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and 79.6% with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub. 3 * Equal contribution; the order of first authors was randomly selected. We assess the performance of BYOL's representation after self-supervised pretraining on the training set of the ImageNet ILSVRC-2012 dataset We first evaluate it on ImageNet (IN) in both linear evaluation and semisupervised setups We then measure its transfer capabilities on other datasets and tasks, including classification, segmentation, object detection and depth estimation In Appendix E, we assess the generality of BYOL by pretraining a representation on the Places365-Standard dataset before reproducing this evaluation protocol Linear evaluation on ImageNet We first evaluate BYOL's representation by training a linear classifier on top of the frozen representation, following the procedure described in, and appendix C.1; we report top-1 and top-5 accuracies in % on the test set in Semi-supervised training on ImageNet Next, we evaluate the performance obtained when fine-tuning BYOL's representation on a classification task with a small subset of ImageNet's train set, this time using label information Transfer to other classification tasks Table 1: Top-1 and top-5 accuracies (in %) under linear evaluation on ImageNet. - Top - 5 Top - 1 Table 2. BYOL consistently outperforms previous approaches across a wide range of architectures. Additionally, as detailed in Appendix C.1, BYOL reaches 77.7% top-1 accuracy with ResNet-50 when fine-tuning over 100% of ImageNet labels. 1% 10% Top - 5 Top - 1 Table 2: Semi-supervised training with a fraction of ImageNet labels. 1% 10% Top - 5 Top - 1 Table 3: Transfer learning results from ImageNet (IN) with the standard ResNet-50 architecture. Linear evaluation : Fine - tuned : Cars SUN397 VOC2007 Pets CIFAR100 DTD Birdsnap Flowers Food101 CIFAR10 Aircraft Caltech - 101 Table 4: Results on transferring BYOL's representation to other vision tasks. mIoU 2 3 AP50 Lower better rel rms Higher better Table 5: Ablations with top-1 accuracy (in %) at 300 epochs under linear evaluation on ImageNet. 18",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 1% labeled data\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"71.2%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 1% labeled data\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"89.5%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 1% labeled data\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"69.1%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 1% labeled data\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"87.9%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 1% labeled data\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"62.2%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 1% labeled data\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"84.1%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 1% labeled data\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"53.2%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 1% labeled data\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"78.4%\"}}, {\"LEADERBOARD\": {\"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of Params\", \"Score\": \"250M\"}}, {\"LEADERBOARD\": {\"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"79.6%\"}}, {\"LEADERBOARD\": {\"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"94.8%\"}}, {\"LEADERBOARD\": {\"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of Params\", \"Score\": \"375M\"}}, {\"LEADERBOARD\": {\"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"78.6%\"}}, {\"LEADERBOARD\": {\"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"94.2%\"}}, {\"LEADERBOARD\": {\"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of Params\", \"Score\": \"94M\"}}, {\"LEADERBOARD\": {\"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"77.4%\"}}, {\"LEADERBOARD\": {\"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"93.6%\"}}, {\"LEADERBOARD\": {\"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"74.3%\"}}, {\"LEADERBOARD\": {\"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"91.6%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 1% labeled data\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"74.96%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"ImageNet - 1% labeled data\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"82.21%\"}}, { \"LEADERBOARD\": { \"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"7.8%\"}}, { \"LEADERBOARD\": { \"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"84.58%\"}} ]"
    },
    {
        "Context": "UVid-Net: Enhanced Semantic Segmentation of UAV Aerial Videos by Embedding Temporal Information Semantic segmentation of aerial videos has been extensively used for decision making in monitoring environmental changes, urban planning, and disaster management. The reliability of these decision support systems is dependent on the accuracy of the video semantic segmentation algorithms. The existing CNN based video semantic segmentation methods have enhanced the image semantic segmentation methods by incorporating an additional module such as LSTM or optical flow for computing temporal dynamics of the video which is a computational overhead. The proposed research work modifies the CNN architecture by incorporating temporal information to improve the efficiency of video semantic segmentation.In this work, an enhanced encoder-decoder based CNN architecture (UVid-Net) is proposed for UAV video semantic segmentation. The encoder of the proposed architecture embeds temporal information for temporally consistent labelling. The decoder is enhanced by introducing the feature retainer module, which aids in accurate localization of the class labels. The proposed UVid-Net architecture for UAV video semantic segmentation is quantitatively evaluated on extended ManipalUAVid dataset. The performance metric mIoU of 0.79 has been observed which is significantly greater than the other state-of-the-art algorithms. Further, the proposed work produced promising results even for the pre-trained model of UVid-Net on urban street scene with fine tuning the final layer on UAV aerial videos. This paper presents an extended version of ManipalUAVid dataset for semantic segmentation of UAV videos This extended dataset consists of new videos captured at additional locations The extended dataset consists of 37 videos with annotations provided for 711 keyframes More details of this dataset can be found in Besides, the performance of semantic segmentation algorithms which analyses each keyframe individually was provided in on the ManipalUAVid dataset The earlier version of ManipalUAVid dataset consists of last two keyframes of each video in the test split which might not be sufficient to observe the temporal smoothness or the error (if any) accumulated over the period of time in the video Therefore, in this work, ManipalUAVid is extended by incorporating four new videos (total key frames: 44) which are entirely in the test split Besides, the training-test split distribution is slightly modified so that a greater number of frames (4-5 frames) per video",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"ManipalUAVid\", \"Metric\": \"mIoU\", \"Score\": \"0.79\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\":\"scapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"78.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL Context\", \"Metric\":\"mIoU\", \"Score\": \"42.3\"}} ]"
    },
    {
        "Context": "Multiscale Vision Transformers We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals fora variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10\u00d7 more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: https: //github.com/facebookresearch/SlowFast. Datasets Table 1. Vision Transformers (ViT) base model starts from a data layer that samples visual input with rate \u03c4 \u00d71\u00d71 to T \u00d7H\u00d7W resolution, where T is the number of frames H height and W width. The first layer, patch1 projects patches (of shape 1\u00d716\u00d716) to form a sequence, processed by a stack of N transformer blocks (stage2) at uniform channel dimension (D) and resolution (T \u00d7 H 16 \u00d7 W 16 ). a sequence , processed by a stack of N transformer blocks ( stage2 ) MHA ( D ) 1\u00d716\u00d716 , D T \u00d7H\u00d7W output sizes D\u00d7T \u00d7 H \u00d7 W Table 3. Comparing ViT-B to two instantiations of MViT with varying complexity, MViT-S in (c) and MViT-B in (b). MViT-S operates at a lower spatial resolution and lacks a first high-resolution stage. The top-1 accuracy corresponds to 5-Center view testing on K400. FLOPs correspond to a single inference",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@1\", \"Score\": \"81.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@5\", \"Score\": \"95.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@1\", \"Score\": \"80.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@5\", \"Score\": \"94.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@1\", \"Score\": \"78.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@5\", \"Score\": \"93.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@1\", \"Score\": \"76\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@5\", \"Score\": \"92.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Charades\", \"Metric\": \"MAP\", \"Score\": \"47.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Charades\", \"Metric\": \"MAP\", \"Score\": \"47.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Charades\", \"Metric\": \"MAP\", \"Score\": \"46.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Charades\", \"Metric\": \"MAP\", \"Score\": \"44.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Charades\", \"Metric\": \"MAP\", \"Score\": \"43.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Charades\", \"Metric\": \"MAP\", \"Score\": \"40\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-600\", \"Metric\": \"Vid acc@1\", \"Score\": \"83.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-600\", \"Metric\": \"Vid acc@5\", \"Score\": \"96.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-600\", \"Metric\": \"Vid acc@1\", \"Score\": \"83.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-600\", \"Metric\": \"Vid acc@1\", \"Score\": \"82.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-600\", \"Metric\": \"Vid acc@5\", \"Score\": \"95.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"AVA v2.2\", \"Metric\": \"mAP\", \"Score\": \"28.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"AVA v2.2\", \"Metric\": \"mAP\", \"Score\": \"27.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"AVA v2.2\", \"Metric\": \"mAP\", \"Score\": \"27.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"AVA v2.2\", \"Metric\": \"mAP\", \"Score\": \"26.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"AVA v2.2\", \"Metric\": \"mAP\", \"Score\": \"26.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"AVA v2.2\", \"Metric\": \"mAP\", \"Score\": \"24.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"Something-Something V2\", \"Metric\": \"Top-1 Accuracy\", \"Score\": \"68.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"Something-Something V2\", \"Metric\": \"Top-5 Accuracy\", \"Score\": \"91.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"Something-Something V2\", \"Metric\": \"Top-1 Accuracy\", \"Score\": \"67.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"Something-Something V2\", \"Metric\": \"Top-5 Accuracy\", \"Score\": \"91.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"Something-Something V2\", \"Metric\": \"Top-1 Accuracy\", \"Score\": \"66.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"Something-Something V2\", \"Metric\": \"Top-5 Accuracy\", \"Score\": \"90.2\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Action Detection\", \"Dataset\": \"ViTube\", \"Metric\": \"F1-score\", \"Score\": \"94.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Detection\", \"Dataset\": \"ViTube\", \"Metric\": \"F1-score\", \"Score\": \"95.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Detection\", \"Dataset\": \"UCF101\", \"Metric\": \"F1-score\", \"Score\": \"94.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Detection\", \"Dataset\": \"UCF101\", \"Metric\": \"F1-score\", \"Score\": \"93.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Detection\", \"Dataset\": \"Middlebury\", \"Metric\": \"F1-score\", \"Score\": \"0.44\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Detection\", \"Dataset\": \"Middlebury\", \"Metric\": \"Vid acc@1\", \"Score\": \"82.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Detection\", \"Dataset\": \"Middlebury\", \"Metric\": \"Vid"
    },
    {
        "Context": "An Effective Domain Adaptive Post-Training Method for BERT in Response Selection We focus on multi-turn response selection in a retrieval-based dialog system. In this paper, we utilize the powerful pre-trained language model Bi-directional Encoder Representations from Transformer (BERT) fora multi-turn dialog system and propose a highly effective post-training method on domain-specific corpus. Although BERT is easily adopted to various NLP tasks and outperforms previous baselines of each task, it still has limitations if a task corpus is too focused on a certain domain. Posttraining on domain-specific corpus (e.g., Ubuntu Corpus) helps the model to train contextualized representations and words that do not appear in general corpus (e.g., English Wikipedia). Experimental results show that our approach achieves new stateof-the-art on two response selection benchmarks (i.e., Ubuntu Corpus V1, Advising Corpus) performance improvement by 5.9% and 6% on R10@1. We evaluate our model on two multi-turn dyadic data sets, Ubuntu IRC (Internet Relay Chat) Corpus V1 and Advising Corpus 1 For the Ubuntu Corpus, training set is composed of 0.5M dialog context containing positive and negative response with the ratio of 1:1 Advising Corpus consists of 100k dialogs for training set and 500 for validation and test set We only use one negative sample for training to make same conditions with Ubuntu Corpus For an evaluation metric, we use Rn@k, evaluating if the ground truth exists in top k from n candidates We also use another evaluation metric mean reciprocal rank (MRR) 10 @1 10 @2 10 @5 Table 2: Model comparison on Ubuntu Corpus V1. 10 @1 10 @2 10 @5 Table 2. Dual Encoder is simple dialogue-response matching model based on RNN, CNN, LSTM, and BiLSTM 10 @1 10 @2 10 @5 Table 3: Evaluation results on the Advising Corpus. improvements . Ubuntu effective on not only small sets but also domain - specific sets . Variable Fine - Tuning Advising Variable for reducing Table 5: Comparison of MLM and NSP on Ubuntu Corpus V1. Experiments are conducted depending on the use of [EOT]. with EOT without",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Conversational Response Selection\", \"Dataset\": \"Ubuntu Dialogue (v1, Ranking)\", \"Metric\": \"R10@1\", \"Score\": \"0.855\"}}, {\"LEADERBOARD\": {\"Task\": \"Conversational Response Selection\", \"Dataset\": \"Ubuntu Dialogue (v1, Ranking)\", \"Metric\": \"R10@2\", \"Score\": \"0.928\"}}, {\"LEADERBOARD\": {\"Task\": \"Conversational Response Selection\", \"Dataset\": \"Ubuntu Dialogue (v1, Ranking)\", \"Metric\": \"R10@5\", \"Score\": \"0.985\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Conversational Response Selection\", \"Dataset\": \"Ubuntu Dialogue (v1, Ranking)\", \"Metric\": \"R10@1\", \"Score\": \"0.726\"}}, { \"LEADERBOARD\": { \"Task\": \"Conversational Response Selection\", \"Dataset\": \"Ubuntu Dialogue (v1, Ranking)\", \"Metric\": \"R10@2\", \"Score\": \"0.822\"}}, { \"LEADERBOARD\": { \"Task\": \"Conversational Response Selection\", \"Dataset\": \"Ubuntu Dialogue (v1, Ranking)\", \"Metric\": \"R10@5\", \"Score\": \"0.974\"}}, { \"LEADERBOARD\": { \"Task\": \"Conversational Response Selection\", \"Dataset\": \"Ubuntu Dialogue (v1, Ranking)\", \"Metric\": \"R2@1\", \"Score\": \"0.926\"}} ]"
    },
    {
        "Context": "Re-verification of a Lip Synchronization Protocol using Robust Reachability The timed automata formalism is an important model for specifying and analysing real-time systems. Robustness is the correctness of the model in the presence of small drifts on clocks or imprecision in testing guards. A symbolic algorithm for the analysis of the robustness of timed automata has been implemented. In this paper, we re-analyse an industrial case lip synchronization protocol using the new robust reachability algorithm. This lip synchronization protocol is an interesting case because timing aspects are crucial for the correctness of the protocol. Several versions of the model are considered: with an ideal video stream, with anchored jitter, and with non-anchored jitter.  Table 1: Verification results for streams with possible initial delay for both normal and robust semantics (marked with *) Anchored Ideal Non - anchored Table 2: Verification results for streams without initial delay for both normal and robust semantics (marked with *) Anchored Ideal Non - anchored",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Efficient MIMO-OFDM Schemes for Future Terrestrial Digital TV with Unequal Received Powers Golden code offers the highest robustness to power unbalance at the receiving side.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Tr\u00e4umerAI: Dreaming Music with StyleGAN",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Music Auto-Tagging\", \"Dataset\": \"TimeTravel\", \"Metric\": \"0..5sec\", \"Score\": \"5\"}}]",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Simulation of Coating -Visco-Elastic liquid in the Mico-Nip of Metering Size Press fora set of operating conditions and coating color formulations, undesirable phenomena like color spitting and coating ribs maybe triggered in the Micro-nip during the coating process. Therefore, our interest in this work focus on another parameter affect on the undesirable phenomena as the vortices in the Micro-nip. The problem deals with the flow through the Micro-nip of metering size press. The flow enters and exits at a tangential velocity of 20 m/s between two rollers with diameter 80 cm and 60 m apart. In the upper and bottom part of the domain the angular velocity is 314 rad /s. It has one sub-domain. Previous studies focus on the Micro-nip without considering the inertia and the viscoelasticity of the material. Roll coating is a technique commonly used in the coating industry to meter a thin fluid film on a moving substrate. During the film formation, the fluid is subjected to very high shear and extensional rates over a very short period of time. The fluid domain changes as a function of the hydrodynamic pressure within the nip as a result of the deformable cover usually used on one of the rolls. The free surface also adds more complexity to the flow due to the force equilibrium in the fluid gas interface. Last of all, the rheological behavior of the coating fluid is usually non-Newtonian, so the metering flow hydrodynamics is finally very difficult to describe. It is concluded that the normal forces of micro-nip increases with increasing the inhibitors. Therefore, it affects on the smoothness and creates defects. On the other hand, it can be concluded that the creation of big vortex in the middle of micro-nip affects on the coating liquid behavior.I.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Findings of the E2E NLG Challenge This paper summarises the experimental setup and results of the first shared task on end-to-end (E2E) natural language generation (NLG) in spoken dialogue systems. Recent end-to-end generation systems are promising since they reduce the need for data annotation. However, they are currently limited to small, delexicalised datasets. The E2E NLG shared task aims to assess whether these novel approaches can generate better-quality output by learning from a dataset containing higher lexical richness, syntactic complexity and diverse discourse phenomena. We compare 62 systems submitted by 17 institutions, covering a wide range of approaches, including machine learning architectures -with the majority implementing sequence-to-sequence models (seq2seq) -as well as systems based on grammatical rules and templates. Therefore, we decided to use pictorial MRs to collect 20% of the dataset Our crowd workers were asked to verbalise all information from the MR; however, they were not However, the human evaluation study provides a different picture Rank-based Magnitude Estimation (RankME) was used for evaluation, where crowd workers compared outputs of 5 systems for the same MR and assigned scores on a continuous scale We evaluated output naturalness and overall quality in separate tasks; for naturalness evaluation, the source MR was not shown to workers The final evaluation results were produced using the TrueSkill algorithm, with partial ordering into significance clusters computed using bootstrap resampling Table 2: Total number of MRs and human refer- ences in the E2E data sections. 4 , 672 4 , 862 MRs Table 3: A list of primary systems in the E2E NLG challenge, with word-overlap metric scores. decoder ) , heuristic slot aligner reranking , data augmentation classification reranking memory \u2663 \u2665 model ( Wen et al . , 2015b ) + controlling the first generated word \u2666 coverage penalty reranking , diverse ensembling BLEU NIST norm . avg . METEOR ROUGE - L CIDEr Table 4: TrueSkill measurements of quality (left) and naturalness (right). 3 4 \u2660 5 - 8 \u2663 10 - 12 \u2665 \u2666 13 - 16 9 - 12 13 - 17 3 - 6 18 - 19 20 - 21 1 - 1 7 - 11 15 - 17 4 - 8 TrueSkill 5 - 10 2 - 4 2 - 3",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Data-to-Text Generation\", \"Dataset\": \"E2E NLG Challenge\", \"Metric\": \"BLEU\", \"Score\": \"65.93\"}}, {\"LEADERBOARD\": {\"Task\": \"Data-to-Text Generation\", \"Dataset\": \"E2E NLG Challenge\", \"Metric\": \"CIDEr\", \"Score\": \"2.2338\"}}, {\"LEADERBOARD\": {\"Task\": \"Data-to-Text Generation\", \"Dataset\": \"E2E NLG Challenge\", \"Metric\": \"METEOR\", \"Score\": \"44.83\"}}, {\"LEADERBOARD\": {\"Task\": \"Data-to-Text Generation\", \"Dataset\": \"E2E NLG Challenge\", \"Metric\": \"NIST\", \"Score\": \"8.6094\"}}, {\"LEADERBOARD\": {\"Task\": \"Data-to-Text Generation\", \"Dataset\": \"E2E NLG Challenge\", \"Metric\": \"ROUGE-L\", \"Score\": \"68.50\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Spoken language identification\", \"Dataset\": \"E2E NLG Challenge\", \"Metric\": \"BLEU\", \"Score\": \"26.29\"}}, { \"LEADERBOARD\": { \"Task\": \"Spoken language identification\", \"Dataset\": \"E2E NLG Challenge\", \"Metric\": \"METEOR\", \"Score\": \"23.15\"}}, { \"LEADERBOARD\": { \"Task\": \"Spoken language identification\", \"Dataset\": \"E2E NLG Challenge\", \"Metric\": \"ROUGE-L\", \"Score\": \"27.68\"}} ]"
    },
    {
        "Context": "Question Directed Graph Attention Network for Numerical Reasoning over Text Numerical reasoning over texts, such as addition, subtraction, sorting and counting, is a challenging machine reading comprehension task, since it requires both natural language understanding and arithmetic computation. To address this challenge, we propose a heterogeneous graph representation for the context of the passage and question needed for such reasoning, and design a question directed graph attention network to drive multi-step numerical reasoning over this context graph. Our model, which combines deep learning and graph reasoning, achieves remarkable results in benchmark datasets such as DROP. We performed experiments on the DROP dataset, which was recently released for research on numerical machine reading comprehension (MRC) Following the previous work, we used Exact Match (EM) and F1 score as the evaluation metrics Table 2: Overall results on the development and test set of DROP. For QDGAT p , we used more careful data pre- processing and a RoBERTa pre-trained on the SQuaD dataset. \u2020 denotes that the result is taken from the public leaderboard. Better results are in bold. Method Dev Test EM F1 Table 3: Ablation study results on the development set of DROP. QDGAT NH removes the number type and entity from the graph, and QDGAT NQ removes question direction from QDGAT. Better results are in bold. EM F1 Table 4: Decomposed performance on different answer types in the development set of DROP. Better results are in bold. Method Number EM F1 Span Date Table 5: The cases from the DROP dataset. The predictions from the QDGAT and NumNet+ are illustrated. The differences between the output of these two models demonstrate the properties of the proposed model. The last two columns",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Question Answering\", \"Dataset\": \"DROP Test\", \"Metric\": \"F1\", \"Score\": \"88.38\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Question Answering\", \"Dataset\": \"DROP Test\", \"Metric\": \"F1\", \"Score\": \"86.1\"}} ]"
    },
    {
        "Context": "A formal proof of the four color theorem A formal proof has not been found for the four color theorem since 1852 when Francis Guthrie first conjectured the four color theorem. Why? A bad idea, we think, directed people to a rough road. Using a similar method to that for the formal proof of the five color theorem, a formal proof is proposed in this paper of the four color theorem, namely, every planar graph is four-colorable. The formal proof proposed can also be regarded as an algorithm to color a planar graph using four colors so that no two adjacent vertices receive the same color.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection A unified deep neural network, denoted the multi-scale CNN (MS-CNN), is proposed for fast multi-scale object detection. The MS-CNN consists of a proposal sub-network and a detection sub-network. In the proposal sub-network, detection is performed at multiple output layers, so that receptive fields match objects of different scales. These complementary scale-specific detectors are combined to produce a strong multi-scale object detector. The unified network is learned end-to-end, by optimizing a multi-task loss. Feature upsampling by deconvolution is also explored, as an alternative to input upsampling, to reduce the memory and computation costs. State-of-the-art object detection performance, at up to 15 fps, is reported on datasets, such as KITTI and Caltech, containing a substantial number of small objects. The performance of the MS-CNN detector was evaluated on the KITTI and Caltech Pedestrian benchmarks KITTI contains three object classes: car, pedestrian and cyclist, and three levels of evaluation: easy, moderate and hard In all ablation experiments, the training set was used for learning and the validation set for evaluation We start with an evaluation of the proposal network In this section we evaluate object detection performance Pedestrian detection on Caltech The MS-CNN detector was also evaluated on the Caltech pedestrian benchmark Table 2. Detection recall of the various detection layers on KITTI validation set (car), as a function of object hight in pixels. det - 16 det - 8 det - 64 det - 32 combined Table 3. Results on the KITTI validation set. \"hXXX\" indicates an input of height \"XXX\", \"2x\" deconvolution, \"ctx\" context encoding, and \"c\" dimensionality reduction convolution. In columns \"Time\" and \"# params\", entries before the \"/\" are for car model and after for pedestrian/cyclist model. 1 # candidates SS Cars Easy Pedestrians Mod IoU overlap threshold Pedestrian Hard IoU MCG 0 1 at recall # params EB BING Table 4. Results on the KITTI benchmark test set (only published works shown). 1 Easy KITTI Car ( moderate ) 3DOP Time SDP+RPN Faster \u2212 RCNN KITTI Cyclist ( moderate ) CompACT \u2212 Deep recall AOG MS \u2212 CNN Cars Pedestrians KITTI Pedestrian ( moderate ) Mod lSVM \u2212",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Face Detection\", \"Dataset\": \"WIDER Face (Hard)\", \"Metric\": \"AP\", \"Score\": \"0.809\"}}, {\"LEADERBOARD\": {\"Task\": \"Pedestrian Detection\", \"Dataset\": \"Caltech\", \"Metric\": \"Reasonable Miss Rate\", \"Score\": \"9.95\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Pedestrian Detection\", \"Dataset\": \"Caltech\", \"Metric\": \"Reasonable Miss Rate\", \"Score\": \"12.4\"}} ]"
    },
    {
        "Context": "Bridging the Gap between Community and Node Representations: Graph Embedding via Community Detection Graph embedding has become a key component of many data mining and analysis systems. Current graph embedding approaches either sample a large number of node pairs from a graph to learn node embeddings via stochastic optimization or factorize a high-order node proximity/adjacency matrix via computationally intensive matrix factorization techniques. These approaches typically require significant resources for the learning process and rely on multiple parameters, which limits their applicability in practice. Moreover, most of the existing graph embedding techniques operate effectively in one specific metric space only (e.g., the one produced with cosine similarity), do not preserve higher-order structural features of the input graph and cannot automatically determine a meaningful number of dimensions for the embedding space. Typically, the produced embeddings are not easily interpretable, which complicates further analyses and limits their applicability. To address these issues, we propose DAOR, a highly efficient and parameter-free graph embedding technique producing metric space-robust, compact and interpretable embeddings without any manual tuning. Compared to a dozen state-of-the-art graph embedding algorithms, DAOR yields competitive results on both node classification (which benefits form high-order proximity) and link prediction (which relies on low-order proximity mostly). Unlike existing techniques, however, DAOR does not require any parameter tuning and improves the embeddings generation speed by several orders of magnitude. Our approach has hence the ambition to greatly simplify and speedup data analysis tasks involving graph representation learning.Index Terms-parameter-free graph embedding, unsupervised learning of network representation, automatic feature extraction, interpretable embeddings, scalable graph embedding. This graph is used only to evaluate the efficiency of the embedding techniques, since the ground-truth categories include only 3% of the graph (as opposed to a 100% coverage for the other graphs) For NetHash, as suggested by the authors, \u00b7 the algorithm meta parameters are tuned once for all datasets to maximize accuracy : the algorithm parameters are tuned for each dataset to maximize accuracy we search the optimal tree depth in {1, 2, 3}",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"DBLP\", \"Metric\": \"Macro F1\", \"Score\": \"87.64\"}}, {\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"DBLP\", \"Metric\": \"Micro F1\", \"Score\": \"87.86\"}}, {\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"BlogCatalog\", \"Metric\": \"Macro F1\", \"Score\": \"17.25\"}}, {\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"BlogCatalog\", \"Metric\": \"Micro F1\", \"Score\": \"33.05\"}}, {\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"Wiki\", \"Metric\": \"Macro F1\", \"Score\": \"15.97\"}}, {\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"Wiki\", \"Metric\": \"Micro F1\", \"Score\": \"53.24\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Citeseer\", \"Metric\": \"Accuracy\", \"Score\": \"75.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Pubmed\", \"Metric\": \"Accuracy\", \"Score\": \"80.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Cora\", \"Metric\": \"Accuracy\", \"Score\": \"86.80%\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"YouTube\", \"Metric\": \"Macro-F1@2%\", \"Score\": \"30.77\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"YouTube\", \"Metric\": \"Micro-F1@2%\", \"Score\": \"38.59\"}} ]"
    },
    {
        "Context": "ECN verbose mode: a statistical method for network path congestion estimation This article introduces a simple and effective methodology to determine the level of congestion in a network with an ECN-like marking scheme. The purpose of the ECN bit is to notify TCP sources of an imminent congestion in order to react before losses occur. However, ECN is a binary indicator which does not reflect the congestion level (i.e. the percentage of queued packets) of the bottleneck, thus preventing any adapted reaction. In this study, we use a counter in place of the traditional ECN marking scheme to assess the number of times a packet has crossed a congested router. Thanks to this simple counter, we drive a statistical analysis to accurately estimate the congestion level of each router on a network path. We detail in this paper an analytical method validated by some preliminary simulations which demonstrate the feasibility and the accuracy of the concept proposed. We conclude this paper with possible applications and expected future work.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains Tracking the 6D pose of objects in video sequences is important for robot manipulation. This task, however, introduces multiple challenges: (i) robot manipulation involves significant occlusions; (ii) data and annotations are troublesome and difficult to collect for 6D poses, which complicates machine learning solutions, and (iii) incremental error drift often accumulates in long term tracking to necessitate re-initialization of the object's pose. This work proposes a data-driven optimization approach for long-term, 6D pose tracking. It aims to identify the optimal relative pose given the current RGB-D observation and a synthetic image conditioned on the previous best estimate and the object's model. The key contribution in this context is a novel neural network architecture, which appropriately disentangles the feature encoding to help reduce domain shift, and an effective 3D orientation representation via Lie Algebra. Consequently, even when the network is trained only with synthetic data can work effectively over real images. Comprehensive experiments over benchmarks -existing ones as well as anew dataset with significant occlusions related to object manipulation -show that the proposed approach achieves consistently robust estimates and outperforms alternatives, even though they have been trained with real images. The approach is also the most computationally efficient among the alternatives and achieves a tracking frequency of 90.9Hz. 1 This section evaluates the proposed approach and compares against state-of-the-art 6D pose tracking methods as well as single-image pose estimation methods on a public benchmark The evaluation closely follows the protocols adopted in comparison methods,,, and reports the AUC (Area Under Curve) results on the keyframes in 12 video test sequences evaluated by the metrics of ADD = 1 m \u2211 x\u2208M ||Rx + T \u2212 (Rx + T )|| which performs exact model matching, and ADD- designed for evaluating symmetric objects, of which the matching between points can be ambiguous for some views Although this dataset contains pose annotated training and validation data collected in real world, the proposed approach does not use any of them but is trained only on synthetic data generated by aforementioned pipeline YCBInEOAT Dataset There have been several public benchmarks, where videos are collected by placing the objects statically on a table-top while a camera",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"6D Pose Estimation\", \"Dataset\": \"YCB-Video\", \"Metric\": \"ADDS AUC\", \"Score\": \"93.3\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Multi-Object Tracking\", \"Dataset\": \"TrackingNet\", \"Metric\": \"Accuracy\", \"Score\": \"74.0\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Multi-Object Tracking\", \"Dataset\": \"TrackingNet\", \"Metric\": \"Normalized Precision\", \"Score\": \"80.1\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Multi-Object Tracking\", \"Dataset\": \"TrackingNet\", \"Metric\": \"Precision\", \"Score\": \"68.7\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Multi-Object Tracking\", \"Dataset\": \"LaSOT\", \"Metric\": \"AUC\", \"Score\": \"56.8\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Multi-Object Tracking\", \"Dataset\": \"GOT-10k\", \"Metric\": \"Average Overlap\", \"Score\": \"65.6\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Multi-Object Tracking\", \"Dataset\": \"GOT-10k\", \"Metric\": \"Success Rate 0.5\", \"Score\": \"72.8\"}} ]"
    },
    {
        "Context": "AXM-Net: Cross-Modal Alignment and Contextual Attention for Person Re-ID Cross-modal person re-identification (Re-ID) is critical for modern video surveillance systems. The key challenge is to align inter-modality representations conforming to semantic information present fora person and ignore background information. In this work, we present a novel convolutional neural network (CNN) based architecture designed to learn semantically Aligned cross-Modal (AXM-Net) visual and textual representations. The underlying building block, AXM-Block, is a unified multi-layer network that dynamically exploits the multi-scale knowledge from both modalities and re-calibrates each modality according to shared semantics. To complement the convolutional design, a contextual attention is applied in text branch to manipulate long-term dependencies. Moreover, we propose contextual attention on local image parts to capture fine-grained details of the person. Our design is unique in its ability to implicitly learn aligned semantics between modalities during feature learning stage. The unified feature learning effectively utilises textual data as a super-annotation signal for visual representation learning and automatically rejects irrelevant information. The entire AXM-Net is trained endto-end on CUHK-PEDES data. We report results on two tasks, person search and cross-modal Re-ID. The AXM-Net outperforms the current state-of-the-art (SOTA) method by 5.22% in Rank@1 on the CUHK-PEDES and by >10% for cross-viewpoint text-to-image Re-ID scenario on CrossRe-ID and CUHK-SYSU datasets.  Table 1. Comparison with SOTA models on the CUHK-PEDES dataset - Rank@5 Rank@10 Rank@1 mAP Table 2. Performance comparison on cross-modal Re-ID. Query \u2212 \u2192 Gallery Rank@1 mAP - Table 3. Performance comparison on the proposed cross-modal split for Market-1501 . Query \u2212 \u2192 Gallery Rank@1 mAP - Table 5. Design parameters for the visual contextual attention part branch Table 4 . Ablation study on the AXM - Net on CUHK - PEDES test set \u2192 Gallery Table 3 . Performance comparison on the proposed cross - modal split for Market - 1501 . Query \u2212 Rank@1 SA",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Text based Person Retrieval\", \"Dataset\": \"CUHK-PEDES\", \"Metric\": \"R@1\", \"Score\": \"61.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Text based Person Retrieval\", \"Dataset\": \"CUHK-PEDES\", \"Metric\": \"R@10\", \"Score\": \"85.75\"}}, {\"LEADERBOARD\": {\"Task\": \"Text based Person Retrieval\", \"Dataset\": \"CUHK-PEDES\", \"Metric\": \"R@5\", \"Score\": \"79.4\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Person Re-Identification\", \"Dataset\": \"CUHK-PEDES\", \"Metric\": \"Rank-1\", \"Score\": \"64.0\"}}, { \"LEADERBOARD\": { \"Task\": \"Person Re-Identification\", \"Dataset\": \"CUHK-PEDES\", \"Metric\":\"mAP\", \"Score\": \"45.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Person Re-Identification\", \"Dataset\": \"CUHK-PEDES\", \"Metric\": \"Top-1\", \"Score\": \"51.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Person Re-Identification\", \"Dataset\": \"CUHK-PEDES\", \"Metric\":\"mAP\", \"Score\": \"45.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Person Re-Identification\", \"Dataset\": \"CUHK-PEDES\", \"Metric\": \"Rank-1\", \"Score\": \"34.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Person Re-Identification\", \"Dataset\": \"CUHK-PEDES\", \"Metric\":\"mAP\", \"Score\": \"40.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Person Re-Identification\", \"Dataset\": \"CUHK-PEDES\", \"Metric\": \"Rank-1\", \"Score\": \"64.0\"}},"
    },
    {
        "Context": "INVESTIGATION OF DIFFERENT SKELETON FEATURES FOR CNN-BASED 3D ACTION RECOGNITION Deep learning techniques are being used in skeleton based action recognition tasks and outstanding performance has been reported. Compared with RNN based methods which tend to overemphasize temporal information, CNN-based approaches can jointly capture spatio-temporal information from texture color images encoded from skeleton sequences. There are several skeleton-based features that have proven effective in RNN-based and handcrafted-feature-based methods. However, it remains unknown whether they are suitable for CNN-based approaches. This paper proposes to encode five spatial skeleton features into images with different encoding methods. In addition, the performance implication of different joints used for feature extraction is studied. The proposed method achieved state-of-the-art performance on NTU RGB+D dataset for 3D human action analysis. An accuracy of 75.32% was achieved in Large Scale 3D Human Activity Analysis Challenge in Depth Videos. The proposed method was evaluated on NTU RGB+D Dataset Currently, NTU RGB+D Dataset is the largest dataset for action recognition This dataset is challenging and there are two types of protocols for evaluation of methods, cross-subject and cross-view The effectiveness of different types of spatial features, different joint selection schemes were evaluated There are five features evaluated, each of which was evaluated with different feature (joint) selection methods and different encoding methods Table 1. Evaluation results of different features and encoding methods. Feature Method Accuracy Fused Accuracy methods . JLd Accuracy Fused Accuracy Table 1 . Evaluation results of different features and encoding",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"Accuracy (CV)\", \"Score\": \"82.31\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"Accuracy (CS)\", \"Score\": \"75.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"Accuracy (CV)\", \"Score\": \"76.3\"}} ]"
    },
    {
        "Context": "M3D-RPN: Monocular 3D Region Proposal Network for Object Detection Understanding the world in 3D is a critical component of urban autonomous driving. Generally, the combination of expensive LiDAR sensors and stereo RGB imaging has been paramount for successful 3D object detection algorithms, whereas monocular image-only methods experience drastically reduced performance. We propose to reduce the gap by reformulating the monocular 3D detection problem as a standalone 3D region proposal network. We leverage the geometric relationship of 2D and 3D perspectives, allowing 3D boxes to utilize well-known and powerful convolutional features generated in the image-space. To help address the strenuous 3D parameter estimations, we further design depth-aware convolutional layers which enable location specific feature development and in consequence improved 3D scene understanding. Compared to prior work in monocular 3D detection, our method consists of only the proposed 3D region proposal network rather than relying on external networks, data, or multiple stages. M3D-RPN is able to significantly improve the performance of both monocular 3D Object Detection and Bird's Eye View tasks within the KITTI urban autonomous driving dataset, while efficiently using a shared multi-class model. We evaluate our proposed framework on the challenging KITTI dataset under two core 3D localization tasks: Bird's Eye View (BEV) and 3D Object Detection We comprehensively compare our method on the official test dataset as well as two validation splits, and perform analysis of the critical components which comprise our framework Table 1. Bird's Eye View. Comparison of our method to image-only 3D localization frameworks on the Bird's Eye View task (APBEV). Type - Type Easy Mod Bird ' s Eye View . Comparison of our method to image - only 3D localization frameworks on the Bird ' s Eye View task ( APBEV ) . Hard Table 2. 3D Detection. Comparison of our method to image-only 3D localization frameworks on the 3D Detection task (AP3D). Type - Type Easy Mod Bird ' s Eye View . Comparison of our method to image - only 3D localization frameworks on the Bird ' s Eye View task ( APBEV ) . Hard Table 3. Multi-class 3D Localization. The performance of our method when applied as a multi-class 3D detection system using a single shared model. We evaluate using the mod setting on KITTI. AP3D [ val1 / val2 / test ] APBEV [",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Vehicle Pose Estimation\", \"Dataset\": \"KITTI Cars Hard\", \"Metric\": \"Average Orientation Similarity\", \"Score\": \"67.08\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Vehicle Pose Estimation\", \"Dataset\": \"KITTI Cars Hard\", \"Metric\": \"Average Orientation Similarity\", \"Score\": \"34.81\"}} ]"
    },
    {
        "Context": "Sublinear Algorithms for Approximating String Compressibility We raise the question of approximating the compressibility of a string with respect to a fixed compression scheme, in sublinear time. We study this question in detail for two popular lossless compression schemes: run-length encoding (RLE) and Lempel-Ziv (LZ), and present sublinear algorithms for approximating compressibility with respect to both schemes. We also give several lower bounds that show that our algorithms for both schemes cannot be improved significantly. Our investigation of LZ yields results whose interest goes beyond the initial questions we set out to study. In particular, we prove combinatorial structural lemmas that relate the compressibility of a string with respect to Lempel-Ziv to the number of distinct short substrings contained in it. In addition, we show that approximating the compressibility with respect to LZ is related to approximating the support size of a distribution.Given an extremely long string, it is natural to wonder how compressible it is. This fundamental question is of interest to a wide range of areas of study, including computational complexity theory, machine learning, storage systems, and communications. As massive data sets are now commonplace, the ability to estimate their compressibility with extremely efficient, even sublinear time, algorithms, is gaining in importance. The most general measure of compressibility, Kolmogorov complexity, is not computable (see [14] fora textbook treatment), nor even approximable. Even under restrictions which make it computable (such as abound on the running time of decompression), it is probably hard to approximate in polynomial time, since an approximation would allow distinguishing random from pseudorandom strings and, hence, inverting one-way functions. However, the question of how compressible a large string is with respect to a specific compression scheme maybe tractable, depending on the particular scheme.We raise the question of approximating the compressibility of a string with respect to a fixed compression scheme, in sublinear time, and give algorithms and nearly matching lower bounds for several versions of the problem. While this question is new, for one compression scheme, answers follow from previous work. Namely, compressibility under Huffman encoding is determined by the entropy of the symbol frequencies. Batu et al. [3] and Brautbar and\u22c6",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Evaluation of SystemC Modelling of Reconfigurable Embedded Systems This paper evaluates the use of pin and cycle accurate SystemC models for embedded system design exploration and early software development. The target system is Mi-croBlaze VanillaNet Platform running MicroBlaze uClinux operating system. The paper compares Register Transfer Level (RTL) Hardware Description Language (HDL) simulation speed to the simulation speed of several different SystemC models. It is shown that simulation speed of pin and cycle accurate models can go up to 150 kHz, compared to 100 Hz range of HDL simulation. Furthermore, utilising techniques that temporarily compromise cycle accuracy, effective simulation speed of up to 500 kHz can be obtained.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "A Symphony Conducted by BruNet We introduce BruNet, a general P2P software framework which we use to produce the first implementation of Symphony, a 1-D Kleinberg small-world architecture. Our framework is designed to easily implement and measure different P2P protocols over different transport layers such as TCP or UDP. This paper discusses our implementation of the Symphony network, which allows each node to keep k \u2264 log N shortcut connections and to route to any other node with a short average delay of O( 1 k log 2 N ). We present experimental results taken from several PlanetLab deployments of size up to 1060 nodes. These successful deployments represent some of the largest PlanetLab deployments of P2P overlays found in the literature, and show our implementation's robustness to massive node dynamics in a WAN environment. AbstractWe introduce BruNet, a general P2P software framework which we use to produce the first implementation of Symphony, a 1-D Kleinberg small-world architecture. Our framework is designed to easily implement and measure different P2P protocols over different transport layers such as TCP or UDP. This paper discusses our implementation of the Symphony network, which allows each node to keep k \u2264 log N shortcut connections and to route to any other node with a short average delay of O( 1 k log 2 N ). We present experimental results taken from several PlanetLab deployments of size up to 1060 nodes. These successful deployments represent some of the largest PlanetLab deployments of P2P overlays found in the literature, and show our implementation's robustness to massive node dynamics in a WAN environment.  Table 1: Packet format 20 Table 1: Packet format net packets , we do not need to include a checksum ( since , similar to Ethernet packets but with a few notable differ - In many respects , the routed P2P packets are 20",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Philosophical Survey of Passwords Over the years security experts in the field of Information Technology have had a tough time in making passwords secure. This paper studies and takes a careful look at this issue from the angle of philosophy and cognitive science. We have studied the process of passwords to rank its strengths and weaknesses in order to establish a quality metric for passwords. Finally we related the process to human senses which enables us to propose a constitutional scheme for the process of password. The basic proposition is to exploit relationship between human senses and password to ensure improvement in authentication while keeping it an enjoyable activity.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Human Intracranial EEG Quantitative Analysis and Automatic Feature Learning for Epileptic Seizure Prediction Objevtive: The aim of this study is to develop an efficient and reliable epileptic seizure prediction system using intracranial EEG (iEEG) data, especially for people with drugresistant epilepsy. The prediction procedure should yield accurate results in a fast enough fashion to alert patients of impending seizures. Methods: We quantitatively analyze the human iEEG data to obtain insights into how the human brain behaves before and between epileptic seizures. We then introduce an efficient pre-processing method for reducing the data size and converting the time-series iEEG data into an image-like format that can be used as inputs to convolutional neural networks (CNNs). Further, we propose a seizure prediction algorithm that uses cooperative multi-scale CNNs for automatic feature learning of iEEG data. Results: 1) iEEG channels contain complementary information and excluding individual channels is not advisable to retain the spatial information needed for accurate prediction of epileptic seizures. 2) The traditional PCA is not a reliable method for iEEG data reduction in seizure prediction. 3) Hand-crafted iEEG features may not be suitable for reliable seizure prediction performance as the iEEG data varies between patients and overtime for the same patient. 4) Seizure prediction results show that our algorithm outperforms existing methods by achieving an average sensitivity of 87.85% and AUC score of 0.84. Conclusion: Understanding how the human brain behaves before seizure attacks and far from them facilitates better designs of epileptic seizure predictors. Significance: Accurate seizure prediction algorithms can warn patients about the next seizure attack so they could avoid dangerous activities. Medications could then be administered to abort the impending seizure and minimize the risk of injury. Closed-loop seizure intervention systems could also help to prevent seizures in patients with drug-resistant epilepsy. To examine the generalizability of our seizure prediction algorithm over different subjects, we first evaluate the performance metrics for the three patients individually and then report the average performance",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Seizure prediction\", \"Dataset\": \"Melbourne University Seizure Prediction\", \"Metric\": \"AUC\", \"Score\": \"0.840\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Seii-Supervised Image Classification\", \"Dataset\": \"EBM-NLP\", \"Metric\": \"AUC\", \"Score\": \"0.93\"}} ]"
    },
    {
        "Context": "Pyramid Feature Attention Network for Saliency detection Saliency detection is one of the basic challenges in computer vision. How to extract effective features is a critical point for saliency detection. Recent methods mainly adopt integrating multi-scale convolutional features indiscriminately. However, not all features are useful for saliency detection and some even cause interferences. To solve this problem, we propose Pyramid Feature Attention network to focus on effective high-level context features and low-level spatial structural features. First, we design Context-aware Pyramid Feature Extraction (CPFE) module for multi-scale high-level feature maps to capture rich context features. Second, we adopt channel-wise attention (CA) after CPFE feature maps and spatial attention (SA) after low-level feature maps, then fuse outputs of CA & SA together. Finally, we propose an edge preservation loss to guide network to learn more detailed information in boundary localization. Extensive evaluations on five benchmark datasets demonstrate that the proposed method outperforms the state-ofthe-art approaches under different evaluation metrics. The performance evaluation is utilized on five standard benchmark datasets: DUTS-test, ECSSD, HKU-IS, PASCAL-S and DUT-OMRON DUTS is a large scale dataset, which contains 10553 images for training and 5019 images for testing Images of this dataset have one or more salient objects and relatively complex background Same as other state-of-the-art salient object detection methods, three popular criteria are used for performance evaluation, i.e wF \u03b2 is a overall evaluation standard computed by the weighted combination of precision and recall: Image GT Amulet DCL DSS NLDF BDMPM PAGRN RFCN SRM UCF Ours Where \u03b2 2 = 0.3 as used in other approaches Table 1. The wF \u03b2 and M AE of different salient object detection approaches on all test datasets. The best three results are shown in red, blue, and green. Methods HKU - IS \u03b2 PASCAL - S M AE DUTS - test ECSSD DUT - OMRON wF Table 2. The effectiveness of edge preservation loss. The score of wF \u03b2 and M AE in our method when \u03b1 is given different values. The best result is shown in red. The test dataset is DUTS-test. without the edge preservation loss .",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Saliency Detection\", \"Dataset\": \"DUT-OMRON\", \"Metric\": \"MAE\", \"Score\": \"0.0414\"}}, {\"LEADERBOARD\": {\"Task\": \"Saliency Detection\", \"Dataset\": \"ECSSD\", \"Metric\": \"MAE\", \"Score\": \"0.0328\"}}, {\"LEADERBOARD\": {\"Task\": \"Saliency Detection\", \"Dataset\": \"PASCAL-S\", \"Metric\": \"MAE\", \"Score\": \"0.0677\"}}, {\"LEADERBOARD\": {\"Task\": \"Saliency Detection\", \"Dataset\": \"HKU-IS\", \"Metric\": \"MAE\", \"Score\": \"0.0324\"}}, {\"LEADERBOARD\": {\"Task\": \"Saliency Detection\", \"Dataset\": \"DUTS-test\", \"Metric\": \"MAE\", \"Score\": \"0.0405\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"RGB Salient Object Detection\", \"Dataset\": \"ECS\", \"Metric\": \"F-measure\", \"Score\": \"0.93\"}}, { \"LEADERBOARD\": { \"Task\": \"RGB Salient Object Detection\", \"Dataset\": \"ECS\", \"Metric\": \"F-measure\", \"Score\": \"0.93\"}}, { \"LEADERBOARD\": { \"Task\": \"RGB Salient Object Detection\", \"Dataset\": \"DUT-OMRON\", \"Metric\": \"F-measure\", \"Score\": \"0.93\"}}, { \"LEADERBOARD\": { \"Task\": \"RGB Salient Object Detection\", \"Dataset\": \"DUT-OMRON\", \"Metric\": \"F-measure\", \"Score\": \"0.93\"}}, { \"LEADERBOARD\": { \"Task\": \"RGB Salient Object Detection\", \"Dataset\": \"PAS\", \"Metric\": \"F-measure\", \"Score\": \"0.968\"}}, { \"LEADERBOARD\": { \"Task\": \"RGB Salient Object Detection\", \"Dataset\": \"Annotated\", \"Metric\": \"F-measure\", \"Score\": \"0.920\"}}, { \"LEADERBOARD\": { \"Task\": \"RGB Salient Object Detection\", \"Dataset\": \"HKU-IS\", "
    },
    {
        "Context": "CSPNET: A NEW BACKBONE THAT CAN ENHANCE LEARNING CAPABILITY OF CNN A PREPRINT Neural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision tasks such as object detection. However, such success greatly relies on costly computation resources, which hinders people with cheap devices from appreciating the advanced technology. In this paper, we propose Cross Stage Partial Network (CSPNet) to mitigate the problem that previous works require heavy inference computations from the network architecture perspective. We attribute the problem to the duplicate gradient information within network optimization. The proposed networks respect the variability of the gradients by integrating feature maps from the beginning and the end of a network stage, which, in our experiments, reduces computations by 20% with equivalent or even superior accuracy on the ImageNet dataset, and significantly outperforms state-of-the-art approaches in terms of AP 50 on the MS COCO object detection dataset. The CSPNet is easy to implement and general enough to cope with architectures based on ResNet, ResNeXt, and DenseNet. Source code is at https://github.com/WongKinYiu/CrossStagePartialNetworks. We will use ImageNet's image classification dataset used in ILSVRC 2012 to validate our proposed CSPNet Besides, we also use the MS COCO object detection dataset to verify the proposed EFM Next, we shall conduct an ablation study of EFM based on the MS COCO dataset Table 1: Ablation study of CSPNet on ImageNet. trans . \u03b3 partial Top - 1 BFLOPs two - way dense Table 2: Ablation study of EFM on MS COCO. atten . BFLOPs FPS fusion AP50 AP75 exact AP Table 3: Compare with state-of-the-art methods on ImageNet. - BFLOPs Top - 5 #Parameter Top - 1 Table 4: Compare with state-of-the-art methods on MSCOCO Object Detection. AP75 BFLOPs AP50 APS #Parameter APL APM AP Table 5: Inference rate on mobile GPU (mGPU) and CPU real-time object detectors (in fps). upgraded . For the same CSPPeleeNet Ref . backbone , although EFM ( SAM ) is 62 fps slower than PRN ( 3l ) on GTX Size GPU CPU mGPU AP50",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Real-Time Object Detection\", \"Dataset\": \"COCO\", \"Metric\": \"FPS\", \"Score\": \"58\"}}, {\"LEADERBOARD\": {\"Task\": \"Real-Time Object Detection\", \"Dataset\": \"COCO\", \"Metric\": \"MAP\", \"Score\": \"33.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Real-Time Object Detection\", \"Dataset\": \"COCO\", \"Metric\": \"inference time (ms)\", \"Score\": \"17\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of params\", \"Score\": \"20.5M\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"79.8%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"95.2%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"box AP\", \"Score\": \"45.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"71.71%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"94.82%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"79.7%\"}}, { \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"box AP\", \"Score\": \"45.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"box AP\", \"Score\": \"45.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"box AP\", \"Score\": \"43.3\"}}, { '"
    },
    {
        "Context": "NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection Current state-of-the-art convolutional architectures for object detection are manually designed. Here we aim to learn a better architecture of feature pyramid network for object detection. We adopt Neural Architecture Search and discover anew feature pyramid architecture in a novel scalable search space covering all cross-scale connections. The discovered architecture, named NAS-FPN, consists of a combination of top-down and bottom-up connections to fuse features across scales. NAS-FPN, combined with various backbone models in the RetinaNet framework, achieves better accuracy and latency tradeoff compared to state-ofthe-art object detection models. NAS-FPN improves mobile detection accuracy by 2 AP compared to state-of-the-art SS-DLite with MobileNetV2 model in [32] and achieves 48.3 AP which surpasses Mask R-CNN [10] detection accuracy with less computation time.  Table 1: Performance of RetinaNet with NAS-FPN and other state-of-the-art detectors on test-dev set of COCO. 3 5 Table 1 : Performance of RetinaNet with NAS - FPN and other state - of - the - art detectors on test - dev set of COCO . Figure 10 : Performance comparison of NAS - FPN with fea - @384 +DB @384 3 inference time ( ms ) 5 1200 test - dev AP # params 1000 model # FLOPs image size @256",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Real-Time Object Detection\", \"Dataset\": \"COCO\", \"Metric\": \"FPS\", \"Score\": \"3.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Real-Time Object Detection\", \"Dataset\": \"COCO\", \"Metric\": \"MAP\", \"Score\": \"48.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Real-Time Object Detection\", \"Dataset\": \"COCO\", \"Metric\": \"inference time (ms)\", \"Score\": \"278.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Real-Time Object Detection\", \"Dataset\": \"COCO\", \"Metric\": \"FPS\", \"Score\": \"3\"}}, {\"LEADERBOARD\": {\"Task\": \"Real-Time Object Detection\", \"Dataset\": \"COCO\", \"Metric\": \"MAP\", \"Score\": \"25.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Real-Time Object Detection\", \"Dataset\": \"COCO\", \"Metric\": \"inference time (ms)\", \"Score\": \"285\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"box AP\", \"Score\": \"29.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"box AP\", \"Score\": \"29.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"box AP\", \"Score\": \"25.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"box AP\", \"Score\": \"22.6\"}} ]"
    },
    {
        "Context": "Masked Autoregressive Flow for Density Estimation Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks. In the following paragraphs, we give a brief description of the four UCI datasets (POWER, GAS, HEPMASS, MINIBOONE) and of the way they were preprocessed The POWER dataset contains measurements of electric power consumption in a household over a period of 47 months Created by Fonollosa et al., this dataset represents the readings of an array of 16 chemical sensors exposed to gas mixtures over a 12 hour period Table 1: Average test log likelihood (in nats) for unconditional density estimation. The best performing model for each dataset is shown in bold (multiple models are highlighted if the difference is not statistically significant according to a paired t-test). Error bars correspond to 2 standard deviations. BSDS300 GAS MINIBOONE POWER HEPMASS Table 2: Average test log likelihood (in nats) for conditional density estimation. The best performing model for each dataset is shown in bold. Error bars correspond to 2 standard deviations. \u2212 397 \u00b1 21 2568 \u00b1 26 2936 \u00b1 27 unconditional conditional 147 \u00b1 20 3049 \u00b1 26 MNIST 2367 \u00b1 29 2576 \u00b1 27 Table 4: Number of hidden layers L and number of hidden units H given as options for each dataset. Each combination is reported in the format L \u00d7 H. BSDS300 GAS MINIBOONE MNIST POWER CIFAR - 10 HEPMASS Table 5: Dimensionality D and number of",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Density Estimation\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Log-likelihood\", \"Score\": \"3049\"}}, {\"LEADERBOARD\": {\"Task\": \"Density Estimation\", \"Dataset\": \"MNIST\", \"Metric\": \"Log-likelihood\", \"Score\": \"-1038.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Density Estimation\", \"Dataset\": \"CIFAR-10 (Conditional)\", \"Metric\": \"Log-likelihood\", \"Score\": \"3058\"}}, {\"LEADERBOARD\": {\"Task\": \"Density Estimation\", \"Dataset\": \"UCI POWER\", \"Metric\": \"Log-likelihood\", \"Score\": \"0.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Density Estimation\", \"Dataset\": \"MNIST (Conditional)\", \"Metric\": \"Log-likelihood\", \"Score\": \"-1030.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Density Estimation\", \"Dataset\": \"BSDS300\", \"Metric\": \"Log-likelihood\", \"Score\": \"153.71\"}}, {\"LEADERBOARD\": {\"Task\": \"Density Estimation\", \"Dataset\": \"UCI MINIBOONE\", \"Metric\": \"Log-likelihood\", \"Score\": \"-12.27\"}}, {\"LEADERBOARD\": {\"Task\": \"Density Estimation\", \"Dataset\": \"UCI HEPMASS\", \"Metric\": \"Log-likelihood\", \"Score\": \"-15.15\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"Binarized MNIST\", \"Metric\": \"nats\", \"Score\": \"84.68\"}} ]"
    },
    {
        "Context": "D\u00e9veloppement et analyse multi outils d'un protocole MAC d\u00e9terministe pour un r\u00e9seau de capteurs sans fil Nous pr\u00e9sentons dans cet article une m\u00e9thodologie multi outils de d\u00e9veloppement et d'analyse de protocole utilis\u00e9e pour valider une nouvelle m\u00e9thode d'acc\u00e8s. La technologie IEEE 802.15.4 / ZigBee sert de base protocolaire \u00e0 la proposition d'une couche MAC d\u00e9terministe offrant un haut niveau de QdS. Ce type de WPAN peut typiquement \u00eatre utilis\u00e9 pour des r\u00e9seaux de capteurs sans fil \u00e0 fortes contraintes temporelles. Afin de valider les protocoles propos\u00e9s, trois outils compl\u00e9mentaires et ad\u00e9quats sont utilis\u00e9s : les R\u00e9seaux de Petri pour la validation formelle du s\u00e9quencement des trames, un simulateur sp\u00e9cifique pour les aspects temporels et des m\u00e9trologies sur un prototypage r\u00e9el \u00e0 base de composants ZigBee FREESCALE pour la caract\u00e9risation fine des couches physique et liaison. ABSTRACT. In this article, we present a multi-tool method for the development and the analysis of anew medium access method. IEEE 802.15.4 / ZigBee technology has been used as a basis for this new determinist MAC layer which enables a high level of QoS. This WPAN can be typically used for wireless sensor networks which require strong temporal constraints. To validate the proposed protocol, three complementary and adequate tools are used: Petri Nets for the formal validation of the algorithm, a dedicated simulator for the temporal aspects, and some measures on areal prototype based on a couple of ZigBee FREESCALE components for the hardware characterization of layers #1 and #2. MOTS-CL\u00c9S : R\u00e9seau de capteur sans fil, IEEE 802.15.4, ZigBee, M\u00e9thode d'acc\u00e8s au m\u00e9dium, Qualit\u00e9 de Service, Prototypage, Validation formelle, RdP, Simulation, Analyse de performance, m\u00e9trologie.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Error-Correcting Data Structures We study data structures in the presence of adversarial noise. We want to encode a given object in a succinct data structure that enables us to efficiently answer specific queries about the object, even if the data structure has been corrupted by a constant fraction of errors. This new model is the common generalization of (static) data structures and locally decodable errorcorrecting codes. The main issue is the tradeoff between the space used by the data structure and the time (number of probes) needed to answer a query about the encoded object. We prove a number of upper and lower bounds on various natural error-correcting data structure problems. In particular, we show that the optimal length of error-correcting data structures for the Membership problem (where we want to store subsets of size s from a universe of size n) is closely related to the optimal length of locally decodable codes for s-bit strings.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Computing a Discrete Logarithm in O(n 3 ) This paper presents a means with time complexity of at worst O(n 3 ) to compute the discrete logarithm on cyclic finite groups of integers modulo p. The algorithm makes use of reduction of the problem to that of finding the concurrent zeros of two periodic functions in the real numbers. The problem is treated as an analog to a form of analog rotor code computed cipher.The computation of a Discrete Logarithm [1]  is a problem for which several algorithms [2]  presently exist. The general problem is addressed from a variety of perspectives depending on the subject matter of interest. The Discrete Logarithm problem is also found addressed in conjunction with the more general problem of factoring. [3]  However, the complexity of present solutions is not known to be polynomial.An algorithm is presented that computes a Discrete Logarithm on cyclic finite groups of integers modulo p (i.e. Z p ). The complexity of the algorithm is at worst O(n 3 ). No comprehensive review of existing algorithms is provided. Those interested in or otherwise unfamiliar with existing algorithms are referred to the references, and most any university text on the subject of computational complexity.Most existing algorithms address the problem from the perspective of abstract algebra, working with various aspects of algebraic structures to improve the execution of \"the na\u00efve algorithm\" of brute force factoring. Our solution addresses the problem by projection onto an arc of 360\u00b0. [4]  This transforms the problem into one operating on the angular rotation of the cycle, rather than conventional notion of modular arithmetic. The projection allows us to address the exponentiation in the na\u00efve algorithm independent of the value of p by arithmetic operations.The Discrete Logarithm problem over cyclic finite groups has a well known definition as the solution of the equation x k = y, given values for x and y, for the value of k, in some group G. We concern our self herewith the cyclic finite multiplicative groups of integers modulo p.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Session-based Recommendation with Graph Neural Networks The problem of session-based recommendation aims to predict user actions based on anonymous sessions. Previous methods model a session as a sequence and estimate user representations besides item representations to make recommendations. Though achieved promising results, they are insufficient to obtain accurate user vectors in sessions and neglect complex transitions of items. To obtain accurate item embedding and take complex transitions of items into account, we propose a novel method, i.e. Session-based Recommendation with Graph Neural Networks, SR-GNN for brevity. In the proposed method, session sequences are modeled as graphstructured data. Based on the session graph, GNN can capture complex transitions of items, which are difficult to be revealed by previous conventional sequential methods. Each session is then represented as the composition of the global preference and the current interest of that session using an attention network. Extensive experiments conducted on two real datasets show that SR-GNN evidently outperforms the state-of-the-art session-based recommendation methods consistently. In this section, we first describe the datasets, compared methods, and evaluation metrics used in the experiments We evaluate the proposed method on two real-world representative datasets, i.e The Yoochoose dataset is obtained from the RecSys Challenge 2015, which contains a stream of user clicks on an e-commerce website within 6 months The Diginetica dataset comes from CIKM Cup 2016, where only its transactional data is used For fair comparison, following, we filter out all sessions of length 1 and items appearing less than 5 times in both datasets The remaining 7,981,580 sessions and 37,483 items constitute the Yoochoose dataset, while 204,771 sessions and 43097 items construct the Diginetica dataset The statistics of datasets are summarized in Following metrics are used to evaluate compared methods Table 1: Statistics of datasets used in the experiments Yoochoose 1 / 64 60 , 858 29 , 618 Diginetica 369 , 859 16 , 766 43 , 097 Table 1 : Statistics of datasets used in the experiments 557 , 248 5 , 917 , 745 55 , 898 8 , 326 , 407 Yoochoose 1 / 4 982 , 961 719 , 470 Table 2: The performance of SR-GNN with other baseline methods over three datasets Method Yoochoose 1 / 64 Diginetica P@20 Table 2 : The performance of SR - GNN with other baseline MRR@20 Yoochoose 1 / 4 Table 3: The performance of different methods with different session lengths evaluated in terms of P@20 Method Yoochoose 1 / 64 Diginetica Table 3 : The performance of different methods with different Long Short",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Session-Based Recommendations\", \"Dataset\": \"Last.FM\", \"Metric\": \"HR@20\", \"Score\": \"22.33\"}}, {\"LEADERBOARD\": {\"Task\": \"Session-Based Recommendations\", \"Dataset\": \"Last.FM\", \"Metric\": \"MRR@20\", \"Score\": \"8.23\"}}, {\"LEADERBOARD\": {\"Task\": \"Session-Based Recommendations\", \"Dataset\": \"Diginetica\", \"Metric\": \"Hit@20\", \"Score\": \"50.73\"}}, {\"LEADERBOARD\": {\"Task\": \"Session-Based Recommendations\", \"Dataset\": \"Diginetica\", \"Metric\": \"MRR@20\", \"Score\": \"17.59\"}}, {\"LEADERBOARD\": {\"Task\": \"Session-Based Recommendations\", \"Dataset\": \"yoochoose1/4\", \"Metric\": \"HR@20\", \"Score\": \"71.36\"}}, {\"LEADERBOARD\": {\"Task\": \"Session-Based Recommendations\", \"Dataset\": \"yoochoose1/4\", \"Metric\": \"MRR@20\", \"Score\": \"31.89\"}}, {\"LEADERBOARD\": {\"Task\": \"Session-Based Recommendations\", \"Dataset\": \"Gowalla\", \"Metric\": \"HR@20\", \"Score\": \"50.32\"}}, {\"LEADERBOARD\": {\"Task\": \"Session-Based Recommendations\", \"Dataset\": \"Gowalla\", \"Metric\": \"MRR@20\", \"Score\": \"24.25\"}}, {\"LEADERBOARD\": {\"Task\": \"Session-Based Recommendations\", \"Dataset\": \"yoochoose1\", \"Metric\": \"MRR@20\", \"Score\": \"30.94\"}}, {\"LEADERBOARD\": {\"Task\": \"Session-Based Recommendations\", \"Dataset\": \"yoochoose1\", \"Metric\": \"Precision@20\", \"Score\": \"70.57\"}}, {\"LEADERBOARD\": {\"Task\": \"Session-Based Recommendations\", \"Dataset\": \"yoochoose1/64\", \"Metric\": \"HR@20\", \"Score\": \"70.57\"}}, {\"LEADERBOARD\": {\"Task\": \"Session-Based Recommendations\", \"Dataset\": \"yoochoose1/64\", \"Metric\": \"MRR@20\", \"Score\": \"30.94\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Session-Based Recommendations\", \"Dataset\": \"Diginetica\", \"Metric\": \"Hit@20\", \"Score\": \"51.31\"}}, { \"LEADERBOARD\": { \"Task\": \"Session-Based Recommendations\", \"Dataset\": \"Diginetica\", \"Metric\": \"MRR@20\", \"Score\": \"18.03\"}}, { \"LEADERBOARD\": { \"Task\": \"Session-Based Recommendations\", \"Dataset\": \"yoochoose1\", \"Metric\": \"MRR@20\", \"Score\": \"31.12\"}}, { \"LEADERBOARD\": { \"Task\": \"Session-Based Recommendations\", \"Dataset\": \"yoochoose1\", \"Metric\": \"Precision@20\", \"Score\": \"71.02\"}}, { \"LEADERBOARD\": { \"Task\": \"Session-Based Recommendations\", \"Dataset\": \"yoochoose1/64\", \"Metric\": \"HR@20\", \"Score\": \"71.02\"}}, { \"LEADERBOARD\": { \"Task\": \"Session-Based Recommendations\", \"Dataset\": \"yoochoose1/64\", \"Metric\": \"MRR@20\", \"Score\": \"31.12\"}} ]"
    },
    {
        "Context": "FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation Over the past few years, we have witnessed the success of deep learning in image recognition thanks to the availability of large-scale human-annotated datasets such as PAS-CAL VOC, ImageNet, and COCO. Although these datasets have covered a wide range of object categories, there are still a significant number of objects that are not included. Can we perform the same task without a lot of human annotations? In this paper, we are interested in few-shot object segmentation where the number of annotated training examples are limited to 5 only. To evaluate and validate the performance of our approach, we have built a few-shot segmentation dataset, FSS-1000, which consists of 1000 object classes with pixelwise annotation of ground-truth segmentation. Unique in FSS-1000, our dataset contains significant number of objects that have never been seen or annotated in previous datasets, such as tiny daily objects, merchandise, cartoon characters, logos, etc.We build our baseline model using standard backbone networks such as VGG-16, ResNet-101, and Inception. To our surprise, we found that training our model from scratch using FSS-1000 achieves comparable and even better results than training with weights pre-trained by ImageNet which is more than 100 times larger than FSS-1000. Both our approach and dataset are simple, effective, and easily extensible to learn segmentation of new object classes given very few annotated training examples. Dataset is available at https We conduct experiments to evaluate the practicability of FSS-1000 and the performance of our method under fewshot learning settings We evaluate models with the same network architecture but trained on different datasets to show that FSS-1000 is effective for few-shot segmentation task Table 1. Large-scale datasets comparison. Mean and standard deviation are based on the expected number of images in each class. Table 1 . Large - scale datasets comparison . Mean and standard deviation are based on the expected number of images in each class . FSS - 1000 ImageNet Mean Stddev Dataset of FSS - 1000 Table 2. Different network settings to explore the best settings for our network architecture. our network architecture . MeanIoU Table 3. Different few-shot segmentation networks trained and tested on FSS-1000. MeanIoU Table 4. Comparison of different models on PASCAL-5 i . GN is Guided Network and Ours* is our model trained on FSS-1000. All models are using 5-shot setting. PASCAL - 5 Mean PASCAL - 5 0 PASCAL - 5 2 Table 5. Comparison of models trained and tested on different datasets. Each model (row) shows the training stages, e.g., model I uses the pre-trained",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Few-Shot Semantic Segmentation\", \"Dataset\": \"FSS-1000\", \"Metric\": \"Mean IoU\", \"Score\": \"80.12\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Few-Shot Semantic Segmentation\", \"Dataset\": \"FSS-1000\", \"Metric\": \"Mean IoU\", \"Score\": \"78.36\"}} ]"
    },
    {
        "Context": "Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation Vision-language navigation (VLN) is the task of navigating an embodied agent to carryout natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems. First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide an intrinsic reward to encourage global matching between instructions and trajectories, and a reasoning navigator is employed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model significantly outperforms previous methods by 10% on SPL and achieves the new state-of-the-art performance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation Learning (SIL) method to explore unseen environments by imitating its own past, good decisions. We demonstrate that SIL can approximate a better and more efficient policy, which tremendously minimizes the success rate performance gap between seen and unseen environments (from 30.7% to 11.7%). We evaluate our approaches on the Roomto-Room (R2R) dataset for vision-language navigation in real 3D environments, which is built upon the Matter-port3D dataset The R2R dataset has 7,189 paths that capture most of the visual diversity and 21,567 humanannotated instructions with an average length of 29 words The R2R dataset is split into training, seen validation, unseen validation, and test sets This setting is preferred and able to clearly measure the generalizability of the navigation policy, so we evaluate our RCM approach under the standard testing scenario We report five evaluation metrics as used by the VLN Challenge: Path Length (PL), Navigation Error (NE), Oracle Success Rate (OSR), Success Rate (SR), and Success rate weighted by inverse Path Length (SPL) Table 1: Comparison on the R2R test set [3]. Our RCM model sig- nificantly outperforms the SOTA methods, especially on SPL (the primary metric for navigation tasks [1]). Moreover, using SIL to imitate itself on the training set can further improve its efficiency: the path length is shortened by 3.25m. Note that with beam search, the agent executes K trajectories at test time and chooses the most confident one as the ending point, which results in a super long path and is heavily penalized by SPL. 12 23 Ours 35 38 28 18 Test Set ( VLN Challenge Leaderboard ) SR \u2191 PL \u2193 NE \u2193 OSR \u2191 Table 2: Ablation study on seen and unseen validation sets. We report the performance of the speaker-follower model without beam search as the baseline. Row 1-5 shows the influence of each individual component by successively removing it from the final model. Row 6",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Visual Navigation\", \"Dataset\": \"R2R\", \"Metric\": \"spl\", \"Score\": \"0.38\"}}, {\"LEADERBOARD\": {\"Task\": \"Vision-Language Navigation\", \"Dataset\": \"Room2Room\", \"Metric\": \"spl\", \"Score\": \"0.59\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Visual Navigation\", \"Dataset\": \"VLN Challenge\", \"Metric\": \"error\", \"Score\": \"3.09\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Navigation\", \"Dataset\": \"VLN Challenge\", \"Metric\": \"length\", \"Score\": \"40.85\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Navigation\", \"Dataset\": \"VLN Challenge\", \"Metric\": \"oracle success\", \"Score\": \"0.81\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Navigation\", \"Dataset\": \"VLN Challenge\", \"Metric\":\"spl\", \"Score\": \"0.22\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Navigation\", \"Dataset\": \"VLN Challenge\", \"Metric\":\"success\", \"Score\": \"0.71\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Navigation\", \"Dataset\": \"VLN Challenge\", \"Metric\": \"error\", \"Score\": \"3.69\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Navigation\", \"Dataset\": \"VLN Challenge\", \"Metric\": \"length\", \"Score\": \"10.43\"}}, { \"LEADERBOARD\": { 'Tas"
    },
    {
        "Context": "Support Vector Guided Softmax Loss for Face Recognition Face recognition has witnessed significant progresses due to the advances of deep convolutional neural networks (CNNs), the central challenge of which, is feature discrimination. To address it, one group tries to exploit miningbased strategies (e.g., hard example mining and focal loss) to focus on the informative examples. The other group devotes to designing margin-based loss functions (e.g., angular, additive and additive angular margins) to increase the feature margin from the perspective of ground truth class. Both of them have been well-verified to learn discriminative features. However, they suffer from either the ambiguity of hard examples or the lack of discriminative power of other classes. In this paper, we design a novel loss function, namely support vector guided softmax loss (SV-Softmax), which adaptively emphasizes the mis-classified points (support vectors) to guide the discriminative features learning. So the developed SV-Softmax loss is able to eliminate the ambiguity of hard examples as well as absorb the discriminative power of other classes, and thus results in more discrimiantive features. To the best of our knowledge, this is the first attempt to inherit the advantages of mining-based and margin-based losses into one framework. Experimental results on several benchmarks have demonstrated the effectiveness of our approach over state-of-the-arts. The MS-Celeb-1M dataset contains about 100k identities with 10 million images We use two datasets, MegaFace and Trillion Pairs 3 , as the test data MegaFace datasets aim at evaluating the performance of face recognition algorithms at the million scale of distractors, which include gallery set and probe set Trillion Pairs datasets are recently released as a public available testing benchmark, which are consisted of the following two parts, ELFW and DELFW All the reported results in this paper are evaluated by a single model, without model ensemble or other fusion strategies To the evaluation metrics, the cosine distance of features is computed as the similarity score Specifically, for face identification, the Cumulative Match Characteristics (CMC) curves are adopted to evaluate the Rank-1 face identification accuracy We test our models on several popular public face datasets, including LFW, MegaFace Challenge and the recent Trillion Pairs Challenge Table 1. Verification performance (%) of different loss functions on LFW test data. Naive - fused Mining - based Method LFW 6000 LFW BLUFR TPR@FAR=1e - 4 TPR@FAR=1e - 3 Pairs Accuracy TPR@FAR=1e - 5 Table 2. Results (%) of different losses on MegaFace Challenge. SV - Softmax F - AM - Softmax ( % ) Table 2 . Results ( % ) of different losses on MegaFace Challenge . Method SV - AM - Softmax F - Arc - Softmax A - Softmax True Identification Verification Verification with 1M Distractors F - Softmax 1 AM - Softmax HM - Softmax HM - AM - Softmax Rank1@1e6 Arc - Softmax Softmax SV - Arc - Softmax TPR@FAR=1e - 6 HM - Arc - Softmax Table 3. Performance (%) of different loss functions on Trillion Pairs Challenge. Method TPR@FAR=1e - 9 Identification Verification TPR@FAR=1e - 3 Table 4. Performance (%) of SV-AM-Softmax",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Face Verification\", \"Dataset\": \"Trillion Pairs Dataset\", \"Metric\": \"Accuracy\", \"Score\": \"72.71\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Verification\", \"Dataset\": \"MegaFace\", \"Metric\": \"Accuracy\", \"Score\": \"97.38%\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Identification\", \"Dataset\": \"Trillion Pairs Dataset\", \"Metric\": \"Accuracy\", \"Score\": \"73.56\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Identification\", \"Dataset\": \"MegaFace\", \"Metric\": \"Accuracy\", \"Score\": \"97.2%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"MS-COCO\", \"Metric\": \"Rank-1\", \"Score\": \"84.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"MS-COCO\", \"Metric\": \"Rank-5\", \"Score\": \"99.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"MS-COCO\", \"Metric\":\"mAP\", \"Score\": \"71.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"MS-COCO\", \"Metric\": \"Rank-1\", \"Score\": \"81.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"MS-COCO\", \"Metric\": \"Rank-5\", \"Score\": \"95.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"MS-COCO\", \"Metric\":\"mAP\", \"Score\": \"82.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"Text-103\", \"Metric\": \"MAP\", \"Score\": \"81.2\"}}, { \"LEADERBOARD\": { \"Task\": "
    },
    {
        "Context": "FreeBSD Mandatory Access Control Usage for Implementing Enterprise Security Policies FreeBSD was one of the first widely deployed free operating systems to provide mandatory access control. It supports a number of classic MAC models. This tutorial paper addresses exploiting this implementation to enforce typical enterprise security policies of varying complexities.Security needs of organizations are becoming more and more sophisticated nowadays. Most general-purpose operating systems (GPOS) provide access control policies to meet these needs. There are cases when the traditionally deployed Discretionary Access Control (DAC) rules are not sufficient: they tend to quickly become unmanageable in the case of large installations, and also are not enough for controlling information flows. This is when the Mandatory Access Control (MAC) comes in: it provides for better manageability and directly targets the information flows. In their turn, the information flows address the confidentiality and integrity needs of information security within an organization. Until very recently, the GPOSes tended to provide various flavors of DAC only. The FreeBSD OS [1] was one of the first widely deployed open source GPOSes to support MAC [2], [3]. In this paper, a number of organizational policy examples are implemented in the environment of the FreeBSD MAC.The authors strongly believe that in order to implement a sound MAC policy it is important to understand MAC's mathematical foundations. These foundations were set by Denning in [4]. There also exists a terminology confusion between MAC and LBAC (lattice-based access control). These models are the same, because MAC security labels [5] directly correspond to security classes of lattice-based models (this was also pointed to by Sandhu [6] and Osborn [7]).Let us first address the definition of the information flow. According to Denning and Sandhu, the security policies regulate how the information \"flows from one object to another\". A typical object is a shared memory segment, a file system objector a network packet. Obviously, controlling the information flows is important to prevent the leakage of the confidential information, the one usually sought by insiders. Another goal is the forgery prevention, so that no untrusted reports are ever submitted to the top level of the organization hierarchy, and no top-ranking company officers take any unchecked or untrusted information into account during decision making.To implement the information flow control, every object is assigned a security label (also called a security classification), implemented by the FreeBSD file label. When the information flows from one object into another, an information flow from the security class of the first object to the security class of the second one also takes place. Whether the information flow is allowed is regulated by the relation between the object security classes. The subjects are the entities performing the information transfer between the objects. In our case, a subject appears when a user logs in to the system and is assigned a set of privileges. As we are considering MAC, the set of privileges is rendered as security clearance. It is implemented by the FreeBSD user label.This paper is organized as follows. In the next section an example of an organization and its document flow is described. The following sections implement organization's information security goals, which gradually increase in complexity. The information security goals specify the target effect: preserving data and process integrity, restricting access to the confidential information, or implementing a consulting services policy. For every security goal, a corresponding classic MAC model or a combination of them is chosen. The models are then implemented in the FreeBSD MAC framework.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "THE IMPACT OF NEW TECHNOLOGIES IN PUBLIC FINANCIAL MANAGEMENT AND PERFORMANCE: AGENDA FOR PUBLIC FINANCIAL MANAGEMENT REFORMANCE IN THE CONTEXT OF GLOBAL BEST PRACTICES  The evaluation phase is also very adapted for ICT use In the evaluation phase, the decisionmaker who in this case would be the administrator, needs to have feedback from the farmers, so as to assess the adequacy of the proposed services to the farmers' needs",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "A SOLUTION TO PRODUCT DETECTION IN DENSELY PACKED SCENES  All experiment are conducted on MMDetection Platform with single GPU and run evaluation by library pycocotools on validation set Table 1: Quantitative Statistics of SKU-110k number of annotations in a single image : 61 Data Description and Analysis 1730996 431546 1208482 Test Set1 Method Train Set Image Number 2 After eliminating the invalid data , the size of the data set is shown below : Total 8219 Annotation Number 2936 1 ] . 11743 Table 2: Statistics of Number of Annotations in Single Image number of annotations in a single image : 61 Data Description and Analysis 1730996 431546 1208482 Test Set1 Method Train Set Image Number 2 After eliminating the invalid data , the size of the data set is shown below : Total 8219 Annotation Number 2936 1 ] . 11743 Table 3: Parameter and Results of Cascade R-CNN mmAP ( % ) mmAP ( test ) mmAP IoU Threshold",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Dense Object Detection\", \"Dataset\": \"SKU-110K\", \"Metric\": \"AP\", \"Score\": \"0.587\"}}, {\"LEADERBOARD\": {\"Task\": \"Dense Object Detection\", \"Dataset\": \"SKU-110K\", \"Metric\": \"AP75\", \"Score\": \"0.673\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Keypoint Detection\", \"Dataset\": \"SKU-110K\", \"Metric\":\"mAP@0.5\", \"Score\": \"62.5%\"}}, { \"LEADERBOARD\": { \"Task\": \"Keypoint Detection\", \"Dataset\": \"SKU-110K\", \"Metric\":\"mAP@0.5\", \"Score\": \"45.2\"}} ]"
    },
    {
        "Context": "A computationally-efficient construction for the matrix-based key distribution in sensor network This paper introduces a variant for the symmetric matrix-based key distribution in sensor network introduced by Du et al. Our slight modification shows that the usage of specific structures for the public matrix instead of fully random matrix with elements in Z q can reduce the computation overhead for generating the public key information and the key itself. An intensive analysis followed by modified scheme demonstrates the value of our contribution in relation with the current work and show the equivalence of the security G c (j)[j mod \u03bb] \u2190 g 1j .",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Sentence Similarity Learning by Lexical Decomposition and Composition Most conventional sentence similarity methods only focus on similar parts of two input sentences, and simply ignore the dissimilar parts, which usually give us some clues and semantic meanings about the sentences. In this work, we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences. The model represents each word as a vector, and calculates a semantic matching vector for each word based on all words in the other sentence. Then, each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector. After this, a two-channel CNN model is employed to capture features by composing the similar and dissimilar components. Finally, a similarity score is estimated over the composed feature vectors. Experimental results show that our model gets the state-of-the-art performance on the answer sentence selection task, and achieves a comparable result on the paraphrase identification task.Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences. It plays an important role fora variety of tasks in both NLP and IR communities. For example, in paraphrase identification task, sentence similarity is used to determine whether two sentences are paraphrases or not (Yin and Sch\u00fctze, 2015;He et al., 2015). For question answering and information retrieval tasks, sentence similarities between query-answer pairs are used for assessing the relevance and ranking all the candidate answers (Severyn and Moschitti, 2015;Wang and Ittycheriah, 2015).However, sentence similarity learning has following challenges:1. There is a lexical gap between semantically equivalent sentences. Take the E 1 and E 2 in Table 1 for example, they have the similar meaning but with different lexicons.2. Semantic similarity should be measured at different levels of granularity (word-level, phrase-level and syntax-level). E.g., \"not related\" in E 2 is an indivisible phrase when matching with \"irrelevant\" in E 1 (shown in square brackets).3. The dissimilarity (shown in angle brackets) between two sentences is also a significant clue (Qiu et al., 2006). For example, by judging the dissimilar parts, we can easily identify that E 3 and E 5 share the similar meaning \"The study is about salmon\", because \"sockeye\" belongs to the salmon family, and \"flounder\" does not. Whereas the meaning of E 4 is quite different from E 3 , which emphasizes \"The study is about red (a special kind of) salmon\", because both \"sockeye\" and \"coho\" are in the salmon family. How we can extract and utilize those information becomes another challenge.In order to handle the above challenges, researchers have been working on sentence similarity algorithms fora longtime. To bridge the lexical gap (challenge 1), some word similarity metrics were proposed to match different but semantically related words. Examples include knowledge-based metrics (Resnik, 1995) and corpus-based metrics (Jiang and Conrath, 1997; Yin and Sch\u00fctze, 2015;He et al., 2015). To measure sentence similarity from various granularities (challenge 2), researchers have explored features extracted from n-grams, continuous phrases, discontinuous phrases, and parse trees (Yin  and Sch\u00fctze, 2015;He et al., 2015;Heilman and Smith, 2010). The third challenge did not get much arXiv:1602.07019v2 [cs.CL] 14 Jul 2017 E1 The research is [irrelevant] to sockeye. E2 The study is [not related] to salmon. E3 The research is relevant to salmon. E4 The study is relevant to sockeye, instead of coho . E5 The study is relevant to sockeye, rather than flounder . We evaluate our model on two tasks: answer sentence selection and paraphrase identification We experiment on two datasets: QASent and WikiQA The statistics of the two datasets can be found in, where QASent was created from the TREC QA track, and WikiQA is constructed from real queries of Bing and Wikipedia We experiment on the Microsoft Research Paraphrase corpus (MSRP), which includes 2753 true and 1323 false instances in the training set, and 1147 true and 578 false instances in the test set Table 2: Results on the QASent dataset. ( CNN + sparse features ) ( Word embedding alignment ) ( CNN only ) win - 1 ( Attention - based CNN ) Figure 2 : Influence of different configuration . win - 5 win - 4 linear win - 3 ( b ) Decomposition . local - 3 local - 2 rigid MRR MAP Table 3: Results on the WikiQA dataset. ( 2 - gram CNN ) ( Attention - based CNN ) ( Attention - based LSTM ) MRR MAP Table 4: Experimental results for paraphrase identification on MSRP corpus. N / A Acc F1",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Question Answering\", \"Dataset\": \"WikiQA\", \"Metric\": \"MAP\", \"Score\": \"0.7058\"}}, {\"LEADERBOARD\": {\"Task\": \"Question Answering\", \"Dataset\": \"WikiQA\", \"Metric\": \"MRR\", \"Score\": \"0.7226\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Paraphrase Identification\", \"Dataset\": \"SQuAD1.1\", \"Metric\": \"Accuracy\", \"Score\": \"90.5\"}} ]"
    },
    {
        "Context": "Grid R-CNN This paper proposes a novel object detection framework named Grid R-CNN, which adopts a grid guided localization mechanism for accurate object detection. Different from the traditional regression based methods, the Grid R-CNN captures the spatial information explicitly and enjoys the position sensitive property of fully convolutional architecture. Instead of using only two independent points, we design a multi-point supervision formulation to encode more clues in order to reduce the impact of inaccurate prediction of specific points. To take the full advantage of the correlation of points in a grid, we propose a two-stage information fusion strategy to fuse feature maps of neighbor grid points. The grid guided localization approach is easy to be extended to different state-of-the-art detection frameworks. Grid R-CNN leads to high quality object localization, and experiments demonstrate that it achieves a 4.1% AP gain at IoU=0.8 and a 10.0% AP gain at IoU=0.9 on COCO benchmark compared to Faster R-CNN with Res50 backbone and FPN architecture. We perform experiments on two object detection datasets, Pascal VOC and COCO On Pascal VOC dataset, we train our model on VOC07+12 trainval set and evaluate on VOC2007 test set On COCO dataset which contains 80 object categories, we train our model on the union of 80k train images and 35k subset of val images and test on a 5k subset of val (minival) and 20k test-dev Table 1. Comparison of different grid points strategies in Grid R- CNN. Experiments show that more grid points bring performance gains. AP Table 2. Comparison of different feature fusion methods. Bi- directional feature fusion, first order feature fusion and second order fusion all demonstrate improvements. Second order fusion achieves the best performance with an improvement of 0.7% on AP. AP . Table 4 . Comparison with R - FCN and FPN on Pascal VOC small large method Table 2 . Comparison of different feature fusion methods . directional feature fusion , first order feature fusion and second terion which is the average AP across IoU thresholds range from dataset . Note that we evaluate the results with a COCO - style cri - AP Table 3. Comparison of enlarging the proposal directly and ex- tended region mapping strategy. terion which is the average AP across IoU thresholds range from small method",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"AP50\", \"Score\": \"60.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"AP75\", \"Score\": \"44.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APL\", \"Score\": \"54.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APM\", \"Score\": \"45.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APS\", \"Score\": \"23.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"box AP\", \"Score\": \"41.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"AP50\", \"Score\": \"58.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"AP75\", \"Score\": \"42.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APL\", \"Score\": \"51.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APM\", \"Score\": \"43.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APS\", \"Score\": \"22.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"box AP\", \"Score\": \"39.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP50\", \"Score\": \"63.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP75\", \"Score\": \"46.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APL\", \"Score\": \"55.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APM\", \"Score\": \"46.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APS\", \"Score\": \"25.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"box AP\", \"Score\": \"43.2\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"PASCAL VOC 2007\", \"Metric\": \"MAP\", \"Score\": \"82.6%\"}}, { \"LEADERBOARD\": { \"Task\": \"Real-Time Object Detection\", \"Dataset\": \"PASCAL VOC 2007\", \"Metric\": \"FPS\", \"Score\": \"24\"}}, { \"LEADERBOARD\": { \"Task\": \"Real-Time Object Detection\", \"Dataset\": \"PASCAL VOC 2007\", \"Metric\": \"MAP\", \"Score\": \"82.4%\"}}, { \"LEADERBOARD\": { \"Task\": \"Real-Time Object Detection\", \"Dataset\": \"PASCAL VOC 2007\", \"Metric\": \"FPS\", \"Score\": \"19.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Real-Time Object Detection\", \"Dataset\": \"PASCAL VOC 2007\", \"Metric\": \"MAP\", \"Score\": \"82.1%\"}} ]"
    },
    {
        "Context": "End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion Knowledge graph embedding has been an active research topic for knowledge base completion, with progressive improvement from the initial TransE, TransH, DistMult et alto the current state-of-the-art ConvE. ConvE uses 2D convolution over embeddings and multiple layers of nonlinear features to model knowledge graphs. The model can be efficiently trained and scalable to large knowledge graphs. However, there is no structure enforcement in the embedding space of ConvE. The recent graph convolutional network (GCN) provides another way of learning graph node embedding by successfully utilizing graph connectivity structure. In this work, we propose a novel end-to-end Structure-Aware Convolutional Network (SACN) that takes the benefit of GCN and ConvE together. SACN consists of an encoder of a weighted graph convolutional network (WGCN), and a decoder of a convolutional network called Conv-TransE. WGCN utilizes knowledge graph node structure, node attributes and edge relation types. It has learnable weights that adapt the amount of information from neighbors used in local aggregation, leading to more accurate embeddings of graph nodes. Node attributes in the graph are represented as additional nodes in the WGCN. The decoder Conv-TransE enables the state-of-the-art ConvE to be translational between entities and relations while keeps the same link prediction performance as ConvE. We demonstrate the effectiveness of the proposed SACN on standard FB15k-237 and WN18RR datasets, and it gives about 10% relative improvement over the state-of-theart ConvE in terms of HITS@1, HITS@3 and HITS@10. Three benchmark datasets (FB15k-237, WN18RR and FB15k-237-Attr) are utilized in this study to evaluate the performance of link prediction The FB15k-237 dataset contains knowledge base relation triples and textual mentions of Freebase entity pairs, as used in the work published in For different datasets, we have found that the following settings work well: for FB15k-237, set the dropout to 0.2, number of kernels to 100, learning rate to 0.003 and embedding size to 200 for SACN; for WN18RR dataset, set dropout to 0.2, number of kernels to 300, learning rate to 0.003, and embedding size to 200 for SACN Each dataset is split into three sets for: training, validation and testing, which is same with the setting of the original ConvE Table 2: Statistics of datasets. FB15k - 237 WN18RR FB15k - 237 - Attr 14 , 744 350 , 449 20 , 466 14 , 541 78 , 334 17 , 535 Table 3: Link prediction for FB15k-237, WN18RR and FB15k-237-Attr datasets. FB15k - 237 @1 Hits @3 WN18RR MRR - @10 Table 4: Kernel size analysis for FB15k-237 and FB15k- 237-Attr datasets. \"SACN+Attr\" means the SACN using FB15k-237-Attr dataset. Hits FB15k - 237 @1 @3 MRR @10 Table 5: Node indegree study using FB15k-237 dataset. @3 Average Hits Conv - TransE @10 SACN",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@1\", \"Score\": \"0.43\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@10\", \"Score\": \"0.54\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@3\", \"Score\": \"0.48\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"MRR\", \"Score\": \"0.47\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"Hits@1\", \"Score\": \"0.26\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"Hits@10\", \"Score\": \"0.54\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"Hits@3\", \"Score\": \"0.39\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"MRR\", \"Score\": \"0.35\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@1\", \"Score\": \"0.443\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@10\", \"Score\": \"0.546\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@3\", \"Score\": \"0.494\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"MR\", \"Score\": \"3533\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"MRR\", \"Score\": \"0.479\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"Hits@1\", \"Score\": \"0.264\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"Hits@10\", \"Score\": \"0.535\"}}, { \"LEADERBOARD\": { \"Task\": "
    },
    {
        "Context": "Multimodal Differential Network for Visual Question Generation Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations. Images can have multiple visual and language contexts that are relevant for generating questions namely places, captions, and tags. In this paper, we propose the use of exemplars for obtaining the relevant context. We obtain this by using a Multimodal Differential Network to produce natural and engaging questions. The generated questions show a remarkable similarity to the natural questions as validated by a human study. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU, METEOR, ROUGE, and CIDEr). We conduct our experiments on Visual Question Generation (VQG) dataset, which contains human annotated questions based on images of MS-COCO dataset This dataset was developed for generating natural and engaging questions based on commonsense reasoning We use VQG-COCO dataset for our experiments which contains a total of 2500 training images, 1250 validation images, and 1250 testing images Each image in the dataset contains five natural questions and five ground truth captions It is worth noting that the work of (Jain et al., 2017) also used the questions from VQA dataset for training purpose, whereas the work by (Mostafazadeh et al., 2016) uses only the VQG-COCO dataset VQA-1.0 dataset is also built on images from MS-COCO dataset We used pretrained caption generation model to extract captions for VQA dataset as the human annotated captions are not therein the dataset We also get good results on the VQA dataset (as shown in) which Table 1: Analysis of variants of our proposed method on VQG-COCO Dataset as mentioned in section 4.4 and different ways of getting a joint embedding (Atten- tion (AtM), Hadamard (HM), Addition (AM) and Joint (JM) method as given in section 4.1.3) for each method. Refer section 5.1 for more details. variant . Joint Method ( JM ) of combining the embeddings BLEU1 METEOR Among the ablations , the proposed MDN CIDEr ROUGE Table 2: State-of-the-Art comparison on VQA-1.0 Dataset. The first block consists of the state-of-the-art results, second block refers to the baselines mentioned in section 5.2, third block provides the results for the variants of mixture module present in section 4.1.3. BLEU1 METEOR CIDEr ROUGE Table 3: State-of-the-Art (SOTA) comparison on VQG- COCO Dataset. The first block consists of the SOTA results, second block refers to the baselines mentioned in section 5.2, third block shows the results for the best",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Question Generation\", \"Dataset\": \"Visual Question Generation\", \"Metric\": \"BLEU-1\", \"Score\": \"36.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Question Generation\", \"Dataset\": \"COCO Visual Question Answering (VQA) real images 1.0 open ended\", \"Metric\": \"BLEU-1\", \"Score\": \"65.1\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Visual Question Generation\", \"Dataset\": \"VQA v2 test-std\", \"Metric\": \"overall\", \"Score\": \"80.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Question Generation\", \"Dataset\": \"VQA v2 test-std\", \"Metric\": \"overall\", \"Score\": \"70.7\"}} ]"
    },
    {
        "Context": "Unsupervised Scene Adaptation with Memory Regularization in vivo We consider the unsupervised scene adaptation problem of learning from both labeled source data and unlabeled target data. Existing methods focus on minoring the inter-domain gap between the source and target domains. However, the intra-domain knowledge and inherent uncertainty learned by the network are under-explored. In this paper, we propose an orthogonal method, called memory regularization in vivo to exploit the intradomain knowledge and regularize the model training. Specifically, we refer to the segmentation model itself as the memory module, and minor the discrepancy of the two classifiers, i.e., the primary classifier and the auxiliary classifier, to reduce the prediction inconsistency. Without extra parameters, the proposed method is complementary to most existing domain adaptation methods and could generally improve the performance of existing methods. Albeit simple, we verify the effectiveness of memory regularization on two synthetic-to-real benchmarks: GTA5 \u2192 Cityscapes and SYNTHIA \u2192 Cityscapes, yielding +11.1% and +11.3% mIoU improvement over the baseline model, respectively. Besides, a similar +12.0% mIoU improvement is observed on the cross-city benchmark: Cityscapes \u2192 Oxford RobotCar. We mainly evaluate the proposed method on the two unsupervised scene adaption settings, i.e., GTA5 Table 1: Ablation study of the memory regularization on both clas- sifiers, i.e., the auxiliary classifier and the primary classifier, in the Stage-I training. The result suggests that the memory regularization helps both classifiers, especially the auxiliary classifier. The final results of the full model combine the results of both classifiers, and therefore improve the performance further. without Lmr with Lmr Table 2: Ablation study of different losses in the Stage-I training. We gradually add the adversarial loss L adv and the memory regular- ization Lmr into consideration. mIoU Table 3: Ablation study of different losses in the Stage-II training. The result suggests that the memory regularization could prevent the model from overfitting to the noise in the pseudo labels. mIoU Table 4: Quantitative results on GTA5 \u2192 Cityscapes. We present pre-class IoU and mIoU. The best accuracy in every column is in bold. DRN - 26 DeepLabv2 DRN - 105",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"GTAV-to-Cityscapes Labels\", \"Metric\": \"mIoU\", \"Score\": \"48.3\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Synthetic-to-Real Translation\", \"Dataset\": \"GTAV-to-Cityscapes Labels\", \"Metric\":\"mIoU\", \"Score\": \"50.3\"}} ]"
    },
    {
        "Context": "Predictability of Fixed-Job Priority Schedulers on Heterogeneous Multiprocessor Real-Time Systems The multiprocessor Fixed-Job Priority (FJP) scheduling of real-time systems is studied. An important property for the schedulability analysis, the predictability (regardless to the execution times), is studied for heterogeneous multiprocessor platforms. Our main contribution is to show that any FJP schedulers are predictable on unrelated platforms. A convenient consequence is the fact that any FJP schedulers are predictable on uniform multiprocessors.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Segatron: Segment-Aware Transformer for Language Modeling and Understanding Transformers are powerful for sequence modeling. Nearly all state-of-the-art language models and pre-trained language models are based on the Transformer architecture. However, it distinguishes sequential tokens only with the token position index. We hypothesize that better contextual representations can be generated from the Transformer with richer positional information. To verify this, we propose a segmentaware Transformer (Segatron), by replacing the original token position encoding with a combined position encoding of paragraph, sentence, and token. We first introduce the segmentaware mechanism to Transformer-XL, which is a popular Transformer-based language model with memory extension and relative position encoding. We find that our method can further improve the Transformer-XL base model and large model, achieving 17.1 perplexity on the WikiText-103 dataset. We further investigate the pre-training masked language modeling task with Segatron. Experimental results show that BERT pre-trained with Segatron (SegaBERT) can outperform BERT with vanilla Transformer on various NLP tasks, and outperforms RoBERTa on zero-shot sentence representation learning. Our code is available on GitHub. 1  Table 1: Comparison with Transformer-XL and competitive baseline results on WikiText-103. PPL Table 2: Ablation over the position encodings using Transformer-XL base architecture. PPL Table 2. From this table, we find that the PPL of Transformer-XL decreases from 24.35 to 24.07/22.51 after adding paragraph/sentence SegaBERT language modeling . 35 this table , we find that the PPL of Transformer - XL decreases Loss Segatron - XL 0 Transformer - XL Training Steps 5k 15k Loss BERT 10k 150 PPL 20k Table 3: Fair comparison on GLUE dev. The two base models are pre-trained in the same setting. For large models comparison, we choose the best of 3 BERT-large models: the original BERT, whole word masking BERT, and BERT without NSP task. Results of BERT-large (best of 3) are from Yang et al. (2019). \u2212 QQP CoLA RTE AVG SST - 2 MNLI QNLI MRPC STS - B Table 4: Results on",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-103\", \"Metric\": \"Number of params\", \"Score\": \"257M\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-103\", \"Metric\": \"Test perplexity\", \"Score\": \"17.1\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Language Modelling\", \"Dataset\": \"enwik8\", \"Metric\": \"Bit per Character (BPC)\", \"Score\": \"0.91\"}}, { \"LEADERBOARD\": { \"Task\": \"Language Modelling\", \"Dataset\": \"enwik8\", \"Metric\": \"Number of params\", \"Score\": \"277M\"}}, { \"LEADERBOARD\": { \"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-103\", \"Metric\": \"Number of params\", \"Score\": \"277M\"}}, { \"LEADERBOARD\": { \"Task\": \"Language Modelling\", \"Dataset\": \"WikiText-103\", \"Metric\": \"Test perplexity\", \"Score\": \"17.4\"}} ]"
    },
    {
        "Context": "4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks In many robotics and VR/AR applications, 3D-videos are readily-available sources of input (a continuous sequence of depth images, or LIDAR scans). However, these 3D-videos are processed frame-by-frame either through 2D convnets or 3D perception algorithms in many cases. In this work, we propose 4-dimensional convolutional neural networks for spatio-temporal perception that can directly process such 3D-videos using high-dimensional convolutions. For this, we adopt sparse tensors [8, 9]  and propose the generalized sparse convolution which encompasses all discrete convolutions. To implement the generalized sparse convolution, we create an open-source auto-differentiation library for sparse tensors that provides extensive functions for highdimensional convolutional neural networks. 1  We create 4D spatio-temporal convolutional neural networks using the library and validate them on various 3D semantic segmentation benchmarks and proposed 4D datasets for 3D-video perception. To overcome challenges in the high-dimensional 4D space, we propose the hybrid kernel, a special case of the generalized sparse convolution, and the trilateral-stationary conditional random field that enforces spatio-temporal consistency in the 7D space-time-chroma space. Experimentally, we show that convolutional neural networks with only generalized sparse convolutions can outperform 2D or 2D-3D hybrid methods by a large margin. 2 Also, we show that on 3D-videos, 4D spatio-temporal convolutional neural networks are robust to noise, outperform 3D convolutional neural networks and are faster than the 3D counterpart in some cases. Next, we create multiple 4D datasets from 3D datasets that have temporal sequences and analyze each of the proposed components for ablation study For evaluation, we use the standard mean Intersection over Union (mIoU) and mean Accuracy (mAcc) for metrics following the previous works The dataset contains 3D scans of six floors of three different buildings The RueMonge 2014 dataset provides semantic labels fora multi-view 3D reconstruction of the Rue Mongue To create a 4D dataset, we crop the 3D reconstruction on-the-fly to generate a temporal sequence We use the Synthia dataset to create 3D video sequences Since the dataset is purely synthetic, we added various noise to the input point clouds to simulate noisy observations ScanNet & Stanford 3D Indoor The ScanNet and the Stanford Indoor datasets are one of the largest non-synthetic datasets, which make the datasets ideal test beds for 3D segmentation For all evaluation, we feed an Table 1: 3D Semantic Label Benchmark on ScanNet \u2020 [5] Method mIOU Table 1 : 3D Semantic Label Benchmark on ScanNet \u2020 [ 5 ] Table 2: Segmentation results on the 4D Synthia dataset Table 2 : Segmentation results on the 4D Synthia dataset mIOU mAcc Table 3: Segmentation results on the noisy Synthia 4D dataset Table 4 : Stanford Area 5 Test ( Fold #1 ) ( S3DIS ) [ 2 ] Building Road mAcc Pedestrian mIoU mIOU Lanemarking Car Vegetation Traffic Sign Table 3 : Segmentation results on the noisy Synthia 4D dataset Sidewalk Fence Traffic Light Pole Table 4: Stanford Area 5 Test (Fold #1) (S3DIS) [2] Table 4 : Stanford Area 5 Test ( Fold #1 ) ( S3DIS ) [ 2 ] Building Road mAcc Pedestrian mIoU mIOU Lanemarking Car Vegetation Traffic Sign Table 3 : Segmentation results on the noisy Synthia 4D dataset Sidewalk Fence",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"S3DIS Area5\", \"Metric\": \"mAcc\", \"Score\": \"71.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"S3DIS Area5\", \"Metric\": \"mIoU\", \"Score\": \"65.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"S3DIS\", \"Metric\": \"Mean IoU\", \"Score\": \"65.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"ScanNet\", \"Metric\": \"3DIoU\", \"Score\": \"0.734\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Semantic Segmentation\", \"Dataset\": \"3DR2N2\", \"Metric\":\"mAcc\", \"Score\": \"89.3%\"}} ]"
    },
    {
        "Context": "Path-Level Network Transformation for Efficient Architecture Search We introduce anew function-preserving transformation for efficient neural architecture search. This network transformation allows reusing previously trained networks and existing successful architectures that improves sample efficiency. We aim to address the limitation of current network transformation operations that can only perform layer-level architecture modifications, such as adding (pruning) filters or inserting (removing) a layer, which fails to change the topology of connection paths. Our proposed path-level transformation operations enable the meta-controller to modify the path topology of the given network while keeping the merits of reusing weights, and thus allow efficiently designing effective structures with complex path topologies like Inception models. We further propose a bidirectional treestructured reinforcement learning meta-controller to explore a simple yet highly expressive treestructured architecture space that can be viewed as a generalization of multi-branch architectures. We experimented on the image classification datasets with limited computational resources (about 200 GPU-hours), where we observed improved parameter efficiency and better test results (97.70% test accuracy on CIFAR-10 with 14.3M parameters and 74.6% top-1 accuracy on ImageNet in the mobile setting), demonstrating the effectiveness and transferability of our designed architectures. Specifically, we apply the proposed method described above to learn CNN cells on CIFAR-10 () for the image classification task and transfer the learned cell structures to ImageNet dataset) We use a standard data augmentation scheme (mirroring/shifting) that is widely used for this dataset and normalize the images using channel means and standard deviations for preprocessing Table 1. Test error rate (%) results of our best discovered architectures as well as state-of-the-art human-designed and automatically designed architectures on CIFAR-10. If \"Reg\" is checked, additional regularization techniques (e.g., Shake-Shake Auto are utilized when training the networks . designed and Cutout ( DeVries & Taylor , 2017 ) ) , along with a longer training schedule ( 600 epochs or 1800 epochs ) Test error Params Table 2. Top-1 (%) and Top-5 (%) classification error rate results on ImageNet in the M obile Setting (\u2264 600M multiply-add opera- tions). \"\u00d7+\" denotes the number of multiply-add operations. Table 3. Start point network with identity mappings on CIFAR-10. 192 Feature map size 48 16 \u00d7 16 [ identity mapping ] \u00d74 Model architecture 192 32 \u00d7 32 96",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Neural Architecture Search\", \"Dataset\": \"CIFAR-10 Image Classification\", \"Metric\": \"Params\", \"Score\": \"14.3M\"}}, {\"LEADERBOARD\": {\"Task\": \"Neural Architecture Search\", \"Dataset\": \"CIFAR-10 Image Classification\", \"Metric\": \"Percentage error\", \"Score\": \"2.30\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of params\", \"Score\": \"86.1M\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"74.66%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"96.32%\"}} ]"
    },
    {
        "Context": "Focusing and Polarization in Intuitionistic Logic A focused proof system provides a normal form to cut-free proofs that structures the application of invertible and non-invertible inference rules. The focused proof system of Andreoli for linear logic has been applied to both the proof search and the proof normalization approaches to computation. Various proof systems in literature exhibit characteristics of focusing to one degree or another. We present anew, focused proof system for intuitionistic logic, called LJF, and show how other proof systems can be mapped into the new system by inserting logical connectives that prematurely stop focusing. We also use LJF to design a focused proof system for classical logic. Our approach to the design and analysis of these systems is based on the completeness of focusing in linear logic and on the notion of polarity that appears in Girard's LC and LU proof systems.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters Due to the compelling improvements brought by BERT, many recent representation models adopted the Transformer architecture as their main building block, consequently inheriting the wordpiece tokenization system despite it not being intrinsically linked to the notion of Transformers. While this system is thought to achieve a good balance between the flexibility of characters and the efficiency of full words, using predefined wordpiece vocabularies from the general domain is not always suitable, especially when building models for specialized domains (e.g., the medical domain). Moreover, adopting a wordpiece tokenization shifts the focus from the word level to the subword level, making the models conceptually more complex and arguably less convenient in practice. For these reasons, we propose CharacterBERT, anew variant of BERT that drops the wordpiece system altogether and uses a Character-CNN module instead to represent entire words by consulting their characters. We show that this new model improves the performance of BERT on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations. 9  More specifically, we adapt these scripts to our needs. 10  We use gradient accumulation for larger batch sizes. We compare BERT and CharacterBERT on multiple medical tasks to evaluate the impact of using a Character-CNN module instead of wordpieces Given all the pre-trained models, the evaluation tasks, and a set of random seeds i \u2208 1..10: 1 We choose a pre-trained model, an evaluation task, and a random seed i then run 15 training epochs with batches of size 32 At each epoch, we evaluate the model on a validation set that is either given or computed as 20% of the training set After completing all training epochs, we load the best model and evaluate it on the test set Table 2: Statistics on pre-training corpora. 8 9 # documents # tokens Table 3: Number of examples of each evaluation task. ClinicalSTS",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Relation Extraction\", \"Dataset\": \"ChemProt\", \"Metric\": \"Micro F1\", \"Score\": \"73.44\"}}, {\"LEADERBOARD\": {\"Task\": \"Natural Language Inference\", \"Dataset\": \"MedNLI\", \"Metric\": \"Accuracy\", \"Score\": \"84.95\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Similarity\", \"Dataset\": \"ClinicalSTS\", \"Metric\": \"Pearson Correlation\", \"Score\": \"85.62\"}}, {\"LEADERBOARD\": {\"Task\": \"Drug\u2013drug Interaction Extraction\", \"Dataset\": \"DDI extraction 2013 corpus\", \"Metric\": \"Micro F1\", \"Score\": \"80.38\"}}, {\"LEADERBOARD\": {\"Task\": \"Clinical Concept Extraction\", \"Dataset\": \"2010 i2b2/VA\", \"Metric\": \"Exact Span F1\", \"Score\": \"89.24\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Multi-target regression\", \"Dataset\": \"USHCN-Daily\", \"Metric\": \"Micro F1\", \"Score\": \"84.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Multi-target regression\", \"Dataset\": \"MIMIC-III\", \"Metric\": \"Micro F1\", \"Score\": \"82.75\"}}, { \"LEADERBOARD\": { \"Task\": \"Multi-target regression\", \"Dataset\": \"MIMIC-III\", \"Metric\": \"Micro F1\", \"Score\": \"80.44\"}}, { \"LEADERBOARD\": { \"Task\": \"Multi-target regression\", \"Dataset\": \"Multi-day Continuous metricFU\", \"Metric\": \"Micro F1\", \"Score\": \"83.03\"}}, { \"LEADERBOARD\": { \"Task\": \"Multi-target regression\", \"Dataset\": \"4.4\", \"Metric\": \"Micro F1\", \"Score\": \"82.81\"}} ]"
    },
    {
        "Context": "As Time Goes By: Constraint Handling Rules A Survey of CHR Research from 1998 to 2007 Constraint Handling Rules (CHR) is a high-level programming language based on multiheaded multiset rewrite rules. Originally designed for writing user-defined constraint solvers, it is now recognized as an elegant general purpose language.CHR-related research has surged during the decade following the previous survey by Fr\u00fchwirth (1998). Covering more than 180 publications, this new survey provides an overview of recent results in a wide range of research areas, from semantics and analysis to systems, extensions and applications.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "END-TO-END SPEECH RECOGNITION WITH ADAPTIVE COMPUTATION STEPS In this paper, we present Adaptive Computation Steps (ACS) algorithm, which enables end-to-end speech recognition models to dynamically decide how many frames should be processed to predict a linguistic output. The model that applies ACS algorithm follows the encoder-decoder framework, while unlike the attention-based models, it produces alignments independently at the encoder side using the correlation between adjacent frames. Thus, predictions can be made as soon as sufficient acoustic information is received, which makes the model applicable in online cases. Besides, a small change is made to the decoding stage of the encoder-decoder framework, which allows the prediction to exploit bidirectional contexts. We verify the ACS algorithm on a Mandarin speech corpus AIShell-1, and it achieves a 31.2% CER in the online occasion, compared to the 32.4% CER of the attention-based model. To fully demonstrate the advantage of ACS algorithm, offline experiments are conducted, in which our ACS model achieves an 18.7% CER, outperforming the attention-based counterpart with the CER of 22.0%.  Table 1. Character Error Rate (CER) on HMM-DNN and end-to- end models. The results of attention-based and ACS models were decoded using beam search algorithm with the width of 8. HMM - Hybrid Models Offline Character Models Online Character Models CER",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Speech Recognition\", \"Dataset\": \"AISHELL-1\", \"Metric\": \"Word Error Rate (WER)\", \"Score\": \"18.7\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Speech Recognition\", \"Dataset\": \"AISHdar\", \"Metric\": \"Word Error Rate (WER)\", \"Score\": \"2.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Speech Recognition\", \"Dataset\": \"AISHdar\", \"Metric\": \"Word Error Rate (WER)\", \"Score\": \"2\"}} ]"
    },
    {
        "Context": "Bounded Underapproximations We show anew and constructive proof of the following language-theoretic result: for every context-free language L, there is a bounded context-free language L \u2286 L which has the same Parikh (commutative) image as L. Bounded languages, introduced by Ginsburg and Spanier, are subsets of regular languages of the form w * 1 w * 2 \u00b7 \u00b7 \u00b7 w * k for some w1, . . . , wk \u2208 \u03a3 * . In particular bounded subsets of context-free languages have nice structural and decidability properties. Our proof proceeds in two parts. First, using Newton's iterations on the language semiring, we construct a context-free subset LN of L that can be represented as a sequence of substitutions on a linear language and has the same Parikh image as L. Second, we inductively construct a Parikh-equivalent bounded context-free subset of LN . We show two applications of this result in model checking: to underapproximate the reachable state space of multithreaded procedural programs and to underapproximate the reachable state space of recursive counter programs. The bounded language constructed above provides a decidable underapproximation for the original problems. By iterating the construction, we get a semi-algorithm for the original problems that constructs a sequence of underapproximations such that no two underapproximations of the sequence can be compared. This provides a progress guarantee: every word w \u2208 L is in some underapproximation of the sequence, and hence, a program bug is guaranteed to be found. In particular, we show that verification with bounded languages generalizes context-bounded reachability for multithreaded programs.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing textto-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256\u00d7256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions. To validate our method, we conduct extensive quantitative and qualitative evaluations Since 80% of birds in this dataset have object-image size ratios of less than 0.5, as a pre-processing step, we crop all images to ensure that bounding boxes of birds have greater-than-0.75 object-image size ratios To show the generalization capability of our approach, a more challenging dataset, MS COCO is also utilized for evaluation Different from CUB and Oxford-102, the MS COCO dataset contains images with multiple objects and various backgrounds Each image in COCO has 5 descriptions, while 10 descriptions are provided by for every image in CUB and Oxford-102 datasets Evaluation metrics It is difficult to evaluate the performance of generative models (e.g., GAN) We choose a recently proposed numerical assessment approach \"inception score\" for quantitative evaluation, where x denotes one generated sample, and y is the label predicted by the Inception model In our experiments, we directly Table 1. Inception scores and average human ranks of our Stack- GAN, GAWWN [24], and GAN-INT-CLS [26] on CUB, Oxford- 102, and MS-COCO datasets. GAWWN Our StackGAN GAN - INT - CLS Table 1. Representative examples are compared in GAWWN Our StackGAN GAN - INT - CLS Table 2. Inception scores calculated with 30,000 samples gener- ated by different baseline models of our StackGAN. 256\u00d7256 Stage - I GAN with different poses and viewpoints . wings and pointy beak This bird is completely red with black Inception score",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Text-to-Image Generation\", \"Dataset\": \"Oxford 102 Flowers\", \"Metric\": \"Inception score\", \"Score\": \"3.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Text-to-Image Generation\", \"Dataset\": \"CUB\", \"Metric\": \"Inception score\", \"Score\": \"3.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Text-to-Image Generation\", \"Dataset\": \"COCO\", \"Metric\": \"Inception score\", \"Score\": \"8.45\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"LSUN Bedroom 64 x 64\", \"Metric\": \"FID\", \"Score\": \"6.47\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"Oxford 102 Flowers\", \"Metric\": \"FID\", \"Score\": \"80.24\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"CUB 128 x 128\", \"Metric\": \"FID\", \"Score\": \"2011.44\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"CUB 128 x 128\", \"Metric\": \"Inception score\", \"Score\": \"3.78\"}} ]"
    },
    {
        "Context": "Forming the COUNCIL Based Clusters in Securing Wireless Ad Hoc Networks In cluster-based routing protocol (CBRP), two-level hierarchical structure is successfully used to reduce over-flooding in wireless ad hoc networks. As it is vulnerable to a single point of failure , we propose anew adaptive distributed threshold scheme to replace the cluster head by a group of cluster heads within each cluster, called COUNCIL, and distribute the service of single cluster head to multiple cluster heads using (k,n) threshold secret sharing scheme. An ad hoc network formed by COUNCIL based clusters can work correctly when the number of compromised cluster heads is smaller thank. To implement this adaptive threshold scheme in wireless ad hoc netw orks, membership of the clusters should be defined in an adaptive way. In this paper, we mainly discuss our algorithm for forming COUNCIL based clusters using the concept of dominating set from graph theory.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Self-Supervised Monocular Scene Flow Estimation . Results of our monocular scene flow approach on the KITTI dataset [11]. Given two consecutive images (left), our method jointly predicts depth (middle) and scene flow (right). (x,z)-coordinates of 3D scene flow are visualized using an optical flow color coding.Scene flow estimation has been receiving increasing attention for 3D environment perception. Monocular scene flow estimation -obtaining 3D structure and 3D motion from two temporally consecutive images -is a highly illposed problem, and practical solutions are lacking to date. We propose a novel monocular scene flow method that yields competitive accuracy and real-time performance. By taking an inverse problem view, we design a single convolutional neural network (CNN) that successfully estimates depth and 3D motion simultaneously from a classical optical flow cost volume. We adopt self-supervised learning with 3D loss functions and occlusion reasoning to leverage unlabeled data. We validate our design choices, including the proxy loss and augmentation setup. Our model achieves state-of-the-art accuracy among unsupervised/self-supervised learning approaches to monocular scene flow, and yields competitive results for the optical flow and monocular depth estimation sub-tasks. Semi-supervised fine-tuning further improves the accuracy and yields promising results in real-time.  Table 1. Impact of geometric augmentations (Aug.) and CAM- Convs (CC.) D2 - all F1 - all Sq . Rel . SF1 - all Monocular scene flow Abs . Rel . Monocular depth D1 - all Table 2. Ablation study on the loss function: based on the Ba- sic 2D loss consisting of photometric and smoothness loss, the 3D point reconstruction loss (3D points) improves scene flow accu- racy, especially when discarding occluded pixels in the loss (Occ.). D2 - all F1 - all SF1 - all D1 - all Table 3. Single decoder vs. separate decoders: using a single decoder yields stable training and comparable accuracy on both tasks to models that target each individual task separately. 100 - D2 - all F1 - all SF1 - all D1 - all - Table 4. Monocular scene flow evaluation on KITTI Scene Flow Training: our self-supervised learning approach significantly out- performs",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Scene Flow Estimation\", \"Dataset\": \"KITTI 2015 Scene Flow Training\", \"Metric\": \"Runtime (s)\", \"Score\": \"0.09\"}}, {\"LEADERBOARD\": {\"Task\": \"Scene Flow Estimation\", \"Dataset\": \"KITTI 2015 Scene Flow Training\", \"Metric\": \"D1-all\", \"Score\": \"31.25\"}}, {\"LEADERBOARD\": {\"Task\": \"Scene Flow Estimation\", \"Dataset\": \"KITTI 2015 Scene Flow Training\", \"Metric\": \"D2-all\", \"Score\": \"34.86\"}}, {\"LEADERBOARD\": {\"Task\": \"Scene Flow Estimation\", \"Dataset\": \"KITTI 2015 Scene Flow Training\", \"Metric\": \"Fl-all\", \"Score\": \"23.49\"}}, {\"LEADERBOARD\": {\"Task\": \"Scene Flow Estimation\", \"Dataset\": \"KITTI 2015 Scene Flow Training\", \"Metric\": \"SF-all\", \"Score\": \"47.05\"}}, {\"LEADERBOARD\": {\"Task\": \"Scene Flow Estimation\", \"Dataset\": \"KITTI 2015 Scene Flow Test\", \"Metric\": \"D1-all\", \"Score\": \"34.02\"}}, {\"LEADERBOARD\": {\"Task\": \"Scene Flow Estimation\", \"Dataset\": \"KITTI 2015 Scene Flow Test\", \"Metric\": \"D2-all\", \"Score\": \"36.34\"}}, {\"LEADERBOARD\": {\"Task\": \"Scene Flow Estimation\", \"Dataset\": \"KITTI 2015 Scene Flow Test\", \"Metric\": \"Fl-all\", \"Score\": \"23.54\"}}, {\"LEADERBOARD\": {\"Task\": \"Scene Flow Estimation\", \"Dataset\": \"KITTI 2015 Scene Flow Test\", \"Metric\": \"Runtime (s)\", \"Score\": \"0.09\"}}, {\"LEADERBOARD\": {\"Task\": \"Scene Flow Estimation\", \"Dataset\": \"KITTI 2015 Scene Flow Test\", \"Metric\": \"SF-all\", \"Score\": \"49.54\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Scene Flow Estimation\", \"Dataset\": \"KITTI 2015 Scene Flow Training\", \"Metric\": \"Runtime (s)\", \"Score\": \"0.05\"}}, { \"LEADERBOARD\": { \"Task\": \"Scene Flow Estimation\", \"Dataset\": \"KITTI 2015 Scene Flow Training\", \"Metric\": \"D1-all\", \"Score\": \"26.81\"}}, { \"LEADERBOARD\": { \"Task\": \"Scene Flow Estimation\", \"Dataset\": \"KITTI 2015 Scene Flow Training\", \"Metric\": \"D2-all\", \"Score\": \"60.97\"}}, { \"LEADERBOARD\": { \"Task\": \"Scene Flow Estimation\", \"Dataset\": \"KITTI 2015 Scene Flow Training\", \"Metric\": \"Fl-all\", \"Score\": \"25.74\"}}, { \"LEADERBOARD\": { \"Task\": \"Scene Flow Estimation\", \"Dataset\": \"KITTI 2015 Scene Flow Training\", \"Metric\": \"SF-all\", \"Score\": \"(>60.97)\"}} ]"
    },
    {
        "Context": "0.5 Petabyte Simulation of a 45-Qubit Quantum Circuit Near-term quantum computers will soon reach sizes that are challenging to directly simulate, even when employing the most powerful supercomputers. Yet, the ability to simulate these early devices using classical computers is crucial for calibration, validation, and benchmarking. In order to make use of the full potential of systems featuring multi-and many-core processors, we use automatic code generation and optimization of compute kernels, which also enables performance portability. We apply a scheduling algorithm to quantum supremacy circuits in order to reduce the required communication and simulate a 45-qubit circuit on the Cori II supercomputer using 8, 192 nodes and 0.5 petabytes of memory. To our knowledge, this constitutes the largest quantum circuit simulation to this date. Our highly-tuned kernels in combination with the reduced communication requirements allow an improvement in time-to-solution over state-of-the-art simulations by more than an order of magnitude at every scale.\u2022 Applied computing \u2192 Physics; arXiv:1704.01127v2 [quant-ph]  Table 1: Re-scheduling of gates for depth-25 quantum supremacy circuits into clusters (using 30 local qubits). Clus- ters are built to contain k \u2264 k max qubits using a heuristic which tries to maximize the number of gates merged into one cluster. Clearly more than k max individual gates can be combined into one single cluster on average. These op- timizations take less than 3 seconds using Python and can be reused for all instance of the same size. 1000 of Gates of Qubits 58 Number Number of clusters k max = 3 82 73 98 Table 2: Results for all simulations carried out on Cori II. Circuit simulation time and speedup are given with respect to the depth-25 quantum supremacy circuit simulations per- formed in [5]. The comm.-column gives the percentage of circuit simulation time spent in communication and syn- chronization. {1024 , 2048 , 4096} nodes of Cori II",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Cartoon-To-Real Translation\", \"Dataset\": \"100 sleep nights of 8 caregivers\", \"Metric\": \"10%\", \"Score\": \"100\"}}]",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "An Improved Approximation Algorithm for the Column Subset Selection Problem * We consider the problem of selecting the \"best\" subset of exactly k columns from an m \u00d7 n matrix A. In particular, we present and analyze a novel two-stage algorithm that runs in O(min{mn 2 , m 2 n}) time and returns as output an m \u00d7 k matrix C consisting of exactly k columns of A. In the first stage (the randomized stage), the algorithm randomly selects \u0398(k log k) columns according to a judiciously-chosen probability distribution that depends on information in the top-k right singular subspace of A. In the second stage (the deterministic stage), the algorithm applies a deterministic column-selection procedure to select and return exactly k columns from the set of columns selected in the first stage. Let C be them \u00d7 k matrix containing those k columns, let PC denote the projection matrix onto the span of those columns, and let A k denote the \"best\" rank-k approximation to the matrix A as computed with the singular value decomposition. Then, we prove that, with probability at least 0.8,",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Temporal Context Aggregation Network for Temporal Action Proposal Refinement Candidate Proposal Segment-Level Regression Output Frame-Level Regression Output Fused Output LGTE TBR Background Skiing Background Segment-Level Regression Start-Boundary Regression End-Boundary Regression Internal Context Starting Context Ending Context Encoded Feature Local Temporal Encoder Global Temporal Encoder Temporal action proposal generation aims to estimate temporal intervals of actions in untrimmed videos, which is a challenging yet important task in the video understanding field. The proposals generated by current methods still suffer from inaccurate temporal boundaries and inferior confidence used for retrieval owing to the lack of efficient temporal modeling and effective boundary context utilization. In this paper, we propose Temporal Context Aggregation Network (TCANet) to generate high-quality action proposals through \"local and global\" temporal context aggregation and complementary as well as progressive boundary refinement. Specifically, we first design a Local-Global Temporal Encoder (LGTE), which adopts the channel grouping strategy to efficiently encode both \"local and global\" temporal inter-dependencies. Furthermore, both the boundary and internal context of proposals are adopted for framelevel and segment-level boundary regressions, respectively. Temporal Boundary Regressor (TBR) is designed to combine these two regression granularities in an end-to-end fashion, which achieves the precise boundaries and reliable confidence of proposals through progressive refinement. Extensive experiments are conducted on three challenging datasets: HACS, ActivityNet-v1.3, and THUMOS-14, where TCANet can generate proposals with high precision and recall. By combining with the existing action classifier, TCANet can obtain remarkable temporal action detection performance compared with other methods. Not surprisingly, the proposed TCANet won the 1 st place in the CVPR 2020 -HACS challenge leaderboard on temporal action localization task. HACS is a large-scale dataset for temporal action detection In our experiments, we compare TCANet with the state-of-the-art method on all three datasets and performed ablation studies on HACS dataset Evaluation Metrics The learning rates on these two datasets are set to 0.0004 and 0.001, and the batch size is 16 for 10 epochs Table 1. Comparison with state-of-the-art methods on HACS. The results are measured by mAP(%) at different tIoU thresholds and average mAP(%). * indicates our implementation. Average Table 2. Comparison between our TCANet with other state-of- the-arts methods on ActivityNet-v1.3. The results are measured by mAP(%) at different tIoU thresholds and average mAP(%). For fair comparisons, we combined our proposals with video-level classification results from [41]. * indicates the reproduced results. Average Table 3. Comparison between our TCANet with other state-of-the- art methods on THUMOS14 dataset. The results are measured by mAP(%) at different tIoU thresholds. We combined our proposals with video-level classifier UntrimmedNet [34]. with video - level classifier UntrimmedNet [ 34 ] . Table 4. Comparison of our TCANet with other state-of-the-art methods on THUMOS14 dataset in terms of AR@AN. set to {0 . 5 , 0 . 75 , 0 . 95} , and we also test the average",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Temporal Action Localization\", \"Dataset\": \"ActivityNet-1.3\", \"Metric\": \"mAP\", \"Score\": \"37.56\"}}, {\"LEADERBOARD\": {\"Task\": \"Temporal Action Localization\", \"Dataset\": \"ActivityNet-1.3\", \"Metric\": \"mAP IOU@0.5\", \"Score\": \"54.33\"}}, {\"LEADERBOARD\": {\"Task\": \"Temporal Action Localization\", \"Dataset\": \"ActivityNet-1.3\", \"Metric\": \"mAP IOU@0.75\", \"Score\": \"39.13\"}}, {\"LEADERBOARD\": {\"Task\": \"Temporal Action Localization\", \"Dataset\": \"ActivityNet-1.3\", \"Metric\": \"mAP IOU@0.95\", \"Score\": \"8.41\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Temporal Action Proposal Generation\", \"Dataset\": \"ActivityNet-1.3\", \"Metric\": \"AR@100\", \"Score\": \"73.73\"}}, { \"LEADERBOARD\": { \"Task\": \"Temporal Action Proposal Generation\", \"Dataset\": \"ActivityNet-1.3\", \"Metric\": \"AUC (val)\", \"Score\": \"65.72\"}} ]"
    },
    {
        "Context": "Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition This work introduces pyramidal convolution (PyConv), which is capable of processing the input at multiple filter scales. PyConv contains a pyramid of kernels, where each level involves different types of filters with varying size and depth, which are able to capture different levels of details in the scene. On top of these improved recognition capabilities, PyConv is also efficient and, with our formulation, it does not increase the computational cost and parameters compared to standard convolution. Moreover, it is very flexible and extensible, providing a large space of potential network architectures for different applications. PyConv has the potential to impact nearly every computer vision task and, in this work, we present different architectures based on PyConv for four main tasks on visual recognition: image classification, video action classification/recognition, object detection and semantic image segmentation/parsing. Our approach shows significant improvements overall these core tasks in comparison with the baselines. For instance, on image recognition, our 50-layers network outperforms in terms of recognition performance on ImageNet dataset its counterpart baseline ResNet with 152 layers, while having 2.39 times less parameters, 2.52 times lower computational complexity and more than 3 times less layers. On image segmentation, our novel framework sets anew state-of-the-art on the challenging ADE20K benchmark for scene parsing. Code is available at: https://github.com/iduta/pyconv For image classification task we perform our experiments on the commonly used ImageNet dataset For image segmentation we use ADE20K benchmark, which is one of the most challenging datasets for image segmentation/parsing We present in the ablation experiments results of the proposed Py-Conv for image recognition task on the ImageNet dataset where, using the network with 50 layers, we vary the number of levels of PyConv The works,, besides using a strong computational architecture with many GPUs, take advantage of a large dataset of 3.5B images collected from Instagram (this dataset is not publicly available) Table 1. For direct comparison we place aside also the baseline architecture ResNet ReLU PyConv Networks for Image Classification Table 1: PyConvResNet and PyConvHGResNet. 3\u00d73 max pool , s=2 ResNet - 50 7\u00d77 , 64 , s=2 Table 2: ImageNet ablation experiments of PyConvResNet. top - 1 ( % ) top - 5 ( % ) params GFLOPs Table 3: Validation error rates comparison results of PyConv on ImageNet with other architectures. ResNet [ 7 ] by a large margin on all depths . For instance , our PyConvResNet improves the top - 1 Network PyConvHGResNet - 152 train top - 5 45 PyConvHGResNet - 152 val 25 ResNet - 152 val top - 1 PyConvHGResNet - 101 val ResNet - 50 train error PyConvResNet - 50 train PyConvHGResNet - 50 train network depth : 152 PyConvHGResNet - 101 train PyConvResNet - 152 train 90 ResNet - 152 train PyConvHGResNet -",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"ADE20K val\", \"Metric\": \"Pixel Accuracy\", \"Score\": \"82.49\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"ADE20K val\", \"Metric\": \"mIoU\", \"Score\": \"45.99\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"ADE20K\", \"Metric\": \"Test Score\", \"Score\": \"56.52\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"ADE20K\", \"Metric\": \"Validation mIoU\", \"Score\": \"45.99\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of params\", \"Score\": \"42.3M\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"81.49%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"95.72%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"ADE20K val\", \"Metric\":\"mIoU\", \"Score\": \"45.3%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"ADE20K\", \"Metric\": \"Test Score\", \"Score\": \"62.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"ADE20K\", \"Metric\": \"Validation mIoU\", \"Score\": \"45.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"ADE20K\", \"Metric\": \"Test Score\", \"Score\": \"56.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"ADE20K\", \"Metric\": \"Validation mIoU\", \"Score\": \"44.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL Context\", \"Metric\":\"mIoU\", \"Score\": \"55.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL"
    },
    {
        "Context": "Cross-View Image Synthesis using Conditional GANs Learning to generate natural scenes has always been a challenging task in computer vision. It is even more painstaking when the generation is conditioned on images with drastically different views. This is mainly because understanding, corresponding, and transforming appearance and semantic information across the views is not trivial. In this paper, we attempt to solve the novel problem of cross-view image synthesis, aerial to street-view and vice versa, using conditional generative adversarial networks (cGAN). Two new architectures called Crossview Fork (X-Fork) and Crossview Sequential (X-Seq) are proposed to generate scenes with resolutions of 64\u00d764 and 256\u00d7256 pixels. X-Fork architecture has a single discriminator and a single generator. The generator hallucinates both the image and its semantic segmentation in the target view. X-Seq architecture utilizes two cGANs. The first one generates the target image which is subsequently fed to the second cGAN for generating its corresponding semantic segmentation map. The feedback from the second cGAN helps the first cGAN generate sharper images. Both of our proposed architectures learn to generate natural images as well as their semantic segmentation maps. The proposed methods show that they are able to capture and maintain the true semantics of objects in source and target views better than the traditional image-to-image translation method which considers only the visual appearance of the scene. Extensive qualitative and quantitative evaluations support the effectiveness of our frameworks, compared to two state of the art methods, for natural scene generation across drastically different views. For the experiments in this work, we use the cross-view image dataset provided by Vo et al. This dataset consists of more than one million pairs of street-view and overhead view images collected from 11 different cities in the US We call it Dayton Dataset The images in the original dataset have resolution of 354\u00d7354 We also recruit the CVUSA dataset for direct comparison of our work with Zhai et al. This dataset consists of 35,532/8,884 train/test split of image pairs To do so, we take the first quarter of the ground level images and segmentations from the dataset and resize them to 256 \u00d7 256 in our experiments Please see for some images from the CVUSA dataset The CVUSA dataset has annotated segmentation maps for ground view images, but for Dayton dataset such information is not available This network is pre-trained on outdoor scenes of the Cityscapes dataset and is Table 1: KL divergence scores between conditional and marginal probabilities (Inception Score). all 64\u00d764 classes CVUSA Top - 5 256\u00d7256 Top - 1 class - Table 2: Accuracies: Top-1 and Top-5. Accuracy ( % ) 64\u00d764 CVUSA Top - 5 256\u00d7256 Top - 1 - Methods Table 3: KL Divergence between model and data distributions. - Accuracy ( % ) 64\u00d764 CVUSA Top - 5 256\u00d7256 Top - 1 - Methods Table 3. As it can be seen, our proposed methods generate much better results than existing generative methods on both datasets. X-Fork generates images very similar to real distribution in all ex- - Accuracy ( % ) 64\u00d764 CVUSA Top - 5 256\u00d7256 Top - 1 - Methods Table 4: SSIM, PSNR and Sharpness Difference between real data and samples generated using different methods. SSIM PSNR 64\u00d764 CVUSA Sharp Diff 256\u00d7256 - Table 5: Evaluation Scores for segmentation maps. Per",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Cross-View Image-to-Image Translation\", \"Dataset\": \"Ego2Top\", \"Metric\": \"SSIM\", \"Score\": \"0.2740\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-View Image-to-Image Translation\", \"Dataset\": \"Ego2Top\", \"Metric\": \"SSIM\", \"Score\": \"0.2738\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-View Image-to-Image Translation\", \"Dataset\": \"cvusa\", \"Metric\": \"SSIM\", \"Score\": \"0.4356\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-View Image-to-Image Translation\", \"Dataset\": \"cvusa\", \"Metric\": \"SSIM\", \"Score\": \"0.4231\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-View Image-to-Image Translation\", \"Dataset\": \"Dayton (256\u00d7256) - ground-to-aerial\", \"Metric\": \"SSIM\", \"Score\": \"0.2725\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-View Image-to-Image Translation\", \"Dataset\": \"Dayton (256\u00d7256) - aerial-to-ground\", \"Metric\": \"SSIM\", \"Score\": \"0.5031\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-View Image-to-Image Translation\", \"Dataset\": \"Dayton (256\u00d7256) - aerial-to-ground\", \"Metric\": \"SSIM\", \"Score\": \"0.4963\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-View Image-to-Image Translation\", \"Dataset\": \"Dayton (64x64) - ground-to-aerial\", \"Metric\": \"SSIM\", \"Score\": \"0.3682\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-View Image-to-Image Translation\", \"Dataset\": \"Dayton (64x64) - ground-to-aerial\", \"Metric\": \"SSIM\", \"Score\": \"0.3663\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-View Image-to-Image Translation\", \"Dataset\": \"Dayton (64\u00d764) - aerial-to-ground\", \"Metric\": \"SSIM\", \"Score\": \"0.5171\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-View Image-to-Image Translation\", \"Dataset\": \"Dayton (64\u00d764) - aerial-to-ground\", \"Metric\": \"SSIM\", \"Score\": \"0.4921\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image-to-Image Translation\", \"Dataset\": \"Cityscapes-to-Foggy Cityscapes\", \"Metric\": \"FID\", \"Score\": \"-8N\"}}, { \"LEADERBOARD\": { \"Task\": \"Image-to-Image Translation\", \"Dataset\": \"Cityscapes-to-Foggy Cityscapes\", \"Metric\": \"Parameters\", \"Score\": \"65.6M\"}}, { \"LEADERBOARD\": { \"Task\": \"Image-to-Image Translation\", \"Dataset\": \"Cityscapes-to-Photo\", \"Metric\": \"FID\", \"Score\": \"95.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Image-to-Image Translation\", \"Dataset\": \"Cityscapes-to-Photo\", \"Metric\": \"Per-pixel Accuracy\", \"Score\": \"96.5%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image-to-Image Translation\", \"Dataset\": \"Cityscapes-to-Photo\", \"Metric\":\"mIoU\", \"Score\": \"65.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Image-to-Image Translation\", \"Dataset\": \"Cityscapes-to-Photo\", \"Metric\":\"mIoU\", \"Score\": \"63.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Image-to-Image Translation\","
    },
    {
        "Context": "DEEP CLUSTERING WITH A DYNAMIC AUTOENCODER: FROM RECONSTRUCTION TOWARDS CENTROIDS CONSTRUCTION A PREPRINT In unsupervised learning, there is no apparent straightforward cost function that can capture the significant factors of variations and similarities. Since natural systems have smooth dynamics, an opportunity is lost if an unsupervised objective function remains static during the training process. The absence of concrete supervision suggests that smooth dynamics should be integrated. Compared to classical static cost functions, dynamic objective functions allow to better make use of the gradual and uncertain knowledge acquired through pseudo-supervision. In this paper, we propose Dynamic Autoencoder (DynAE), a novel model for deep clustering that overcomes a clusteringreconstruction trade-off, by gradually and smoothly eliminating the reconstruction objective function in favor of a construction one. Experimental evaluations on benchmark datasets show that our approach achieves state-of-the-art results compared to the most relevant deep clustering methods. github.com/nairouz/DynAE We compare DynAE with several clustering algorithms on four famous datasets: MNIST-full, MNIST-test, USPS, and Fashion-MNIST \u2022 MNIST-full: A 10 classes dataset of 70000 samples \u2022 MNIST-test: A test subset of the MNIST-full dataset with 10000 data samples \u2022 USPS: A 10 classes dataset of 9298 samples \u2022 Fashion-MNIST: A 10 classes dataset of 70000 samples ACC and NMI are the most utilized evaluation metrics in the deep clustering literature Table 2: The ACC and NMI of different clustering approaches. Each category is separated from the other ones by a double horizontal line. -indicates that the program ran out of memory. Best method in bold, second best emphasized. ACC USPS MNIST - test Fashion - MNIST NMI MNIST - full - Table 3: The ACC and NMI of DEC*, IDEC* and DynAE. Best method in bold, second best emphasized. ACC USPS MNIST - test Fashion - MNIST NMI MNIST - full Table 4: The execution time (in seconds) of different deep clustering approaches. 13100 55 13000 49 USPS 15000 123000 890 110 640 120000 732 MNIST - test 349 Fashion - MNIST 857 MNIST - full Table 5: The execution time (in seconds) of DEC*, IDEC* and DynAE. 10840 USPS MNIST - test Fashion - MNIST MNIST - full",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Clustering\", \"Dataset\": \"MNIST-full\", \"Metric\": \"Accuracy\", \"Score\": \"0.987\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Clustering\", \"Dataset\": \"MNIST-full\", \"Metric\": \"NMI\", \"Score\": \"0.964\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Clustering\", \"Dataset\": \"MNIST-test\", \"Metric\": \"Accuracy\", \"Score\": \"0.987\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Clustering\", \"Dataset\": \"MNIST-test\", \"Metric\": \"NMI\", \"Score\": \"0.963\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Clustering\", \"Dataset\": \"Fashion-MNIST\", \"Metric\": \"Accuracy\", \"Score\": \"0.591\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Clustering\", \"Dataset\": \"Fashion-MNIST\", \"Metric\": \"NMI\", \"Score\": \"0.642\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Clustering\", \"Dataset\": \"USPS\", \"Metric\": \"Accuracy\", \"Score\": \"0.981\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Clustering\", \"Dataset\": \"USPS\", \"Metric\": \"NMI\", \"Score\": \"0.948\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"MNIST-full\", \"Metric\": \"Accuracy\", \"Score\": \"0.965\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"MNIST-full\", \"Metric\": \"NMI\", \"Score\": \"0.913\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"MNIST-test\", \"Metric\": \"Accuracy\", \"Score\": \"0.967\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"MNIST-test\", \"Metric\": \"NMI\", \"Score\": \"0.919\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"Fashion-MNIST\", \"Metric\": \"Accuracy\", \"Score\": \"0.628\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"Fashion-MNIST\", \"Metric\": \"NMI\", \"Score\": \"0.644\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"USPS\", \"Metric\": \"Accuracy\", \"Score\": \"0.957\"}}, { \"LEADERBOARD\": { "
    },
    {
        "Context": "LIPREADING USING TEMPORAL CONVOLUTIONAL NETWORKS Lip-reading has attracted a lot of research attention lately thanks to advances in deep learning. The current state-of-theart model for recognition of isolated words in-the-wild consists of a residual network and Bidirectional Gated Recurrent Unit (BGRU) layers. In this work, we address the limitations of this model and we propose changes which further improve its performance. Firstly, the BGRU layers are replaced with Temporal Convolutional Networks (TCN). Secondly, we greatly simplify the training procedure, which allows us to train the model in one single stage. Thirdly, we show that the current state-of-the-art methodology produces models that do not generalize well to variations on the sequence length, and we addresses this issue by proposing a variable-length augmentation. We present results on the largest publiclyavailable datasets for isolated word recognition in English and Mandarin, LRW and LRW1000, respectively. Our proposed model results in an absolute improvement of 1.2% and 3.2%, respectively, in these datasets which is the new state-of-the-art performance. Pre-processing: Each video sequence from the LRW dataset is processed by 1) doing face detection and face alignment, 2) aligning each frame to a reference mean face shape 3) cropping a fixed 96 \u00d7 96 pixels wide ROI from the aligned face image so that the mouth region is always roughly centered on the image crop 4) transform the cropped image to gray level, as there does not seem to be a performance difference with respect to using RGB The mouth ROIs are pre-cropped in the LRW1000 dataset so there is no need for pre-processing Table 1: Comparison with state-of-the-art methods in the literature on the LRW and LRW-1000 datasets. Performance is in terms of classification accuracy (the higher the better). We also indicate the backbone employed, as some works either use higher-capacity networks, or use an ensemble of two networks. Networks marked with * use 2D convolutions except for the first being a 3D one. - LRW - 1000 ( Accuracy ) 4 - LRW ( Accuracy ) Table 2: Classification accuracy of different models on LRW when frames are randomly removed from the test sequences. The model in the first row is the same as the one in N = 0 N = 3 N = 4 N = 1 N = 2 N =5",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Lipreading\", \"Dataset\": \"Lip Reading in the Wild\", \"Metric\": \"Top-1 Accuracy\", \"Score\": \"85.30\"}}, {\"LEADERBOARD\": {\"Task\": \"Lipreading\", \"Dataset\": \"CAS-VSR-W1k (LRW-1000)\", \"Metric\": \"Top-1 Accuracy\", \"Score\": \"41.4%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Lipreading\", \"Dataset\": \"Lip Reading in the Wild\", \"Metric\": \"Top-1 Accuracy\", \"Score\": \"83.34\"}} ]"
    },
    {
        "Context": "Using the General Intensional Programming System (GIPSY) for Evaluation of Higher-Order Intensional Logic (HOIL) Expressions The General Intensional Programming System (GIPSY) has been built around the Lucid family of intensional programming languages that rely on the higher-order intensional logic (HOIL) to provide context-oriented multidimensional reasoning of intensional expressions. HOIL combines functional programming with various intensional logics to allow explicit context expressions to be evaluated as first-class values that can be passed as parameters to functions and return as results with an appropriate set of operators defined on contexts. GIPSY's frameworks are implemented in Java as a collection of replaceable components for the compilers of various Lucid dialects and the demand-driven eductive evaluation engine that can run distributively. GIPSY provides support for hybrid programming models that couple intensional and imperative languages fora variety of needs. Explicit context expressions limit the scope of evaluation of math expressions (effectively a Lucid program is a mathematics or physics expression constrained by the context) in tensor physics, regular math in multiple dimensions, etc., and for cyberforensic reasoning as one of the use-cases of interest. Thus, GIPSY is a support testbed for HOIL-based languages some of which enable such reasoning, as informal cyberforensic case analysis with event reconstruction. In this paper we discuss the GIPSY architecture, its evaluation engine and example use-cases.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "BERT-based Ensembles for Modeling Disclosure and Support in Conversational Social Media Text There is a growing interest in understanding how humans initiate and hold conversations. The affective understanding of conversations focuses on the problem of how speakers use emotions to react to a situation and to each other. In the CL-Aff Shared Task, the organizers released Get it #OffMyChest dataset, which contains Reddit comments from casual and confessional conversations, labeled for their disclosure and supportiveness characteristics. In this paper, we introduce a predictive ensemble model exploiting the finetuned contextualized word embeddings, RoBERTa and ALBERT. We show that our model outperforms the base models in all considered metrics, achieving an improvement of 3% in the F1 score. We further conduct statistical analysis and outline deeper insights into the given dataset while providing anew characterization of impact for the dataset. We further compare our ensemble model with four other ensemble models and show that our model performs the best among all the models in four out of five evaluation metrics using 10-fold cross validation We evaluate all the models on the following metrics: Accuracy, F1, Precision-1, Recall-1, and the mean of Accuracy and F1, denoted as Acc&F1 from hereon In this section, we provide a comprehensive statistical analysis of the dataset Get it #OffMyChest, which comprises of comments and parent posts from the subreddit /r/CasualConversations, and /r/OffMyChest We further propose new characterizations and outline semantic features for the given dataset Table 1. Label-averaged values for each metric for RoBERTa,ALBERT, and our best performing ensemble model. Accuracy Precision - 1 Recall - 1 F1 Acc&F1 Table 2. Label-wise values for each metric for our best performing ensemble model. 1 Recall - 1 F1 Acc&F1 Table 3. Weights assigned to each model in different Ensemble Models. Each cell contains a pair (x, y) where x denotes the weight assigned to RoBERTa and y denotes the weight assigned to ALBERT. dividual labels . Its performance on different labels is evaluated using the above Table 4. Label-averaged values for each metric for different ensemble models. F1 Acc&F1 Model / Metrics Accuracy Precision - 1 Recall - 1 Table 5. Weekday-wise label distribution of the labelled dataset. Support Emotional General Informational EmotionalSupport Weekday / Label Disclosure Table 6. The relationship between Labels and Impact, as represented by Pearson correlation coefficient, \u03c1. \u03c1 with Impact",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Text Classification\", \"Dataset\": \"AffCon 2020 Emotion Detection\", \"Metric\": \"F1 score\", \"Score\": \"0.558\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Dialogue Act Classification\", \"Dataset\": \"One-class CIFAR-10\", \"Metric\": \"F1\", \"Score\": \"94.4\"}} ]"
    },
    {
        "Context": "Sub-Pixel Back-Projection Network For Lightweight Single Image Super-Resolution Convolutional neural network (CNN)-based methods have achieved great success for single-image superresolution (SISR). However, most models attempt to improve reconstruction accuracy while increasing the requirement of number of model parameters. To tackle this problem, in this paper, we study reducing the number of parameters and computational cost of CNN-based SISR methods while maintaining the accuracy of super-resolution reconstruction performance. To this end, we introduce a novel network architecture for SISR, which strikes a good trade-off between reconstruction quality and low computational complexity. Specifically, we propose an iterative back-projection architecture using sub-pixel convolution instead of deconvolution layers. We evaluate the performance of computational and reconstruction accuracy for our proposed model with extensive quantitative and qualitative evaluations. Experimental results reveal that our proposed method uses fewer parameters and reduces the computational cost while maintaining reconstruction accuracy against state-of-the-art SISR methods over well-known four SR benchmark datasets. 1 Code is available at https://github.com/supratikbanerjee/ SubPixel-BackProjection_SuperResolution. Each model was tested with four datasets, namely, Set5, Set14, BSDS100, and Urban100 Table 1: Quantitative Results on four datasets. The highest reconstruction accuracy is indicated in red and second highest reconstruction accuracy in blue. [\u00d72 upscaling] Datasets Urban100 PSNR SSIM BSDS100 Set5 Set14 Table 1 : Quantitative Results on four datasets . The highest reconstruction accuracy is indicated in red and second highest Multi - Adds",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"Set5 - 2x upscaling\", \"Metric\": \"PSNR\", \"Score\": \"38.05\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"Set5 - 2x upscaling\", \"Metric\": \"SSIM\", \"Score\": \"0.9606\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"Urban100 - 2x upscaling\", \"Metric\": \"PSNR\", \"Score\": \"32.07\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"Urban100 - 2x upscaling\", \"Metric\": \"SSIM\", \"Score\": \"0.9277\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"BSDS100 - 2x upscaling\", \"Metric\": \"PSNR\", \"Score\": \"32.21\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"BSDS100 - 2x upscaling\", \"Metric\": \"SSIM\", \"Score\": \"0.9001\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"Set14 - 2x upscaling\", \"Metric\": \"PSNR\", \"Score\": \"33.62\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"Set14 - 2x upscaling\", \"Metric\": \"SSIM\", \"Score\": \"0.9178\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Super-Resolution\", \"Dataset\": \"BSD100 - 4x upscaling\", \"Metric\": \"PSNR\", \"Score\": \"27.83\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Super-Resolution\", \"Dataset\": \"Set5 - 4x upscaling\", \"Metric\": \"PSNR\", \"Score\": \"30.40\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Super-Resolution\", \"Dataset\": \"Set14 - 4x upscaling\", \"Metric\": \"PSNR\", \"Score\": \"27.88\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Super-Resolution\", \"Dataset\": \"Set14 - 4x upscaling\", \"Metric\": \"SSIM\", \"Score\": \"0.749\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Super-Resolution\", \"Dataset\": \"Urban100 - 4x upscaling\", \"Metric\": \"PSNR\", \"Score\": \"25.94\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Super-Resolution\", \"Dataset\": \"Urban100 - 4x upscaling\", \"Metric\": \"SSIM\", \"Score\": \"0.756\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Super-Resolution\", \"Dataset\": \"Ur"
    },
    {
        "Context": "GATED MECHANISM FOR ATTENTION BASED MULTIMODAL SENTIMENT ANALYSIS Multimodal sentiment analysis has recently gained popularity because of its relevance to social media posts, customer service calls and video blogs. In this paper, we address three aspects of multimodal sentiment analysis; 1. Cross modal interaction learning, i.e. how multiple modalities contribute to the sentiment, 2. Learning long-term dependencies in multimodal interactions and 3. Fusion of unimodal and cross modal cues. Out of these three, we find that learning cross modal interactions is beneficial for this problem. We perform experiments on two benchmark datasets, CMU Multimodal Opinion level Sentiment Intensity (CMU-MOSI) and CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) corpus. Our approach on both these tasks yields accuracies of 83.9% and 81.1% respectively, which is 1.6% and 1.34% absolute improvement over current state-ofthe-art. We evaluated our system on two standard multimodal sentiment analysis datasets from CMU multimodal SDK 1, 1) CMU-MOSI: CMU Multimodal Opinion level Sentiment Intensity and; 2) CMU-MOSEI: CMU Multimodal Opinion Sentiment and Emotion Intensity Table 1: Comparison of performance of each step in the proposed model. Accuracy values are mentioned in the table CMU - MOSEI CMU - MOSI Table 2: Comparative results on CMU-MOSI and CMU-MOSEI multimodal sentiment analysis. ( * ) results are taken from Zadeh et al - ( 83 . 62 ) Sun et al [ 4 ] \u00a6 ( 83 . 75 ) *",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Multimodal Sentiment Analysis\", \"Dataset\": \"CMU-MOSEI\", \"Metric\": \"Accuracy\", \"Score\": \"81.14\"}}, {\"LEADERBOARD\": {\"Task\": \"Multimodal Sentiment Analysis\", \"Dataset\": \"MOSI\", \"Metric\": \"Accuracy\", \"Score\": \"83.91%\"}}, {\"LEADERBOARD\": {\"Task\": \"Multimodal Sentiment Analysis\", \"Dataset\": \"MOSI\", \"Metric\": \"F1 score\", \"Score\": \"81.17\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Multimodal Sentiment Analysis\", \"Dataset\": \"MOSI\", \"Metric\": \"Accuracy\", \"Score\": \"85.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Multimodal Sentiment Analysis\", \"Dataset\": \"MOSI\", \"Metric\": \"MAE\", \"Score\": \"0.01\"}}, { \"LEADERBOARD\": { \"Task\": \"Multimodal Sentiment Analysis\", \"Dataset\": \"MOSI\", \"Metric\": \"PSNR\", \"Score\": \"83.6\"}}, { \"LEADERBOARD\": { \"Task\": \"Multimodal Sentiment Analysis\", \"Dataset\": \"MOSI\", \"Metric\": \"SSIM\", \"Score\": \"0.76\"}} ]"
    },
    {
        "Context": "We propose a dynamical process for network evolution, aiming at explaining the emergence of the small world phenomenon, i.e., the statistical observation that any pair of individuals are linked by a short chain of acquaintances computable by a simple decentralized routing algorithm, known as greedy routing. Previously proposed dynamical processes enabled to demonstrate experimentally (by simulations) that the small world phenomenon can emerge from local dynamics. However, the analysis of greedy routing using the probability distributions arising from these dynamics is quite complex because of mutual dependencies. In contrast, our process enables complete formal analysis. It is based on the combination of two simple processes: a random walk process, and an harmonic forgetting process. Both processes reflect natural behaviors of the individuals, viewed as nodes in the network of inter-individual acquaintances. We prove that, in k-dimensional lattices, the combination of these two processes generates long-range links mutually independently distributed as a k-harmonic distribution. We analyze the performances of greedy routing at the stationary regime of our process, and prove that the expected number of steps for routing from any source to any target in any multidimensional lattice is a polylogarithmic function of the distance between the two nodes in the lattice. Up to our knowledge, these results are the first formal proof that navigability in small worlds can emerge from a dynamical process for network evolution. Our dynamical process can find practical applications to the design of spatial gossip and resource location protocols. . Additional supports from the ANR projects ALADDIN and ALPAGE, and from the COST Action 295 DYNAMO.\u2021 CNRS and University Paris 7. Email: Emmanuelle.Lebhar@liafa.jussieu.fr. Additional supports from the ANR project ALADDIN, and from the COST Action 295 DYNAMO.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis Most conditional generation tasks expect diverse outputs given a single conditional context. However, conditional generative adversarial networks (cGANs) often focus on the prior conditional information and ignore the input noise vectors, which contribute to the output variations. Recent attempts to resolve the mode collapse issue for cGANs are usually task-specific and computationally expensive. In this work, we propose a simple yet effective regularization term to address the mode collapse issue for cGANs. The proposed method explicitly maximizes the ratio of the distance between generated images with respect to the corresponding latent codes, thus encouraging the generators to explore more minor modes during training. This mode seeking regularization term is readily applicable to various conditional generation tasks without imposing training overhead or modifying the original network structures. We validate the proposed algorithm on three conditional image synthesis tasks including categorical generation, image-toimage translation, and text-to-image synthesis with different baseline models. Both qualitative and quantitative results demonstrate the effectiveness of the proposed regularization method for improving diversity without loss of quality. * Equal contribution We evaluate the proposed regularization method through extensive quantitative and qualitative evaluation More implementation and evaluation details, please refer to the appendixes We conduct evaluations using the following metrics To evaluate the quality of the generated images, we use FID to measure the distance between the generated distribution and the real one through features extracted by Inception Network To evaluate diversity, we employ LPIPS following These metrics evaluate the extent of mode missing of generative models We calculate the bin-proportions of the training samples and the synthesized samples to evaluate the difference between the generated distribution and the real data distribution For evaluation, we randomly generate N images fora given conditional context on various tasks More evaluation details of one trial are presented as follows For NDB and JSD, we employ all the training samples for clustering and choose K = 20 bins for facades, and K = 50 bins for Table 1: NDB and JSD results on the CIFAR-10 dataset. automobile deer horse frog airplane bird cat truck ship dog Table 2: FID results on the CIFAR-10 dataset. DCGAN MSGAN Table 3: Quantitative results on the facades and maps dataset. Datasets Facades Maps BicycleGAN [ 35 ] MSGAN Pix2Pix [ 11 ] Table 4: Quantitative results of the Yosemite (Summer Winter) and the Cat Dog dataset. Datasets Cat2Dog Dog2Cat Summer2Winter DRIT [ 15 ] MSGAN Winter2Summer Table 5: Quantitative results on the CUB-200-2011 dataset. We conduct experiments in two settings: 1) Conditioned on text descriptions, where every description can be mapped to different text codes. 2) Conditioned on text codes, where the text codes are fixed so that their effects are excluded. Conditioned on text codes Conditioned on text descriptions MSGAN StackGAN++ [ 32 ] Table 6: Quantitative results with different \u03bb ms on the facades dataset. 1 3 Table 7:",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Multimodal Unsupervised Image-To-Image Translation\", \"Dataset\": \"CelebA-HQ\", \"Metric\": \"FID\", \"Score\": \"33.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Multimodal Unsupervised Image-To-Image Translation\", \"Dataset\": \"AFHQ\", \"Metric\": \"FID\", \"Score\": \"61.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Generation\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"FID\", \"Score\": \"28.73\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"FID\", \"Score\": \"12.30\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Inception score\", \"Score\": \"8.25\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"FID\", \"Score\": \"12.89\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Inception score\", \"Score\": \"8.21\"}} ]"
    },
    {
        "Context": "FABRICATION OF MINIATURIZED VARIABLE-FOCUS LENS USING LIQUID FILLING TECHNIQUE This paper describes a simple method for fabricating a variable-focus lens by using PDMS (polydimethylsiloxane) and filling with liquid for the variable-focus lens. The lens diameter of 2-mm was designed in this experiment and expected to reach the focal length in the range of 3 ~ 12 mm. The theoretical value between the liquid volume and the lens contact angle at different focal lengths were simulated and measured.  Table 1 List of symbols. 3 , and",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Kvasir-SEG: A Segmented Polyp Dataset Pixel-wise image segmentation is a highly demanding task in medical-image analysis. In practice, it is difficult to find annotated medical images with corresponding segmentation masks. In this paper, we present Kvasir-SEG: an open-access dataset of gastrointestinal polyp images and corresponding segmentation masks, manually annotated by a medical doctor and then verified by an experienced gastroenterologist. Moreover, we also generated the bounding boxes of the polyp regions with the help of segmentation masks. We demonstrate the use of our dataset with a traditional segmentation approach and a modern deep-learning based Convolutional Neural Network (CNN) approach. The dataset will be of value for researchers to reproduce results and compare methods. By adding segmentation masks to the Kvasir dataset, which only provide frame-wise annotations, we enable multimedia and computer vision researchers to contribute in the field of polyp segmentation and automatic analysis of colonoscopy images. The Kvasir-SEG dataset is based on the previous Kvasir dataset, which is the first multi-class dataset for gastrointestinal (GI) tract disease detection and classification The original Kvasir dataset comprises 8,000 GI tract images from 8 classes where each class consists of 1000 images We replaced the 13 images from the polyp class with new images to improve the quality of the dataset A more detailed explanation about each image classes, the data collection procedure and the dataset details can be found in The Kvasir dataset was used for the Multimedia for Medicine Challenge (the Medico Task) in 2017 and 2018 at the MediaEval Benchmarking Initiative for Multimedia Evaluation 1 to develop and compare methods to reach clinical level performance on multiclass classification of endoscopic findings in the large bowel However, the dataset was limited to frame classification only, due to only a frame-wise annotations trained their model on the CVC-ClinicDB, and Table 1: Quantitative performance of ResUNet model on Kvasir-SEG dataset. Dice coefficient Mean IoU Loss",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Medical Image Segmentation\", \"Dataset\": \"Kvasir-SEG\", \"Metric\": \"mean Dice\", \"Score\": \"0.7877\"}}, {\"LEADERBOARD\": {\"Task\": \"Polyp Segmentation\", \"Dataset\": \"Kvasir-SEG\", \"Metric\": \"DSC\", \"Score\": \"0.7877\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Medical Image Segmentation\", \"Dataset\": \"Kvasir-SEG\", \"Metric\": \"FPS\", \"Score\": \"69.59\"}}, { \"LEADERBOARD\": { \"Task\": \"Medical Image Segmentation\", \"Dataset\": \"Kvasir-SEG\", \"Metric\":\"mIoU\", \"Score\": \"0.7800\"}}, { \"LEADERBOARD\": { \"Task\": \"Medical Image Segmentation\", \"Dataset\": \"Kvasir-SEG\", \"Metric\":\"mean Dice\", \"Score\": \"0.8576\"}}, { \"LEADERBOARD\": { \"Task\": \"Medical Image Segmentation\", \"Dataset\": \"Endotect Polyp Segmentation\", \"Metric\": \"DSC\", \"Score\": \"0.7870\"}}, { \"LEADERBOARD\": { \"Task\": \"Medical Image Segmentation\", \"Dataset\": \"Endotect Polyp Segmentation\", \"Metric\": \"FPS\", \"Score\": \"70.23\"}}, { \"LEADERBOARD\": { \"Task\": \"Medical Image Segmentation\", \"Dataset\": \"Endotect Polyp Segmentation\", \"Metric\":\"mIoU\", \"Score\": \"0.701\"}} ]"
    },
    {
        "Context": "COMPLEX TRANSFORMER: A FRAMEWORK FOR MODELING COMPLEX-VALUED SEQUENCE While deep learning has received a surge of interest in a variety of fields in recent years, major deep learning models barely use complex numbers. However, speech, signal and audio data are naturally complex-valued after Fourier Transform, and studies have shown a potentially richer representation of complex nets. In this paper, we propose a Complex Transformer, which incorporates the transformer model as a backbone for sequence modeling; we also develop attention and encoder-decoder network operating for complex input. The model achieves state-of-the-art performance on the MusicNet dataset and an In-phase Quadrature (IQ) signal dataset. The GitHub implementation to reproduce the experimental results is available at https://github.com/ muqiaoy/dl_signal. Index Terms-Deep learning, transformer network, sequence modeling, complex-valued deep neural network  Table 1. Experimental results for automatic music transcrip- tion. Model # Parameters APS (%) tion . Table 1 . Experimental results for automatic music transcrip - APS ( % ) # Parameters Table 2. Experimental results for In-phase Quadrature (IQ) data. Model Accuracy (%) data . Accuracy ( % ) Experimental results for In - phase Quadrature ( IQ ) Model Table 2 . Table 3. Experimental results (in loss) for sequence genera- tion. Model MusicNet IQ dataset tion . IQ dataset Table 3 . Experimental results ( in loss ) for sequence genera - MusicNet",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Music Transcription\", \"Dataset\": \"MusicNet\", \"Metric\": \"APS\", \"Score\": \"74.22\"}}, {\"LEADERBOARD\": {\"Task\": \"Music Transcription\", \"Dataset\": \"MusicNet\", \"Metric\": \"Number of params\", \"Score\": \"11.61M\"}}, {\"LEADERBOARD\": {\"Task\": \"Music Transcription\", \"Dataset\": \"MusicNet\", \"Metric\": \"APS\", \"Score\": \"71.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Music Transcription\", \"Dataset\": \"MusicNet\", \"Metric\": \"Number of params\", \"Score\": \"9.79M\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"80.8%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"95.1%\"}} ]"
    },
    {
        "Context": "Bit Error Rate Performance Analysis on Modulation Techniques of Wideband Code Division Multiple Access In the beginning of 21 st century there has been a dramatic shift in the market dynamics of telecommunication services. The transmission from base station to mobile or downlink transmission using M-ary Quadrature Amplitude modulation (QAM) and Quadrature phase shift keying (QPSK) modulation schemes are considered in Wideband-Code Division Multiple Access (W-CDMA) system. We have done the performance analysis of these modulation techniques when the system is subjected to Additive White Gaussian Noise (AWGN) and multipath Rayleigh fading are considered in the channel. The research has been performed by using MATLAB 7.6 for simulation and evaluation of Bit Error Rate (BER) and Signal-To-Noise Ratio (SNR) for W-CDMA system models. It is shows that the analysis of Quadrature phases shift key and 16-ary Quadrature Amplitude modulations which are being used in wideband code division multiple access system, Therefore, the system could go for more suitable modulation technique to suit the channel quality, thus we can deliver the optimum and efficient data rate to mobile terminal.  No ) 4481 Simulation result for evaluation on BER vs . SNR Noise 7520 Signal to 15615 11334",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Multi-Shift de Bruijn Sequence A (non-circular) de Bruijn sequence w of order n is a word such that every word of length n appears exactly once in was a factor. In this paper, we generalize the concept to a multi-shift setting: a m-shift de Bruijn sequence of order n is a word such that every word of length n appears exactly once in was a factor that starts at an index im + 1 for some integer i \u2265 0. We show the number of the m-shift de Bruijn sequences of order n is an !a (m\u2212n)(a n \u22121) for 1 \u2264 n \u2264 m and is (a m !) a n\u2212m for 1 \u2264 m \u2264 n, where a is the size of the alphabet. We provide two algorithms for generating a multi-shift de Bruijn sequence. The multi-shift de Bruijn sequence is important in solving the Frobenius problem in a free monoid.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Ideal Stabilization We define and explore the concept of ideal stabilization. The program is ideally stabilizing if its every state is legitimate. Ideal stabilization allows the specification designer to prescribe with arbitrary degree of precision not only the fault-free program behavior but also its recovery operation. Specifications mayor may not mention all possible states. We identify approaches to designing ideal stabilization to both kinds of specifications. For the first kind, we state the necessary condition for an ideally stabilizing solution. On the basis of this condition we prove that there is no ideally stabilizing solution to the leader election problem. We illustrate the utility of the concept by providing examples of well-known programs and proving them ideally stabilizing. Specifically, we prove ideal stabilization of the conflict manager, the alternator, the propagation of information with feedback and the alternating bit protocol. \u22c6",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Abstraction and Refinement in Static Model-Checking interpretation is a general methodology for building static analyses of programs. It was introduced by P. and R. Cousot in [3]. We present, in this paper, an application of a generic abstract interpretation to domain of model-checking. Dynamic checking are usually easier to use, because the concept are established and wide well know. But they are usually limited to systems whose states space is finite. In another part, certain faults cannot be detected dynamically, even by keeping track of the history of the states space.Indeed, the classical problem of finding the right test cases is far from trivial and limit the abilities of dynamic checkers further. Static checking have the advantage that they work on a more abstract level than dynamic checker and can verify system properties for all inputs. Problem, it is hard to guarantee that a violation of a modeled property corresponds to a fault in the concrete system. We propose an approach, in which we generate counter-examples dynamically using the abstract interpretation techniques. a)",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Simple Baselines for Human Pose Estimation and Tracking There has been significant progress on pose estimation and increasing interests on pose tracking in recent years. At the same time, the overall algorithm and system complexity increases as well, making the algorithm analysis and comparison more difficult. This work provides simple and effective baseline methods. They are helpful for inspiring and evaluating new ideas for the field. State-of-the-art results are achieved on challenging benchmarks. The code will be available at https://github. com/leoxiaobin/pose.pytorch.  Table 2. Ablation study of our method on COCO val2017 dataset. Those settings used in comparison are in bold. For example, (a, e, f) compares backbones. #Deconv . Layers Deconv . Kernel Size AP Table 3. Comparison with Hourglass [22] and CPN [6] on COCO val2017 dataset. Their results are cited from [6]. OHKM means Online Hard Keypoints Mining. 2 . Kernel size . Methods ( a , c , d ) show that a smaller kernel size gives a marginally performance . Methods ( a , e , f ) show steady improvement by using deeper Input Size AP Table 4. Comparisons on COCO test-dev dataset. Top: methods in the literature, trained only on COCO training dataset. Middle: results submitted to COCO test-dev leaderboard [9], which have either extra training data (*) or models ensamled ( + ). Bottom: our single model results, trained only on COCO training dataset. split",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Pose Estimation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP\", \"Score\": \"73.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Pose Estimation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP50\", \"Score\": \"91.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Pose Estimation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP75\", \"Score\": \"81.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Pose Estimation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APL\", \"Score\": \"80\"}}, {\"LEADERBOARD\": {\"Task\": \"Pose Estimation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APM\", \"Score\": \"70.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Pose Estimation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AR\", \"Score\": \"79\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO\", \"Metric\": \"Validation AP\", \"Score\": \"72.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-challenge\", \"Metric\": \"AP\", \"Score\": \"74.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-challenge\", \"Metric\": \"AP50\", \"Score\": \"90.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-challenge\", \"Metric\": \"AP75\", \"Score\": \"80.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-challenge\", \"Metric\": \"APL\", \"Score\": \"87.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-challenge\", \"Metric\": \"AR\", \"Score\": \"80.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-challenge\", \"Metric\": \"AR50\", \"Score\": \"95.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-challenge\", \"Metric\": \"AR75\", \"Score\": \"86.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-challenge\", \"Metric\": \"ARL\", \"Score\": \"82.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-challenge\", \"Metric\": \"ARM\", \"Score\": \"75.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP50\", \"Score\": \"92.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP75\", \"Score\": \"84.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APL\", \"Score\": \"82.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APM\", \"Score\": \"73.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AR\", \"Score\": \"81.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AR50\", \"Score\": \"95.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AR75\", \"Score\": \"88.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"ARL\", \"Score\": \"87.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"ARM\", \"Score\": \"77.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP50\", \"Score\": \"91.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP75\", \"Score\": \"81.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APL\", \"Score\": \"80.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APM\", \"Score\": \"70.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Keypoint Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AR\", \"Score\": \"79.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Pose Tracking\", \"Dataset\": \"PoseTrack2017\", \"Metric\": \"MOTA\", \"Score\": \"57.81\"}}, {\"LEADERBOARD\": {\"Task\": \"Pose Tracking\", \"Dataset\": \"PoseTrack2017\", \"Metric\": \"mAP\", \"Score\": \"74.57\"}}, {\"LEADERBOARD\": {\"Task\": \"Pose Tracking\", \"Dataset\": \"PoseTrack2018\", \"Metric\": \"MOTA\", \"Score\": \"61.37\"}}, {\"LEADERBOARD\": {\"Task\": \"Pose Tracking\", \"Dataset\": \"PoseTrack2018\", \"Metric\": \"mAP\", \"Score\": \"74.03\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Pose Tracking\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP\", \"Score\": \"68.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Pose Tracking\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APL\", \"Score\": \"70.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Pose Tracking\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APM\", \"Score\": \"66.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Pose Tracking\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APS\", \"Score\": \"22.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Pose Tracking\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"box AP\", \"Score\": \"42.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Pose Tracking\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP50\", \"Score\": \"65.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Pose Tracking\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP75\", \"Score\": \"43.3\"}},"
    },
    {
        "Context": "Patent Classification by Fine-Tuning BERT Language Model In this work we focus on fine-tuning a pre-trained BERT model and applying it to patent classification. When applied to large datasets of over two millions patents, our approach outperforms the state of the art by an approach using CNN with word embeddings. In addition, we focus on patent claims without other parts in patent documents. Our contributions include: (1) anew state-of-the-art result based on pretrained BERT model and fine-tuning for patent classification, (2) a large dataset USPTO-3M at the CPC subclass level with SQL statements that can be used by future researchers, (3) showing that patent claims alone are sufficient for classification task, in contrast to conventional wisdom.  Table 1: Patent Classification Performance Top 1 Top 5 O ( % ) N / A Precision Recall < 74 F1 < 35 < 45",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Multi-Label Text Classification\", \"Dataset\": \"USPTO-3M\", \"Metric\": \"F1\", \"Score\": \"66.83%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"USPS\", \"Metric\": \"F1\", \"Score\": \"45.0%\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"USPS\", \"Metric\": \"F1\", \"Score\": \"43.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"USPS\", \"Metric\": \"F1\", \"Score\": \"38.6\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"USPS\", \"Metric\": \"F1\", \"Score\": \"40.4%\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"USPS\", \"Metric\": \"F1\", \"Score\": \"40.4%\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"USPS\", \"Metric\": \"F1\", \"Score\": \"40.2%\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"arstrial\", \"Metric\": \"F1\", \"Score\": \"44.51\"}}, { \"LEADERBOARD\": { "
    },
    {
        "Context": "Multilingual Models for Compositional Distributed Semantics We present a novel technique for learning semantic representations, which extends the distributional hypothesis to multilingual data and joint-space embeddings. Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences, while maintaining sufficient distance between those of dissimilar sentences. The models do not rely on word alignments or any syntactic information and are successfully applied to a number of diverse languages. We extend our approach to learn semantic representations at the document level, too. We evaluate these models on two cross-lingual document classification tasks, outperforming the prior state of the art. Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data. First, we replicate the cross-lingual document classification task of, learning distributed representations on the Europarl corpus and evaluating on documents from the Reuters RCV1/RCV2 corpora Subsequently, we design a multi-label classification task using the TED corpus, both for training and evaluating The use of a wider range of languages in the second experiments allows us to better evaluate our models' capabilities in learning a shared multilingual semantic representation Here we describe our experiments on the TED corpus, which enables us to scale up to multilingual learning Consisting of a large number of relatively short and parallel documents, this corpus allows us to evaluate the performance of the DOC model described in \u00a73.2 We use the training data of the corpus to learn distributed representations across 12 languages In the single mode, vectors are learnt from a single language pair (en-X), while in the joint mode vectorlearning is performed on all parallel Table 1: Classification accuracy for training on English and German with 1000 labeled examples on the RCV corpus. Cross-lingual compositional representations (ADD, BI and their multilingual extensions), I-Matrix (Klementiev et al., 2012) translated (MT) and glossed (Glossed) word baselines, and the majority class baseline. The baseline results are from Klementiev et al. (2012). dim = 40 dim = 128 en \u2192 de de \u2192 en Table 2: F1-scores for the TED document classification task for individual languages. Results are re- ported for both directions (training on English, evaluating on L2 and vice versa). Bold indicates best result, underline best result amongst the vector-based systems. L2 \u2192 en Language en \u2192 L2 Training Rom ' n Russian Turkish Pt - Br Pt - Br Roman . Russian Turkish Test Language Setting Languages Arabic German Spanish French Italian Dutch Polish Table 3: F1-scores for TED corpus document classification results when training and testing",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Cross-Lingual Document Classification\", \"Dataset\": \"Reuters RCV1/RCV2 English-to-German\", \"Metric\": \"Accuracy\", \"Score\": \"88.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Cross-Lingual Document Classification\", \"Dataset\": \"Reuters RCV1/RCV2 German-to-English\", \"Metric\": \"Accuracy\", \"Score\": \"79.2\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Text Classification\", \"Dataset\": \"R7\", \"Metric\": \"Accuracy\", \"Score\": \"71.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Text Classification\", \"Dataset\": \"R7\", \"Metric\": \"Accuracy\", \"Score\": \"71.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Text Classification\", \"Dataset\": \"ROUGE-L\", \"Metric\": \"Accuracy\", \"Score\": \"85.3\"}} ]"
    },
    {
        "Context": "Digging Into Self-Supervised Monocular Depth Estimation Per-pixel ground-truth depth data is challenging to acquire at scale. To overcome this limitation, self-supervised learning has emerged as a promising alternative for training models to perform monocular depth estimation. In this paper, we propose a set of improvements, which together result in both quantitatively and qualitatively improved depth maps compared to competing self-supervised methods.Research on self-supervised monocular training usually explores increasingly complex architectures, loss functions, and image formation models, all of which have recently helped to close the gap with fully-supervised methods. We show that a surprisingly simple model, and associated design choices, lead to superior predictions. In particular, we propose (i) a minimum reprojection loss, designed to robustly handle occlusions, (ii) a full-resolution multi-scale sampling method that reduces visual artifacts, and (iii) an auto-masking loss to ignore training pixels that violate camera motion assumptions. We demonstrate the effectiveness of each component in isolation, and show high quality, state-of-the-art results on the KITTI benchmark. We evaluate our models, named Monodepth2, on the KITTI 2015 stereo dataset, to allow comparison with previously published monocular methods In Section A of the supplementary material we show odometry evaluation on KITTI In we evaluate our pose estimation network following the protocol in In order to evaluate our two-frame model on the five-frame test sequences, we make separate predictions for each of the four frame-to-frame transformation for each set of five frames and combine them to form local trajectories Baseline methods were evaluated using their provided disparity files, which were either available publicly or from private communication with the authors Additional Evaluation D.1 Improved Ground Truth As mentioned in the main paper, the evaluation method introduced by Eigen for KITTI uses the reprojected LI-DAR points but does not handle occlusions, moving objects, or the fact that the car is moving introduced a set of high quality depth maps for the Table 1. Quantitative results. Com- parison of our method to existing methods on KITTI 2015 [13] using the Eigen split. Best results in each category are in bold; second best are underlined. underlined . methods on KITTI 2015 [ 13 ] using category are in bold ; second best are the Eigen split . Table 1 . Quantitative results . parison - Table 2. Ablation. Results for different variants of our model (Monodepth2) with monocular training on KITTI 2015 [13] using the Eigen split. (a) The baseline model, with none of our contributions, performs poorly. The addition of our minimum reprojection, auto-masking and full-res multi-scale components, significantly improves performance. (b) Even without ImageNet pretrained weights, our much simpler model brings large improvements above the baseline -see also Table 1. (c) If we train with the full Eigen dataset (instead of the subset introduced for monocular training by [76]) our improvement over",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Monocular Depth Estimation\", \"Dataset\": \"KITTI Eigen split\", \"Metric\": \"absolute relative error\", \"Score\": \"0.106\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Monocular Depth Estimation\", \"Dataset\": \"KITTI Eigen split\", \"Metric\": \"absolute relative error\", \"Score\": \"0.133\"}} ]"
    },
    {
        "Context": "On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units Deep feedforward neural networks with piecewise linear activations are currently producing the state-of-the-art results in several public datasets (e.g., CIFAR-10, CIFAR-100, MNIST, and SVHN). The combination of deep learning models and piecewise linear activation functions allows for the estimation of exponentially complex functions with the use of a large number of subnetworks specialized in the classification of similar input examples. During the training process, these subnetworks avoid overfitting with an implicit regularization scheme based on the fact that they must share their parameters with other subnetworks. Using this framework, we have made an empirical observation that can improve even more the performance of such models. We notice that these models assume a balanced initial distribution of data points with respect to the domain of the piecewise linear activation function. If that assumption is violated, then the piecewise linear activation units can degenerate into purely linear activation units, which can result in a significant reduction of their capacity to learn complex functions. Furthermore, as the number of model layers increases, this unbalanced initial distribution makes the model ill-conditioned. Therefore, we propose the introduction of batch normalisation units into deep feedforward neural networks with piecewise linear activations, which drives a more balanced use of these activation units, where each region of the activation function is trained with a relatively large proportion of training samples. Also, this batch normalisation promotes the pre-conditioning of very deep learning models. We show that by introducing maxout and batch normalisation units to the network in network model results in a model that produces classification results that are better than or comparable to the current state of the art in CIFAR-10, CIFAR-100, MNIST, and SVHN datasets. We evaluate our proposed method on four common deep learning benchmarks: CIFAR-10, CIFAR-100, MNIST and SVHN The CIFAR-10 [8] dataset contains 60000 32x32 RGB images of 10 classes of common visual objects (e.g., animals, vehicles, etc.), where 50000 images are for training and the rest 10000 for testing The MNIST dataset is a standard benchmark for comparing learning methods Finally, the Street View House Number (SVHN) dataset is a real-word digit dataset with over 600000 32x32 RGB images containing images of house numbers (i.e., digits 0-9) The dataset is partitioned into training, test and extra sets, where the extra 530000 images are less difficult samples to be used as extra training data For each of these datasets, we validate our algorithm using the same training and validation splitting described by Goodfellow et al Moreover, we do not perform data augmentation for any of these datasets and only compare our MIM model",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"MNIST\", \"Metric\": \"Percentage error\", \"Score\": \"0.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-100\", \"Metric\": \"Percentage correct\", \"Score\": \"70.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"SVHN\", \"Metric\": \"Percentage error\", \"Score\": \"2.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Percentage correct\", \"Score\": \"91.5\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-100\", \"Metric\": \"Percentage correct\", \"Score\": \"84.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Percentage correct\", \"Score\": \"94.8\"}} ]"
    },
    {
        "Context": "ConsNet: Learning Consistency Graph for Zero-Shot Human-Object Interaction Detection Human-Object Interaction Detection, Graph Neural Networks, Zero- Shot Learning ACM Reference Format We consider the problem of Human-Object Interaction (HOI) Detection, which aims to locate and recognize HOI instances in the form of \u27e8\u210e , , \u27e9 in images. Most existing works treat HOIs as individual interaction categories, thus cannot handle the problem of long-tail distribution and polysemy of action labels. We argue that multi-level consistencies among objects, actions and interactions are strong cues for generating semantic representations of rare or previously unseen HOIs. Leveraging the compositional and relational peculiarities of HOI labels, we propose ConsNet, a knowledge-aware framework that explicitly encodes the relations among objects, actions and interactions into an undirected graph called consistency graph, and exploits Graph Attention Networks (GATs) to propagate knowledge among HOI categories as well as their constituents. Our model takes visual features of candidate human-object pairs and word embeddings of HOI labels as inputs, maps them into visual-semantic joint embedding space and obtains detection results by measuring their similarities. We extensively evaluate our model on the challenging V-COCO and HICO-DET datasets, and results validate that our approach outperforms stateof-the-arts under both fully-supervised and zero-shot settings. Code is available at https://github.com/yeliudev/ConsNet. In this section, we evaluate the proposed method on the challenging V-COCO and HICO-DET datasets We first evaluate our method under the fully-supervised settings on both of the datasets, following by zero-shot settings on HICO-DET dataset An extensive ablation study is also reported after the evaluations V-COCO is a subset of MS-COCO 2014 dataset, it has 2,533 images for training, 2,867 images for validation and 4,946 images for testing HICO-DET is another large-scale HOI detection dataset that extends annotations of HICO from image-level to instancelevel We follow the standard evaluation metric introduced by Chao et al Table 1: Role Detection results on V-COCO dataset under fully-supervised settings. mAP Table 2: HOI Detection results on HICO-DET dataset un- der fully-supervised settings. R and H represent ResNet and Hourglass respectively. Non - Rare Rare Full Table 3: HOI Detection results on HICO-DET dataset under zero-shot settings. UC, UO and UA denote unseen action- object combination, unseen object and unseen action scenar- ios respectively. UO 12 . 74\u00b10 . 34 16 . 99\u00b11 . 67 19 . 81\u00b10 . 32 11 . 31\u00b11 . 03 Unseen 12 . 45\u00b10 . 16 Seen 20 . 51\u00b10 . 62 Full Table 4: Ablation study results on HICO-DET dataset under fully-supervised settings. \u2205 means predicting HOI labels us- ing visual embedding network directly. Non - Rare Rare Full",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Human-Object Interaction Detection\", \"Dataset\": \"HICO-DET\", \"Metric\": \"mAP\", \"Score\": \"25.94\"}}, {\"LEADERBOARD\": {\"Task\": \"Human-Object Interaction Detection\", \"Dataset\": \"HICO-DET\", \"Metric\": \"mAP\", \"Score\": \"22.15\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Human-Object Interaction Detection\", \"Dataset\": \"HICO-DET\", \"Metric\":\"mAP\", \"Score\": \"87.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Human-Object Interaction Detection\", \"Dataset\": \"V-COCO\", \"Metric\": \"MAP\", \"Score\": \"44.2\"}} ]"
    },
    {
        "Context": "Generalised sequential crossover of words and languages In this paper, we propose anew operation, Generalised Sequential Crossover (GSCO) of words, which in some sense an abstract model of crossing over of the chromosomes in the living organisms. We extend GSCO over language L iteratively (GSCO * (L) as well as iterated GSCO over two languages GSCO * (L1, L2)). Our study reveals that GSCO * (L) is subclass of regular languages for any L. We compare the different classes of GSCO languages with the prominent sub-regular classes.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Optimal Iris Fuzzy Sketches Fuzzy sketches, introduced as a link between biometry and cryptography, area way of handling biometric data matching as an error correction issue. We focus hereon iris biometrics and look for the best error-correcting code in that respect. We show that two-dimensional iterative min-sum decoding leads to results near the theoretical limits. In particular, we experiment our techniques on the Iris Challenge Evaluation (ICE) database and validate our findings.  Table 1: Theoretical limits on ICE database Theoretical limits on ICE database Best theoretical FRR",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Iris-View Image-to-Image Translation\", \"Dataset\": \"cvusa\", \"Metric\": \"SSIM\", \"Score\": \"0.82\"}} ]"
    },
    {
        "Context": "SimCSE: Simple Contrastive Learning of Sentence Embeddings This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We hypothesize that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we draw inspiration from the recent success of learning sentence embeddings from natural language inference (NLI) datasets and incorporate annotated pairs from NLI datasets into contrastive learning by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 74.5% and 81.6% Spearman's correlation respectively, a 7.9 and 4.6 points improvement compared to previous best results. We also show that contrastive learning theoretically regularizes pretrained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available. 1 1) with different datasets and compare the results in (for a fair comparison, we also run experiments with the same # of training pairs) We find that most of these models using supervised datasets outperform our unsupervised approach, showing a clear benefit from supervised signals Among all the options, using entailment pairs from the NLI (SNLI + MNLI) datasets perform the best We think this is reasonable, as the NLI datasets consist of high-quality and crowd-sourced pairs, and human annotators are expected to write the hypotheses manually based on the premises, and 3 Though our final model only takes entailment pairs as positives, here we also try neutral and contradiction pairs 4 https://www.quora.com/q/quoradata/ 5 ParaNMT is automatically constructed by machine translation systems and we should not call it a supervised dataset, although it even underperforms our unsupervised SimCSE Contradiction as hard negatives Finally, we further take the advantage of the NLI Table 2: Comparison of different data augmentations on STS-B development set (Spearman's correlation). Crop k%: randomly crop and keep a continuous span with 100-k% of the length; word deletion k%: ran- domly delete k% words; delete one word: randomly delete one word; MLM k%: use BERT base to replace k% of words. All of them include the standard 10% dropout (except \"w/o dropout\"). 10% 20% 30% Data augmentation STS - B Table 3: Comparison of different unsupervised ob- jectives. Results are Spearman's correlation on the STS-B development set using BERT base , trained on 1- million pairs from Wikipedia. The two columns denote whether we use one encoder f \u03b8 or two independent en- coders f \u03b81 and f \u03b82 (\"dual-encoder\"). Next 3 sentences: randomly sample one from the next 3 sentences. Delete one word: delete one word randomly (see 1 f \u03b8 Table 4: Effects of different dropout probabilities p",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semantic Textual Similarity\", \"Dataset\": \"STS Benchmark\", \"Metric\": \"Spearman Correlation\", \"Score\": \"0.867\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Textual Similarity\", \"Dataset\": \"SICK\", \"Metric\": \"Spearman Correlation\", \"Score\": \"0.8195\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Textual Similarity\", \"Dataset\": \"STS12\", \"Metric\": \"Spearman Correlation\", \"Score\": \"0.7746\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Textual Similarity\", \"Dataset\": \"STS13\", \"Metric\": \"Spearman Correlation\", \"Score\": \"0.8727\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Textual Similarity\", \"Dataset\": \"STS14\", \"Metric\": \"Spearman Correlation\", \"Score\": \"0.8236\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Textual Similarity\", \"Dataset\": \"STS16\", \"Metric\": \"Spearman Correlation\", \"Score\": \"0.8393\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Textual Similarity\", \"Dataset\": \"STS15\", \"Metric\": \"Spearman Correlation\", \"Score\": \"0.8666\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Textual Similarity\", \"Dataset\": \"STS Benchmark\", \"Metric\": \"Spearman Correlation\", \"Score\": \"0.6921\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Textual Similarity\", \"Dataset\": \"SICK\", \"Metric\": \"Spearman Correlation\", \"Score\": \"0.6425\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Textual Similarity\", \"Dataset\": \"MRPC\", \"Metric\": \"Spearman Correlation\", \"Score\": \"0.5677\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Textual Similarity\", \"Dataset\": \"STS\", \"Metric\": \"Spearman Correlation\", \"Score\": \"0.4904\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Textual Similarity\", \"Dataset\": \"STS\", \"Metric\": \"Spearman Correlation\", \"Score\": \"0.6121\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Textual Similarity\", \"Dataset\": \"STS\", \"Metric\": \"Spearman Correlation\", \"Score\": \"0.7011\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Textual Similarity\", \"Dataset\": \"MR"
    },
    {
        "Context": "DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer We present a novel Dynamic Differentiable Reasoning (DDR) framework for jointly learning branching programs and the functions composing them; this resolves a significant nondifferentiability inhibiting recent dynamic architectures. We apply our framework to two settings in two highly compact and data efficient architectures: DDRprog for CLEVR Visual Question Answering and DDRstack for reverse Polish notation expression evaluation. DDRprog uses a recurrent controller to jointly predict and execute modular neural programs that directly correspond to the underlying question logic; it explicitly forks subprocesses to handle logical branching. By effectively leveraging additional structural supervision, we achieve a large improvement over previous approaches in subtask consistency and a small improvement in overall accuracy. We further demonstrate the benefits of structural supervision in the RPN setting: the inclusion of a stack assumption in DDRstack allows our approach to generalize to long expressions where an LSTM fails the task. Recall that the loss is evaluated on the predicted answers to all n subproblems corresponding to the outputs of each OP function Table 1. Accuracy on all CLEVR question types for baselines and competitive models. The Human baseline is from the original CLEVR work. * denotes additional program supervision. SA refers to stacked spatial attention (Yang et al., 2015) . Integer dropping the learning rate to 1e - 5 for the last few epochs to Discussion training and for model selection during validation , but not We train on a single GTX 1080 TI and Overall Exist Query Epochs Model Compare Count Table 5. ResNetFeaturizer Table 6 . CNN 3 \u00d7 224 \u00d7 224 Table 5 . ResNetFeaturizer Output Size Table 7. Hyperparameter details for DDRprog. Only the learning rate and model size were coarsely cross validated due to hardware limitations: hyperparameter are not optimal. 2 3 Architecture 64",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Visual Question Answering\", \"Dataset\": \"CLEVR\", \"Metric\": \"Accuracy\", \"Score\": \"98.3\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Visual Question Answering\", \"Dataset\": \"CLEVR-Humans\", \"Metric\": \"Accuracy\", \"Score\": \"67.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Question Answering\", \"Dataset\": \"CLEVR\", \"Metric\": \"Accuracy\", \"Score\": \"99.7\"}} ]"
    },
    {
        "Context": "MINA: Multilevel Knowledge-Guided Attention for Modeling Electrocardiography Signals Electrocardiography (ECG) signals are commonly used to diagnose various cardiac abnormalities. Recently, deep learning models showed initial success on modeling ECG data, however they are mostly black-box, thus lack interpretability needed for clinical usage. In this work, we propose MultIlevel kNowledge-guided Attention networks (MINA) that predict heart diseases from ECG signals with intuitive explanation aligned with medical knowledge. By extracting multilevel (beat-, rhythm-and frequency-level) domain knowledge features separately, MINA combines the medical knowledge and ECG data via a multilevel attention model, making the learned models highly interpretable. Our experiments showed MINA achieved PR-AUC 0.9436 (outperforming the best baseline by 5.51%) in real world ECG dataset. Finally, MINA also demonstrated robust performance and strong interpretability against signal distortion and noise contamination. In this section, we first describe the dataset used for the experiments, followed by the description of the baseline models Table 2: Data profile of PhysioNet Challenge 2017 dataset Mean StDev Min # of points Median # recording",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Atrial Fibrillation Detection\", \"Dataset\": \"PhysioNet Challenge 2017\", \"Metric\": \"F1\", \"Score\": \"0.8342\"}}, {\"LEADERBOARD\": {\"Task\": \"Atrial Fibrillation Detection\", \"Dataset\": \"PhysioNet Challenge 2017\", \"Metric\": \"PR-AUC\", \"Score\": \"0.9436\"}}, {\"LEADERBOARD\": {\"Task\": \"Atrial Fibrillation Detection\", \"Dataset\": \"PhysioNet Challenge 2017\", \"Metric\": \"ROC-AUC\", \"Score\": \"0.9488\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Medical Image Generation\", \"Dataset\": \"Automatic Cardiac Diagnosis Challenge (ACDC)\", \"Metric\": \"Dice\", \"Score\": \"0.9336\"}}, { \"LEADERBOARD\": { \"Task\": \"Medical Image Generation\", \"Dataset\": \"Automatic Cardiac Diagnosis Challenge (ACDC)\", \"Metric\": \"Grad Det-Jac\", \"Score\": \"0.9336\"}}, { \"LEADERBOARD\": { \"Task\": \"Medical Image Generation\", \"Dataset\": \"Automatic Cardiac Diagnosis Challenge (ACDC)\", \"Metric\": \"Hausdorff Distance (mm)\", \"Score\": \"2.77\"}}, { \"LEADERBOARD\": { \"Task\": \"Medical Image Generation\", \"Dataset\": \"Automatic Cardiac Diagnosis Challenge (ACDC)\", \"Metric\": \"Hausdorff Distance (mm)\", \"Score\": \"60.94\"}}, { \"LEADERBOARD\": { \"Task\": \"Medical Image Generation\", \"Dataset\": \"Automatic Cardiac Diagnosis Challenge (ACDC)\", \"Metric\": \"Hausdorff Distance (mm)\", \"Score\": \"89.71\"}} ]"
    },
    {
        "Context": "From Random Graph to Small World by Wandering",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Free-Form Image Inpainting with Gated Convolution : Free-form image inpainting results by our system built on gated convolution. Each triad shows original image, free-form input and our result from left to right. The system supports free-form mask and guidance like user sketch. It helps user remove distracting objects, modify image layouts and edit faces in images.We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed fora single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: https://github.com/JiahuiYu/ generative_inpainting.  Table 2: Results of mean 1 error and mean 2 error on val- idation images of Places2 with both rectangle masks and free-form masks. Both PartialConv* and ours are trained on same random combination of rectangle and free-form masks. No edge guidance is utilized in training/inference to ensure fair comparison. * denotes our implementation within the same framework due to unavailability of official implementation and models. 2 err . rectangular mask 1 err . free - form mask",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Inpainting\", \"Dataset\": \"Places2 val\", \"Metric\": \"free-form mask l1 err\", \"Score\": \"9.1%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Inpainting\", \"Dataset\": \"Places2 val\", \"Metric\": \"free-form mask l2 err\", \"Score\": \"1.6%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Inpainting\", \"Dataset\": \"Places2 val\", \"Metric\": \"rect mask l1 error\", \"Score\": \"8.6%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Inpainting\", \"Dataset\": \"Places2 val\", \"Metric\": \"rect mask l2 err\", \"Score\": \"2.0%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Inpainting\", \"Dataset\": \"Paris StreetView\", \"Metric\": \"10-20% Mask PSNR\", \"Score\": \"28.73\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Inpainting\", \"Dataset\": \"Paris StreetView\", \"Metric\": \"20-30% Mask PSNR\", \"Score\": \"26.16\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Inpainting\", \"Dataset\": \"Paris StreetView\", \"Metric\": \"30-40% Mask PSNR\", \"Score\": \"24.26\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Inpainting\", \"Dataset\": \"Paris StreetView\", \"Metric\": \"40-50% Mask PSNR\", \"Score\": \"22.62\"}} ]"
    },
    {
        "Context": "Building One-Shot Semi-supervised (BOSS) Learning up to Fully Supervised Performance Reaching the performance of fully supervised learning with unlabeled data and only labeling one sample per class might be ideal for deep learning applications. We demonstrate for the first time the potential for building one-shot semi-supervised (BOSS) learning on Cifar-10 and SVHN up to attain test accuracies that are comparable to fully supervised learning. Our method combines class prototype refining, class balancing, and self-training. A good prototype choice is essential and we propose a technique for obtaining iconic examples. In addition, we demonstrate that class balancing methods substantially improve accuracy results in semi-supervised learning to levels that allow self-training to reach the level of fully supervised learning performance. Rigorous empirical evaluations provide evidence that labeling large datasets is not necessary for training deep neural networks. We made our code available at https://github.com/lnsmith54/BOSS to facilitate replication and for use with future real-world applications.  Table 1. Class accuracies. One-shot semi-supervised average (of 2 runs) class accuracies for Cifar-10 test data with the FixMatch model, that was trained on sets of manually chosen prototypes for each class. Prototype set 6 was modified from set 2 and prototype set 7 was modified from set 4 (i.e., prototype refining). deer horse Mean auto frog airplane bird cat truck ship dog Table 2. Main results. BOSS methods are compared using five sets of class prototypes (i.e., 1 prototype per class) for Cifar-10, plus two sets from prototype refining. The FixMatch column shows test accuracies (average and standard deviation of 4 runs) for the original FixMatch code on the prototype sets. The next four columns gives the accuracy results for the class balance methods (see text for a description of class balance methods). Results for the PyTorch reimplementation of FixMatch and modified with the BOSS methods are shown in brackets",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"cifar-10, 10 Labels\", \"Metric\": \"Accuracy (Test)\", \"Score\": \"95.1\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"CIFAR-10, 4000 Labels\", \"Metric\": \"Accuracy\", \"Score\": \"95.13\"}} ]"
    },
    {
        "Context": "On the Self-stabilization of Mobile Robots in Graphs On the Self-stabilization of Mobile Robots in Graphs Sur l'Auto-stabilisation de Robots Mobiles dans un Graphe Self-stabilization is a versatile technique to withstand any transient fault in a distributed system. Mobile robots (or agents) are one of the emerging trends in distributed computing as they mimic autonomous biologic entities. The contribution of this paper is threefold. First, we present anew model for studying mobile entities in networks subject to transient faults. Our model differs from the classical robot model because robots have constraints about the paths they are allowed to follow, and from the classical agent model because the number of agents remains fixed throughout the execution of the protocol. Second, in this model, we study the possibility of designing self-stabilizing algorithms when those algorithms are run by mobile robots (or agents) evolving on a graph. We concentrate on the core building blocks of robot and agents problems: naming and leader election. Not surprisingly, when no constraints are given on the network graph topology and local execution model, both problems are impossible to solve. Finally, using minimal hypothesis with respect to impossibility results, we provide deterministic and probabilistic solutions to both problems, and show equivalence of these problems by an algorithmic reduction mechanism.L'auto-stabilisation est une technique g\u00e9n\u00e9rique pour tol\u00e9rer toute d\u00e9faillance transitoire dans un syst\u00e8me distribu\u00e9. Les robots (ou agents) mobiles constituent l'un des mod\u00e8les\u00e9mergents de l'informatique distribu\u00e9e du fait de leur ressemblance avec les entit\u00e9s biologiques autonomes. La contribution de cet article est triple. D'abord, nous pr\u00e9sentons un nouveau mod\u00e8le pour l'\u00e9tude des entit\u00e9s mobiles dans des r\u00e9seaux sujets\u00e0 des d\u00e9faillances transitoires. Notre mod\u00e8le diff\u00e8re du mod\u00e8le classique des robots car les robots ont des contraintes sur les chemins qu'ils peuvent emprunter, et du mod\u00e8le classique des agents mobiles car le nombre d'agents reste fixe pendant toute la dur\u00e9e de l'ex\u00e9cution du protocole. Ensuite, dans ce mod\u00e8le, nous\u00e9tudions la possibilit\u00e9 de l'existence d'algorithmes auto-stabilisants quand ces algorithmes sont ex\u00e9cut\u00e9s par des robots mobiles\u00e9voluant dans un graphe. Nous nous concentrons sur les briques de bases des syst\u00e8mes\u00e0 base d'agents et de robots : le nomage et l'\u00e9lection d'un chef. Conform\u00e9ment\u00e0 l'intuition, quand aucune contrainte n'est faite sur le r\u00e9seau et les param\u00e8tres de l'ex\u00e9cution, les deux probl\u00e8mes sont impossibles\u00e0 r\u00e9soudre. Enfin, quand les hypoth\u00e8ses minimales pour r\u00e9soudre ces deux probl\u00e8mes sont disponibles, nous proposons des solutions d\u00e9terministes et probabilistes pour les deux probl\u00e8mes, et montrons que ces deux probl\u00e8mes sont\u00e9quivalents au moyen d'une r\u00e9duction algorithmique.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "DCNAS: Densely Connected Neural Architecture Search for Semantic Image Segmentation Existing NAS methods for dense image prediction tasks usually compromise on restricted search space or search on proxy task to meet the achievable computational demands. To allow as wide as possible network architectures and avoid the gap between realistic and proxy setting, we propose a novel Densely Connected NAS (DCNAS) framework, which directly searches the optimal network structures for the multi-scale representations of visual information, over a large-scale target dataset without proxy. Specifically, by connecting cells with each other using learnable weights, we introduce a densely connected search space to cover an abundance of mainstream network designs. Moreover, by combining both path-level and channel-level sampling strategies, we design a fusion module and mixture layer to reduce the memory consumption of ample search space, hence favoring the proxyless searching. Compared with contemporary works, experiments reveal that the proxyless searching scheme is capable of bridging the gap between searching and training environments. Further, DC-NAS achieves new state-of-the-art performances on public semantic image segmentation benchmarks, including 84.3% on Cityscapes, and 86.9% on PASCAL VOC 2012. We also retain leading performances when evaluating the architecture on the more challenging ADE20K and PASCAL-Context dataset. To evaluate DCNAS comprehensively, we first verify the effect of the proxyless searching paradigm by measuring the correlation of the performance between the searching and training environments Secondly, we quantitatively demonstrate the superiority of DCNAS on several broadly used benchmarks according to corresponding evaluation metrics Table 1. Comparisons with state of the arts. The table presents the comparison results of mIoU ( % ) \u2191 FLOPs ( G ) \u03c4 \u2191 \u03c1 \u2191 GPU Days Table 2. Performance on Cityscapes. The table summarizes the performance on Cityscapes testing dataset. Validation: Models trained with both train-fine and val-fine parts. ImageNet: The backbones of model trained on ImageNet. Coarse: Models that exploit extra datas in Cityscapes with coarse annotation. \u2020 : Adopts the well labeled Mapillary Vistas [45] dataset. mIoU ( % ) Table 3. Performance on PASCAL VOC 2012. The table presents per-class semantic segmentation results on the PASCAL VOC 2012 test dataset. Our method advances the new state-of-the-art with mIoU 86.9%. - Methods mIoU Pascal Context ADE20K Pix - Acc mIoU 60 - cls mIoU 59 - cls - Table 2 presents the comparison results of our method and several state-of-the-art methods. Without - Methods mIoU",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"ADE20K val\", \"Metric\": \"mIoU\", \"Score\": \"47.12\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"83.6%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL Context\", \"Metric\": \"mIoU\", \"Score\": \"55.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL VOC 2012 test\", \"Metric\": \"Mean IoU\", \"Score\": \"86.9%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"ADE20K val\", \"Metric\":\"mIoU\", \"Score\": \"86.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"85.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL Context\", \"Metric\":\"mIoU\", \"Score\": \"83.9\"}} ]"
    },
    {
        "Context": "The Heap Lambda Machine This paper introduces anew machine architecture for evaluating lambda expressions using the normalorder reduction, which guarantees that every lambda expression will be evaluated if the expression has its normal form and the system has enough memory. The architecture considered here operates using heap memory only. Lambda expressions are represented as graphs, and all algorithms used in the processing unit of this machine are non-recursive.ZU064-05-FPR heap 9 September 2018 12:33 2 A. SalikhmetovThe memory manager in our machine consists of a single register and three commands only. Taking into account the similarity of our memory allocator and heap-based dynamic memory allocators, we have decided to refer this machine to as the Heap Lambda Machine. Worth mentioning here is also the fact that careful design of the processing unit algorithms allowed us to avoid using garbage collection.The purpose of this paper is to describe the architecture of our machine and to demonstrate all vital parts of the emulator. In order to evaluate lambda expression in the memory, the machine walks through the expression tree and looks for nodes that can be reduced",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Meta-Learned Confidence for Few-shot Learning Transductive inference is an effective means of tackling the data deficiency problem in few-shot learning settings. A popular transductive inference technique for few-shot metric-based approaches, is to update the prototype of each class with the mean of the most confident query examples, or confidence-weighted average of all the query samples. However, a caveat here is that the model confidence maybe unreliable, which may lead to incorrect predictions. To tackle this issue, we propose to meta-learn the confidence for each query sample, to assign optimal weights to unlabeled queries such that they improve the model's transductive inference performance on unseen tasks. We achieve this by meta-learning an input-adaptive distance metric over a task distribution under various model and data perturbations, which will enforce consistency on the model predictions under diverse uncertainties for unseen tasks. Moreover, we additionally suggest a regularization which explicitly enforces the consistency on the predictions across the different dimensions of a high-dimensional embedding vector. We validate our few-shot learning model with meta-learned confidence on four benchmark datasets, on which it largely outperforms strong recent baselines and obtains new state-of-the-art results. Further application on semi-supervised few-shot learning tasks also yields significant performance improvements over the baselines. The source code of our algorithm is available at https://github.com/seongmin-kye/MCT.Preprint. Under review. Dataset We validate our method on four popular benchmark datasets for few-shot classification Please seethe Section A.1 of the appendix regarding the detailed information for each of the datasets For our full models, we evaluate the expectation over task distribution p(\u03c4 ) via Monte-Carlo (MC) approximation with a single sample during training to obtain the learning objective, where we set \u03bb = 0.5 which we found with a validation set We validate our method on four benckmark datasets for few-shot classification This dataset consists of a subset of 100 classes sampled from the ImageNet dataset This dataset is another subset of ImageNet, that consists of 779, 165 images of 84 \u00d7 84 pixels collected from 608 classes Table 1: Average classification performance over 1000 randomly generated episodes, with 95% confidence Inductive Backbone 58 . 56\u00b10 . 39 63 . 85\u00b10 . 48 75 . 53\u00b10 . 80 64 . 49\u00b10 . 64 78 . 63\u00b10 . 46 76 . 65\u00b10 . 38 79 . 44\u00b10 . 34 82 . 15\u00b10 . 45 61 . 20\u00b11 . 80 59 . 63\u00b10 . 52 79 . 01\u00b10 . 13 76 . 36\u00b10 . 10 62 . 64\u00b10 . 61 65 . 34\u00b10 . 63 5 - shot 76 . 34\u00b10 . 48 61 . 26\u00b10 . 20 69 . 86\u00b10 . 65 81 . 63\u00b10 . 44 61 . 65\u00b10 . 15 miniImageNet 55 . 51\u00b10 . 86 1 - shot Table 2: Average classification performance on CIFAR-FS and FC100. Table 4: Semi-supervised few-shot classification performance. We consider 5-way classification on miniIm- 5 - shot mini tiered w / D",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"Mini-ImageNet - 1-Shot Learning\", \"Metric\": \"Accuracy\", \"Score\": \"78.55%\"}}, {\"LEADERBOARD\": {\"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"Mini-Imagenet 5-way (1-shot)\", \"Metric\": \"Accuracy\", \"Score\": \"78.55\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"OMNIGLOT - 5-Shot, 20-way\", \"Metric\": \"Accuracy\", \"Score\": \"99.35%\"}}, { \"LEADERBOARD\": { \"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"OMNIGLOT - 1-Shot, 20-way\", \"Metric\": \"Accuracy\", \"Score\": \"99.65%\"}}, { \"LEADERBOARD\": { \"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"OMNIGLOT - 1-Shot, 5-way\", \"Metric\": \"Accuracy\", \"Score\": \"99.39%\"}}, { \"LEADERBOARD\": { \"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"OMNIGLOT - 1-Shot, 5-way\", \"Metric\": \"Accuracy\", \"Score\": \"98.20%\"}}, { \"LEADERBOARD\": { \"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"OMNIGLOT - 1-Shot, 20-way\", \"Metric\": \"Accuracy\", \"Score\": \"98.52%\"}}, { \"LEADERBOARD\": { \"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"OMNIGLOT - 1-Shot, 20-way\", \"Metric\": \"Accuracy\", \"Score\": \"98."
    },
    {
        "Context": "A computational method for bounding the probability of reconstruction on trees For a tree Markov random field non-reconstruction is said to hold if as the depth of the tree goes to infinity the information that atypical configuration at the leaves gives about the value at the root goes to zero. The distribution of the measure at the root conditioned on atypical boundary can be computed using a distributional recurrence. However the exact computation is not feasible because the support of the distribution grows exponentially with the depth.In this work, we introduce a notion of a survey of a distribution over probability vectors which is a succinct representation of the true distribution. We show that a survey of the distribution of the measure at the root can be constructed by an efficient recursive algorithm. The key properties of surveys are that the size does not grow with the depth, they can be constructed recursively, and they still provide a good bound for the distance between the true conditional distribution and the unconditional distribution at the root. This approach applies to a large class of Markov random field models including randomly generated ones. As an application we show bounds on the reconstruction threshold for the Potts model on small-degree trees.  Table 1: The Kesten-Stigum upper bound on the non-reconstruction threshold and the values of \u03bb up to which non-reconstruction has been shown in d Theorem 12",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Powers of layers for image-to-image translation We propose a simple architecture to address unpaired image-to-image translation tasks: style or class transfer, denoising, deblurring, deblocking, etc. We start from an image autoencoder architecture with fixed weights. For each task we learn a residual block operating in the latent space, which is iteratively called until the target domain is reached. A specific training schedule is required to alleviate the exponentiation effect of the iterations. At test time, it offers several advantages: the number of weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with the number of iterations. This is useful, for instance, when the type or amount of noise to suppress is not known in advance. Experimentally, we provide proofs of concepts showing the interest of our method for many transformations. The performance of our model is comparable or better than CycleGAN with significantly fewer parameters.Generative adversarial networks (GANs) [13] is a framework where two networks, a generator and a discriminator, are learned together in a zero-sum game fashion. The generator learns to produce more and more realistic images wrt. the training dataset with real images. The discriminator learns to discriminate better and better between real data and generated images. GANs are used in many tasks such as domain adaptation, style transfer, inpainting and talking head generation [3,24,33,44]. We report results for 6 of the 8 unpaired image-to-image translation tasks introduced in the CycleGAN paper (the two remaining ones lead to the same conclusions) and we used the datasets from the website The FID measures the similarity between two datasets of images, we use it to compare the target dataset with the transformed dataset Table 2: PSNR on Urban-100. Comparison between our choice (PoL) and independent blocks. independent Gaussian noise ( std 30 ) POL Gaussian blur ( sigma 4 ) Table 3: Comparison between different maximum number of compositions. We report the most adapted metric on Urban-100: PSNR for the Gaussian noise and blur, NIQE for deblocking. NIQE ( lower=better ) JPEG ( quality=25 ) PSNR ( higher=better ) Gaussian noise ( std 30 ) Gaussian blur ( \u03c3=4 ) Table 4: PSNR on Urban-100 [17] -Gaussian noise (std=30). Comparison between augmentation steps during the warm-up phase. The augmentation step is to the number of epochs performed with the same number of compositions (1 epoch corresponds to 800 backward passes). 1 augmentation step 2 4 16 8 ntr Table 6: Comparison between our approach (PoL) and CycleGAN. We three tasks, all computed with the Urban-100 dataset: PSNR (higher is better) with different amount of",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"photo2vangogh\", \"Metric\": \"Frechet Inception Distance\", \"Score\": \"152.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"photo2vangogh\", \"Metric\": \"Number of params\", \"Score\": \"15.9M\"}}, {\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"zebra2horse\", \"Metric\": \"Frechet Inception Distance\", \"Score\": \"112.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"zebra2horse\", \"Metric\": \"Number of params\", \"Score\": \"15.9M\"}}, {\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"horse2zebra\", \"Metric\": \"Frechet Inception Distance\", \"Score\": \"53.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"horse2zebra\", \"Metric\": \"Number of params\", \"Score\": \"15.9M\"}}, {\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"vangogh2photo\", \"Metric\": \"Frechet Inception Distance\", \"Score\": \"134.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"vangogh2photo\", \"Metric\": \"Number of Params\", \"Score\": \"15.9M\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image-to-Image Translation\", \"Dataset\": \"Urban100\", \"Metric\": \"PSNR\", \"Score\": \"21.12\"}}, { \"LEADERBOARD\": { \"Task\": \"Image-to-Image Translation\", \"Dataset\": \"Paris Street\", \"Metric\": \"PSNR\", \"Score\": \"23.25\"}}, { \"LEADERBOARD\": { \"Task\": \"Image-to-Image Translation\", \"Dataset\": \"Paris Street\", \"Metric\": \"SSIM\", \"Score\": \"0.89\"}} ]"
    },
    {
        "Context": "Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace Clustering State-of-the-art subspace clustering methods are based on expressing each data point as a linear combination of other data points while regularizing the matrix of coefficients with \u2113 1 , \u2113 2 or nuclear norms. \u2113 1 regularization is guaranteed to give a subspace-preserving affinity (i.e., there are no connections between points from different subspaces) under broad theoretical conditions, but the clusters may not be connected. \u2113 2 and nuclear norm regularization often improve connectivity, but give a subspace-preserving affinity only for independent subspaces. Mixed \u2113 1 , \u2113 2 and nuclear norm regularizations offer a balance between the subspacepreserving and connectedness properties, but this comes at the cost of increased computational complexity. This paper studies the geometry of the elastic net regularizer (a mixture of the \u2113 1 and \u2113 2 norms) and uses it to derive a provably correct and scalable active set method for finding the optimal coefficients. Our geometric analysis also provides a theoretical justification and a geometric interpretation for the balance between the connectedness (due to \u2113 2 regularization) and subspace-preserving (due to \u2113 1 regularization) properties for elastic net subspace clustering. Our experiments show that the proposed active set method not only achieves state-of-the-art clustering performance, but also efficiently handles large-scale datasets. We test our method on the four datasets presented in The Coil-100 dataset contains 7,200 grayscale images of 100 different objects The PIE dataset contains images of the faces of 68 people taken under 13 different poses, 43 different illuminations, and 4 different expressions The MNIST dataset contains 70,000 images of handwritten digits 0-9 Table 1. Dataset information. ORGEN ( w . SPAMS ) 100 1 ] ; and ( b , c ) N = 100 , 000 and \u03bb \u2208 [ 0 . 05 , 0 . 999 ] . 700 \u2212 2 \u2212 1 n ( #groups ) GPSR ( a ) Running time versus N N 0 1 100 RFSS 2 entries #Nonzero 200 3 ORGEN ( w . RFSS ) 300 400 1024 600 ( b ) Running time versus \u03bb Running ( sec ) \u03bb D ( ambient dim . ) Table 2. Performance of different clustering algorithms. The running time includes the time for computing the affinity matrix and for performing spectral clustering. The sparsity is the number of nonzero coefficients in each representation cj averaged over j = 1, \u00b7 \u00b7 \u00b7 , N . The value \"M\" means that the memory limit of 16GB was exceeded,",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Clustering\", \"Dataset\": \"MNIST-full\", \"Metric\": \"Accuracy\", \"Score\": \"0.976\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Clustering\", \"Dataset\": \"MNIST-full\", \"Metric\": \"NMI\", \"Score\": \"0.937\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Clustering\", \"Dataset\": \"coil-100\", \"Metric\": \"Accuracy\", \"Score\": \"0.6924\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Clustering\", \"Dataset\": \"Extended Yale-BNIST\", \"Metric\": \"Accuracy\", \"Score\": \"0.706\"}} ]"
    },
    {
        "Context": "CurveLane-NAS: Unifying Lane-Sensitive Architecture Search and Adaptive Point Blending We address the curve lane detection problem which poses more realistic challenges than conventional lane detection for better facilitating modern assisted/autonomous driving systems. Current handdesigned lane detection methods are not robust enough to capture the curve lanes especially the remote parts due to the lack of modeling both long-range contextual information and detailed curve trajectory. In this paper, we propose a novel lane-sensitive architecture search framework named CurveLane-NAS to automatically capture both long-ranged coherent and accurate short-range curve information. It consists of three search modules: a) a feature fusion search module to find a better fusion of the local and global context for multi-level hierarchy features; b) an elastic backbone search module to explore an efficient feature extractor with good semantics and latency; c) an adaptive point blending module to search a multi-level post-processing refinement strategy to combine multi-scale head prediction. Furthermore, we also steer forward to release a more challenging benchmark named CurveLanes for addressing the most difficult curve lanes. It consists of 150K images with 680K labels. 3 Experiments on the new CurveLanes show that the SOTA lane detection methods suffer substantial performance drop while our model can still reach an 80+% F1-score. Extensive experiments on traditional lane benchmarks such as CULane also demonstrate the superiority of our CurveLane-NAS, e.g. achieving anew SOTA 74.8% F1-score on CULane. We conduct neural architecture search on two large lane detection datasets: the CULane, and the new CurveLanes dataset Evaluation metrics Evaluation metrics is important since it is also the target of our architecture search We follow the literature and use the corresponding evaluation metrics for each particular dataset Following the official implementation of the evaluation, we compute the intersection-over-union (IoU) between GT labels and predictions, where each lane has 30 pixel width The F1 measure is used as the evaluation metric: F 1 = 2\u00d7P recision\u00d7Recall P recision+Recall , where P recision = T PT P +F P and Recall = T PT P +F N We also use the official metric as the evaluation metrics: Accuracy = N pred N GT , where where N pred is the number of correctly predicted lane points and N GT is the number of ground-truth lane points We found that fora large dataset Table 2. Comparison of F1-measure of the state-of-the-art models on CULane test set. CurveLane-S, CurveLane-M, and CurveLane-L are the searched architectures of our method. Our method outperforms the SOTA models by a large margin with a small computational overhead. 1998 2359 Searched SCNN R101 1640 ENet 1990 SCNN [ 20 ] SAD [ 11 ] SAD PointLane [ 6 ] CurveLane - S CurveLane - M CurveLane - L 2817 2052 1746 Table 3. Comparison of different algorithms on the new dataset CurveLanes. CurveLane-S, CurveLane-M, and CurveLane-L are the searched architectures of our method. The SOTA methods such as SCNN and SAD suffer substantial performance drop (20%\u02dc30% F1 score). FLOPS ( G ) Precision Recall F1 Table 4. Ablative study with the F1-measure on the CULane dataset. CurveLane S to L denote our searched backbone architectures. The performance of models combined with all the modules are listed in the final column.",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Lane Detection\", \"Dataset\": \"CULane\", \"Metric\": \"F1 score\", \"Score\": \"74.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Lane Detection\", \"Dataset\": \"CULane\", \"Metric\": \"F1 score\", \"Score\": \"73.5\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Lane Detection\", \"Dataset\": \"CULane\", \"Metric\": \"F1 score\", \"Score\": \"74.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Lane Detection\", \"Dataset\": \"CULane\", \"Metric\": \"F1 score\", \"Score\": \"84.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Lane Detection\", \"Dataset\": \"CULane\", \"Metric\": \"F1 score\", \"Score\": \"95.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Lane Detection\", \"Dataset\": \"CULane\", \"Metric\": \"F1 score\", \"Score\": \"94.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Lane Detection\", \"Dataset\": \"CULane\", \"Metric\": \"F1 score\", \"Score\": \"90.6\"}}, { \"LEADERBOARD\": { \"Task\": \"Lane Detection\", \"Dataset\": \"CULane\", \"Metric\": \"F1 score\", \"Score\": \"96.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Lane Detection\", \"Dataset\": \"CULane\", \"Metric\": \"F1 score"
    },
    {
        "Context": "Graph-based Reduction of Program Verification Conditions Increasing the automaticity of proofs in deductive verification of C programs is a challenging task. When applied to industrial C programs known heuristics to generate simpler verification conditions are not efficient enough. This is mainly due to their size and a high number of irrelevant hypotheses.This work presents a strategy to reduce program verification conditions by selecting their relevant hypotheses. The relevance of a hypothesis is determined by the combination of a syntactic analysis and two graph traversals. The first graph is labeled by constants and the second one by the predicates in the axioms. The approach is applied on a benchmark arising in industrial program verification.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop Model-based human pose estimation is currently approached through two different paradigms. Optimizationbased methods fit a parametric body model to 2D observations in an iterative manner, leading to accurate imagemodel alignments, but are often slow and sensitive to the initialization. In contrast, regression-based methods, that use a deep network to directly estimate the model parameters from pixels, tend to provide reasonable, but not pixel accurate, results while requiring huge amounts of supervision. In this work, instead of investigating which approach is better, our key insight is that the two paradigms can form a strong collaboration. A reasonable, directly regressed estimate from the network can initialize the iterative optimization making the fitting faster and more accurate. Similarly, a pixel accurate fit from iterative optimization can act as strong supervision for the network. This is the core of our proposed approach SPIN (SMPL oPtimization IN the loop). The deep network initializes an iterative optimization routine that fits the body model to 2D joints within the training loop, and the fitted estimate is subsequently used to supervise the network. Our approach is self-improving by nature, since better network estimates can lead the optimization to better solutions, while more accurate optimization fits provide better supervision for the network. We demonstrate the effectiveness of our approach in different settings, where 3D ground truth is scarce, or not available, and we consistently outperform the state-of-the-art model-based pose estimation approaches by significant margins. The project website with videos, results, and code can be found at https://seas.upenn.edu/\u02dcnkolot/projects/spin. Here we give a quick description of the datasets we use for training and evaluation We train using the first three datasets (no training data from 3DPW), while similarly to, we also incorporate training data with 2D annotations from other datasets, i.e., LSP-Extended, MPII, and COCO For the different settings we investigate, e.g., training with/without in the loop update, or training with/without 3D ground truth), we train a single model per setting and we use it to report results on all datasets, without fine-tuning on each particular dataset Moreover, we clarify, that we always evaluate the network\"s output Also, since different datasets often use different error metrics to report results, we use the metrics that are more often met in the literature for each dataset tocols, e.g.,, we use subjects S1, S5, S6, S7, S8 for training and we evaluate on subjects S9 and S11 MPI-INF-3DHP: It is a dataset captured Table 1: Evaluation on the 3DPW dataset. The numbers are mean reconstruction errors in mm. The model-based supervision alone (Ours -static fits) outperforms similar architectures trained on the same ([15, 17]) or more data ([3, 16]). Incorporating the fitting in the loop (Ours -in the loop) further improves performance. Rec . Error Table 2: Evaluation on foreground-background and six-part seg- mentation on the LSP test set. The numbers are accuracies and f1 Part Seg . FB Seg . acc . f1 Table 4: Evaluation on the MPI-INF-3DHP dataset. The compar- ison is under different metrics before (left) and after (right) rigid alignment. Our approach outperforms the previous baselines. (For PCK and AUC, higher is better, while for MPJPE, lower is better). PCK AUC MPJPE PCK AUC MPJPE Rigid Alignment Absolute -",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"MPI-INF-3DHP\", \"Metric\": \"3DPCK\", \"Score\": \"92.5\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"MPI-INF-3DHP\", \"Metric\": \"AUC\", \"Score\": \"55.6\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"MPI-INF-3DHP\", \"Metric\": \"MJPE\", \"Score\": \"67.5\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"3D Poses in the Wild Challenge\", \"Metric\": \"MPJAE\", \"Score\": \"25.42\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"3D Poses in the Wild Challenge\", \"Metric\": \"MPJPE\", \"Score\": \"102.56\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"3DPW\", \"Metric\": \"MPJPE\", \"Score\": \"96.9\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"3DPW\", \"Metric\": \"MPVPE\", \"Score\": \"116.4\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"3DPW\", \"Metric\": \"PA-MPJPE\", \"Score\": \"59.2\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"3DPW\", \"Metric\": \"acceleration error\", \"Score\": \"29.8\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"3D Poses in the Wild Challenge\", \"Metric\": \"MPJAE\", \"Score\": \"19.69\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"3D Poses in the Wild Challenge\", \"Metric\": \"MPJPE\", \"Score\": \"83.15\"}} ]"
    },
    {
        "Context": "TRANSFORMER-BASED ACOUSTIC MODELING FOR HYBRID SPEECH RECOGNITION We propose and evaluate transformer-based acoustic models (AMs) for hybrid speech recognition. Several modeling choices are discussed in this work, including various positional embedding methods and an iterated loss to enable training deep transformers. We also present a preliminary study of using limited right context in transformer models, which makes it possible for streaming applications. We demonstrate that on the widely used Librispeech benchmark, our transformer-based AM outperforms the best published hybrid result by 19% to 26% relative when the standard n-gram language model (LM) is used. Combined with neural network LM for rescoring, our proposed approach achieves state-of-the-art results on Librispeech. Our findings are also confirmed on a much larger internal dataset. To evaluate the effectiveness of the proposed transformer-based acoustic model, we first perform experiments on the Librispeech corpus This corpus contains about 960 hours of read speech data for training, and 4 development and test sets ({dev, test} -{clean,other}), where other sets are more acoustic challenging ing, the best checkpoints for test-clean and test-other are selected separately on the corresponding development sets 4 Finally, we perform a large scale experiment on one of our internal tasks, English video ASR 3 test sets are used for evaluation purpose: an 8.5-hour curated set of carefully select very clean videos, an 19-hour clean set and a 18.6-hour noisy set For our initial evaluation purpose, both training and test sets are segmented into maximum 10 second segments Table 1: Effect of Positional Embeddings (PE) for Transformer. PE test-clean test-other None 3.11 6.94 Sinusoid 3.13 6.67 Frame Stacking 3.04 6.64 Convolution 2.87 6.46 test - clean test - other Table 1 : Effect of Positional Embeddings ( PE ) for Transformer . Table 2: Architecture comparison on the Librispeech benchmark Model Arch #Params (M) test-clean test-other BLSTM (800,5) 79 3.11 7.44 Trf-FS (768,12) 91 3.04 6.64 vggBLSTM (800,5) 95 2.99 6.95 vggTrf. (768,12) 93 2.87 6.46 vggBLSTM (1000,6) 163 2.86 6.63 vggTrf. (768, 20) 149 2.77 6.10 CE losses are calculated separately . These additional CE losses are test - clean test - other Table 2 : Architecture comparison on the Librispeech benchmark Table 3: Using iterated loss to train deep transformer models. Model Arch Iter Loss test-clean test-other vggTrf. (768, 12) N 2.87 6.46 (Params: 93M) Y 2.77 6.10 vggTrf. (512, 24) N not converged (Params: 81M) Y",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Speech Recognition\", \"Dataset\": \"LibriSpeech test-clean\", \"Metric\": \"Word Error Rate (WER)\", \"Score\": \"2.26\"}}, {\"LEADERBOARD\": {\"Task\": \"Speech Recognition\", \"Dataset\": \"LibriSpeech test-other\", \"Metric\": \"Word Error Rate (WER)\", \"Score\": \"4.85\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Speech Recognition\", \"Dataset\": \"LibriSpeech test-clean\", \"Metric\": \"Word Error Rate (WER)\", \"Score\": \"1.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Speech Recognition\", \"Dataset\": \"LibriSpeech test-other\", \"Metric\": \"Word Error Rate (WER)\", \"Score\": \"16.5\"}} ]"
    },
    {
        "Context": "EVOLVING MULTIALGEBRAS UNIFY ALL USUAL SEQUENTIAL COMPUTATION MODELS It is well-known that Abstract State Machines (ASMs) can simulate \"stepby-step\" any type of machines (Turing machines, RAMs, etc.). We aim to overcome two facts: 1) simulation is not identification, 2) the ASMs simulating machines of some type do not constitute a natural class among all ASMs. We modify Gurevich's notion of ASM to that of EMA (\"Evolving MultiAlgebra\") by replacing the program (which is a syntactic object) by a semantic object: a functional which has to be very simply definable over the static part of the ASM. We prove that very natural classes of EMAs correspond via \"literal identifications\" to slight extensions of the usual machine models and also to grammar models. Though we modify these models, we keep their computation approach: only some contingencies are modified. Thus, EMAs appear as the mathematical model unifying all kinds of sequential computation paradigms.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "DEEP GRAPH LIBRARY: A GRAPH-CENTRIC, HIGHLY- PERFORMANT PACKAGE FOR GRAPH NEURAL NET- WORKS Advancing research in the emerging field of deep graph learning requires new tools to support tensor computation over graphs. In this paper, we present the design principles and implementation of Deep Graph Library (DGL) 1 . DGL distills the computational patterns of GNNs into a few generalized sparse tensor operations suitable for extensive parallelization. By advocating graph as the central programming abstraction, DGL can perform optimizations transparently. By cautiously adopting a framework-neutral design, DGL allows users to easily port and leverage the existing components across multiple deep learning frameworks. Our evaluation shows that DGL significantly outperforms other popular GNN-oriented frameworks in both speed and memory consumption over a variety of benchmarks and has little overhead for small scale workloads.  Table 2: The categorized number of lines of codes (LoC) need to change when porting a GNN layer in DGL from PyTorch to TensorFlow. Code comments are excluded. Total II Change / Total III Change Type Table 3: Epoch running time in seconds (full graph training). OOM means out-of-memory. Link Prediction Node Classification 4 OOM PyG over DGL - PT Model DGL GraphNets over DGL - TF 8 CPU GPU PyG Table 4: Epoch running time in seconds for mini- batch training using neighbor sampling (NS) and cluster sampling (CS). Link Prediction Node Classification PyG over DGL - PT GraphNets over DGL - TF DGL Model PyG",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"Cora\", \"Metric\": \"Accuracy\", \"Score\": \"83.98%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Drug-targeting\", \"Metric\": \"Accuracy\", \"Score\": \"71.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Drug- Moderate val\", \"Metric\": \"Accuracy\", \"Score\": \"80.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Coauthor CS\", \"Metric\": \"Accuracy\", \"Score\": \"94.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Coauthor Physics\", \"Metric\": \"Accuracy\", \"Score\": \"94.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Cora\", \"Metric\": \"Accuracy\", \"Score\": \"84.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Cora\", \"Metric\": \"Accuracy\", \"Score\": \"80.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"CiteSeer with Public Split: fixed 20 nodes per class\", \"Metric\": \"Accuracy\", \"Score\": \"73.3\"}}, { "
    },
    {
        "Context": "Improving Performance of Cluster Based Routing Protocol using Cross-Layer Design The main goal of routing protocol is to efficiency delivers data from source to destination. All routing protocols are the same in this goal, but the way they adopt to achieve it is different, so routing strategy has an egregious role on the performance of an ad hoc network. Most of routing protocols proposed for ad hoc networks have a flat structure. These protocols expand the control overhead packets to discover or maintain a route. On the other hand a number of hierarchical-based routing protocols have been developed, mostly are based on layered design. These protocols improve network performances especially when the network size grows up since details about remote portion of network can be handled in an aggregate manner. Although, there is another approach to design a protocol called cross-layer design. Using this approach information can exchange between different layer of protocol stack, result in optimizing network performances.In this paper, we intend to exert cross-layer design to optimize Cluster Based Routing Protocol (Cross-CBRP). Using NS-2 network simulator we evaluate rate of cluster head changes, throughput and packet delivery ratio. Comparisons denote that Cross-CBRP has better performances with respect to the original CBRP.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Detail-revealing Deep Video Super-resolution Previous CNN-based video super-resolution approaches need to align multiple frames to the reference. In this paper, we show that proper frame alignment and motion compensation is crucial for achieving high quality results. We accordingly propose a \"sub-pixel motion compensation\" (SPMC) layer in a CNN framework. Analysis and experiments show the suitability of this layer in video SR. The final end-to-end, scalable CNN framework effectively incorporates the SPMC layer and fuses multiple frames to reveal image details. Our implementation can generate visually and quantitatively high-quality results, superior to current state-of-the-arts, without the need of parameter tuning. To our knowledge, there is no such publicly available video dataset that is large enough to train our deep networks Table 1. Performance of baseline models DF - 0up Ours DF - Bic Table 1 . Performance of baseline models BW Table 2. Comparison with video SR methods (PSNR/SSIM) VSRNet Table 2 . Comparison with video SR methods ( PSNR / SSIM ) VSRnet Bicubic VESPCN Ours ( F3 ) Ours ( F5 ) BayesSR DESR Table 3. Comparison with image SR methods (PSNR/SSIM) - SRCNN Ours ( F1 ) FSRCNN VDSR Ours ( F3 ) Table 3 . Comparison with image SR methods ( PSNR / SSIM )",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"Set5 - 4x upscaling\", \"Metric\": \"PSNR\", \"Score\": \"30.96\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"Set5 - 4x upscaling\", \"Metric\": \"SSIM\", \"Score\": \"0.87\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"Set14 - 4x upscaling\", \"Metric\": \"PSNR\", \"Score\": \"27.57\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Super-Resolution\", \"Dataset\": \"Set14 - 4x upscaling\", \"Metric\": \"SSIM\", \"Score\": \"0.76\"}}, {\"LEADERBOARD\": {\"Task\": \"Video Super-Resolution\", \"Dataset\": \"Vid4 - 4x upscaling\", \"Metric\": \"PSNR\", \"Score\": \"25.88\"}}, {\"LEADERBOARD\": {\"Task\": \"Video Super-Resolution\", \"Dataset\": \"Vid4 - 4x upscaling\", \"Metric\": \"SSIM\", \"Score\": \"0.774\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Video Super-Resolution\", \"Dataset\": \"Vid4 - 4x upscaling\", \"Metric\": \"PSNR\", \"Score\": \"27.16\"}}, { \"LEADERBOARD\": { \"Task\": \"Video Super-Resolution\", \"Dataset\": \"Vid4 - 4x upscaling\", \"Metric\": \"SSIM\", \"Score\": \"0.75\"}}, { \"LEADERBOARD\": { \"Task\": \"Video Super-Resolution\", \"Dataset\": \"Vid4 - 2x upscaling\", \"Metric\": \"PSNR\", \"Score\": \"31.10\"}}, { \"LEADERBOARD\": { \"Task\": \"Video Super-Resolution\", \"Dataset\": \"Vid4 - 2x upscaling\", \"Metric\": \"SSIM\", \"Score\": \"0.88\"}}, { \"LEADERBOARD\": { \"Task\": \"Video Super-Resolution\", \"Dataset\": \"Set5 - 2x upscaling\", \"Metric\": \"PSNR\", \"Score\": \"37.94\"}}, { \"LEADERBOARD\": { \"Task\": \"Video Super-Resolution\", \"Dataset\": \"Set5 - 2x upscaling\", \"Metric\": \"SSIM\", \"Score\": \"0.96\"}}, { \"LEADERBOARD\": { \"Task\": \"Video Super-Resolution\", \"Dataset\": \""
    },
    {
        "Context": "Enhancing VAEs for Collaborative Filtering: Flexible Priors & Gating Mechanisms Neural network based models for collaborative filtering have started to gain attention recently. One branch of research is based on using deep generative models to model user preferences where variational autoencoders were shown to produce state-of-the-art results. However, there are some potentially problematic characteristics of the current variational autoencoder for CF. The first is the too simplistic prior that VAEs incorporate for learning the latent representations of user preference. The other is the model\"s inability to learn deeper representations with more than one hidden layer for each network. Our goal is to incorporate appropriate techniques to mitigate the aforementioned problems of variational autoencoder CF and further improve the recommendation performance. Our work is the first to apply flexible priors to collaborative filtering and show that simple priors (in original VAEs) maybe too restrictive to fully model user preferences and setting a more flexible prior gives significant gains. We experiment with the VampPrior, originally proposed for image generation, to examine the effect of flexible priors in CF. We also show that VampPriors coupled with gating mechanisms outperform SOTA results including the Variational Autoencoder for Collaborative Filtering by meaningful margins on 2 popular benchmark datasets (MovieLens & Netflix). Experiments were conducted to evaluate the effect of flexible priors, hierarchical stochastic units and gating mechanisms in the context of collaborative filtering Table 1: Results for MovieLens 20M and Netflix dataset. Standard errors are around 0.002 for ML-20M and 0.001 for Netflix. NDCG@100 Recall@20 Recall@50 Table 2: Comparison of performance between Gated and Un- Gated for models of different depth 3 . The model with better performance (1 Layer vs 2 Layers) is marked in bold. No - Gate Gated",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Recommendation Systems\", \"Dataset\": \"MovieLens 20M\", \"Metric\": \"Recall@20\", \"Score\": \"0.41308\"}}, {\"LEADERBOARD\": {\"Task\": \"Recommendation Systems\", \"Dataset\": \"MovieLens 20M\", \"Metric\": \"Recall@50\", \"Score\": \"0.55109\"}}, {\"LEADERBOARD\": {\"Task\": \"Recommendation Systems\", \"Dataset\": \"MovieLens 20M\", \"Metric\": \"nDCG@100\", \"Score\": \"0.44522\"}}, {\"LEADERBOARD\": {\"Task\": \"Recommendation Systems\", \"Dataset\": \"Netflix\", \"Metric\": \"Recall@20\", \"Score\": \"0.37678\"}}, {\"LEADERBOARD\": {\"Task\": \"Recommendation Systems\", \"Dataset\": \"Netflix\", \"Metric\": \"Recall@50\", \"Score\": \"0.46252\"}}, {\"LEADERBOARD\": {\"Task\": \"Recommendation Systems\", \"Dataset\": \"Netflix\", \"Metric\": \"nDCG@100\", \"Score\": \"0.40861\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Recommendation Systems\", \"Dataset\": \"MovieLens 20M\", \"Metric\": \"Recall@20\", \"Score\": \"0.4325\"}}, { \"LEADERBOARD\": { \"Task\": \"Recommendation Systems\", \"Dataset\": \"MovieLens 20M\", \"Metric\": \"Recall@20\", \"Score\": \"0.42\"}}, { \"LEADERBOARD\": { \"Task\": \"Recommendation Systems\", \"Dataset\": \"MovieLens 20M\", \"Metric\": \"nDCG@20\", \"Score\": \"0.43\"}}, { \"LEADERBOARD\": { \"Task\": \"Recommendation Systems\", \"Dataset\": \"MovieLens 20M\", \"Metric\": \"Recall@20\", \"Score\": \"0.42\"}}, { \"LEADERBOARD\": { \"Task\": \"Recommendation Systems\", \"Dataset\": \"MovieLens 20M\", \"Metric\": \"Recall@20\", \"Score\": \"0.56\"}}, { \"LEADERBOARD\": { \"Task\": \"Recommendation Systems\", \"Dataset\": \"MovieLens 20M\", \"Metric\": \"nDCG@20\", \"Score\": \"0.434\"}}, { \"LEADERBOARD\": { \"Task\": \"Recommendation Systems\", \"Dataset\": \"Net"
    },
    {
        "Context": "Back-tracing Representative Points for Voting-based 3D Object Detection in Point Clouds 3D object detection in point clouds is a challenging vision task that benefits various applications for understanding the 3D visual world. Lots of recent research focuses on how to exploit end-to-end trainable Hough voting for generating object proposals. However, the current voting strategy can only receive partial votes from the surfaces of potential objects together with severe outlier votes from the cluttered backgrounds, which hampers full utilization of the information from the input point clouds. Inspired by the back-tracing strategy in the conventional Hough voting methods, in this work, we introduce anew 3D object detection method, named as Back-tracing Representative Points Network (BRNet), which generatively back-traces the representative points from the vote centers and also revisits complementary seed points around these generated points, so as to better capture the fine local structural features surrounding the potential objects from the raw point clouds. Therefore, this bottom-up and then top-down strategy in our BR-Net enforces mutual consistency between the predicted vote centers and the raw surface points and thus achieves more reliable and flexible object localization and class prediction results. Our BRNet is simple but effective, which significantly outperforms the state-of-the-art methods on two large-scale point cloud datasets, ScanNet V2 (+7.5% in terms of mAP@0.50) and SUN RGB-D (+4.7% in terms of mAP@0.50), while it is still lightweight and efficient. Code will be available at https  Table 1. 3D object detection results on the ScanNet V2 validation set(left) and the SUN RGB-D V1 validation set(right). Evaluation metric is average precision with 3D IOU thresholds as 0.25 and 0.50. *Note for fair comparison, we report the results of H3DNet on the ScanNet V2 dataset under both 1 and 4 PointNet++ backbones (BB) settings. While we only report the result of H3DNet with 4 PointNet++ backbones (BB) on the SUN RGB-D dataset, as the work [41] only reports the result under this setting. - mAP@0 . 50 Input H3DNet ( 1BB ) * [ 41 ] 3D object detection results on the ScanNet V2 validation set . mAP@0 . 25 - Table 2. 3D object detection results on the ScanNet V2 validation set. The evaluation metric is the average precision with 3D IOU threshold as 0.50. *Note that for H3DNet only the per-category results with 4 PointNet++ backbones are",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"3D Object Detection\", \"Dataset\": \"ScanNetV2\", \"Metric\": \"mAP@0.25\", \"Score\": \"66.1\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Object Detection\", \"Dataset\": \"ScanNetV2\", \"Metric\": \"mAP@0.5\", \"Score\": \"50.9\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Object Detection\", \"Dataset\": \"SUN-RGBD val\", \"Metric\": \"mAP@0.25\", \"Score\": \"61.1\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Object Detection\", \"Dataset\": \"SUN-RGBD val\", \"Metric\": \"mAP@0.5\", \"Score\": \"43.7\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"S3DIS\", \"Metric\":\"mAP@0.25\", \"Score\": \"67.2\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"S3DIS\", \"Metric\":\"mAP@0.5\", \"Score\": \"48.1\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"SUN-RGBD val\", \"Metric\":\"mAP@0.25\", \"Score\": \"60.1\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Object Detection\", \"Dataset\": \"SUN-RGBD val\", \"Metric\":\"mAP@0.5\", \"Score\": \"39.0\"}} ]"
    },
    {
        "Context": "A Multiprocessor Communication Architecture For High Speed Networks Over the years, communication speed of networks has increased from a few Kbps to several Mbps, as also the bandwidth demand, Communication Protocols, however have not improved to that extent. With the advent of Wavelength Division Multiplexing (WDM), it is now possible to \"tune\" protocols to current and future demands. The purpose of this paper is to evolve a High Speed Network architecture, which will cater to the needs of bandwidth-consuming applications, such as voice, video and high definition image transmission.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Semi-Supervised Semantic Segmentation with Pixel-Level Contrastive Learning from a Class-wise Memory Bank This work presents a novel approach for semi-supervised semantic segmentation, i.e., per-pixel classification problem assuming that only a small set of the available data is labeled. We propose a novel representation learning module based on contrastive learning. This module enforces the segmentation network to yield similar pixel-level feature representations for same-class samples across the whole dataset. To achieve this, we maintain a memory bank continuously updated with feature vectors from labeled data. These features are selected based on their quality and relevance for the contrastive learning. In an end-to-end training, the features from both labeled and unlabeled data are optimized to be similar to same-class samples from the memory bank. Our approach outperforms the current stateof-the-art for semi-supervised semantic segmentation and semi-supervised domain adaptation on well-known public benchmarks, with larger improvements on the most challenging scenarios, i.e., less available labeled data. This section describes the datasets and implementation details used in the evaluation of the presented work It is areal urban scene dataset composed of 2975 training and 500 validation samples, with 19 semantic classes It is a natural scenes dataset with 21 semantic classes The dataset has 10582 and 1449 images for training and validation respectively It is a synthetic dataset captured from a video game with realistic urban-like scenarios with 24966 images in total The original dataset provides 33 different categories but, following, we only use the 19 classes that are shared with Cityscapes The evaluation is done on the Cityscapes data, since it provides more complex scenes compared to Pascal VOC Table 1. Strong and weak data augmentation set-ups Architecture : Deeplabv3+ with ResNet - 50 backbone Architecture : Deeplabv2 with ResNet - 101 backbone Weak 1 / 4 Parameter 1 / 8 Table 1 . Strong and weak data augmentation set - ups 1 / 30 Strong FS Table 3. Performance (Mean IoU) for the Pascal VOC val set for different labeled-unlabeled ratios and, in parentheses, the differ- ence w.r.t. the corresponding fully supervised (FS) result. Architecture : Deeplabv3+ with ResNet - 50 backbone Architecture : Deeplabv2 with ResNet - 101 backbone 1 / 4 1 / 20 1 / 8 1 / 30 FS 1 / 50 Table 4. Mean IoU in Cityscapes val set. Central columns evaluate the semi-supervised domain adaptation task (GTA5 \u2192 Cityscapes). The last column evaluates a semi-supervised setting in Cityscapes (no adaptation). Different labeled-unlabeled ratios for Cityscapes are compared. All methods use ImageNet pre- trained",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Semantic Segmentation\", \"Dataset\": \"Cityscapes 100 samples labeled\", \"Metric\": \"Validation mIoU\", \"Score\": \"64.9%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Semantic Segmentation\", \"Dataset\": \"Cityscapes 100 samples labeled\", \"Metric\": \"Validation mIoU\", \"Score\": \"59.4%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Semantic Segmentation\", \"Dataset\": \"Cityscapes 25% labeled\", \"Metric\": \"Validation mIoU\", \"Score\": \"71.6%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Semantic Segmentation\", \"Dataset\": \"Cityscapes 25% labeled\", \"Metric\": \"Validation mIoU\", \"Score\": \"65.9%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Semantic Segmentation\", \"Dataset\": \"Pascal VOC 2012 5% labeled\", \"Metric\": \"Validation mIoU\", \"Score\": \"70.0%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Semantic Segmentation\", \"Dataset\": \"Pascal VOC 2012 12.5% labeled\", \"Metric\": \"Validation mIoU\", \"Score\": \"71.6%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Semantic Segmentation\", \"Dataset\": \"Cityscapes 12.5% labeled\", \"Metric\": \"Validation mIoU\", \"Score\": \"70.0%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Semantic Segmentation\", \"Dataset\": \"Cityscapes 12.5% labeled\", \"Metric\": \"Validation mIoU\", \"Score\": \"64.4%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Semantic Segmentation\", \"Dataset\": \"Pascal VOC 2012 2% labeled\", \"Metric\": \"Validation mIoU\", \"Score\": \"67.9%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Semantic Segmentation\", \"Dataset\": \"Pascal VOC 2012 1% labeled\", \"Metric\": \"Validation mIoU\", \"Score\": \"59.52%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Semantic Segmentation\", \"Dataset\": \"Cityscapes 100 samples labeled\", \"Metric\": \"Validation mIoU\", \"Score\": \"51.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Semantic Segmentation\", \"Dataset\": \"Cityscapes 50% labeled\", \"Metric\": \"Validation mIoU\", \"Score\": \"51.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Semantic Segmentation\", \"Dataset\": \"Cityscapes 25% labeled\", \"Metric\": \"Validation mIoU\", \"Score\": \"63.87%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Semantic Segmentation\", \"Dataset\": \"Cityscapes 25% labeled\", \"Metric\": \"Validation mIoU\", \"Score\": \"69.57%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Semantic Segmentation\", 'Data"
    },
    {
        "Context": "Compact Trilinear Interaction for Visual Question Answering In Visual Question Answering (VQA), answers have a great correlation with question meaning and visual contents. Thus, to selectively utilize image, question and answer information, we propose a novel trilinear interaction model which simultaneously learns high level associations between these three inputs. In addition, to overcome the interaction complexity, we introduce a multimodal tensor-based PARALIND decomposition which efficiently parameterizes trilinear interaction between the three inputs. Moreover, knowledge distillation is first time applied in Free-form Opened-ended VQA. It is not only for reducing the computational cost and required memory but also for transferring knowledge from trilinear interaction model to bilinear interaction model. The extensive experiments on benchmarking datasets TDIUC, VQA-2.0, and Visual7W show that the proposed compact trilinear interaction model achieves state-of-the-art results when using a single model on all three datasets. The source code is available at https://github.com/aioz-ai/ICCV19_ Dataset We conduct the experiments on three benchmarking VQA datasets that are Visual7W for the MC VQA, VQA-2.0 and TDIUC for the FFOE VQA We use training set to train and validation set to evaluate in all mentioned datasets when conducting ablation study In all experiments, the learning rate is set to 10 Evaluation Metrics We follow the literature in which the evaluation metrics for each VQA task are different For FFOE VQA, the single accuracy, which is a standard VQA accuracy (Acc), is applied for both TDIUC and VQA-2.0 datasets In addition, due to the imbalance in the question types of TDIUC dataset, follow, we also report four other metrics that compensate for the skewed question-type distribution For MC VQA, we follow the evaluation metric (Acc-MC) proposed by in which the performance is measured by the portion of correct answers selected by the VQA model from the candidate answer set Table 1. Overall performance of the proposal and the baselines BAN2, SAN in different evaluation metrics on TDIUC validation set. The performance is shown with and without considering Ab- surd question category. BAN2-CTI and SAN-CTI are student models trained under our proposed CTI teacher model. Acc Ari - N Har Har - N Ari Models Evaluation metrics Table 2. Performance (Acc) of the proposal and the baselines BAN2, SAN for each question-type on TDIUC validation set. BAN2-CTI and SAN-CTI are student models trained under our compact trilinear interaction teacher model. SAN SAN - CTI [ 43 ] [ 18 ] BAN2 BAN2 - CTI Table 3. Performance of the proposal and baselines BAN2, SAN in VQA-2.0 validation set and test-dev set. BAN2-CTI and SAN- CTI are student models trained under proposed teacher model. Validation Accuracy Test - dev Table 4. The performance (Acc-MC) and the number of parameters of the proposed",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Visual Question Answering\", \"Dataset\": \"VQA v2 test-dev\", \"Metric\": \"Accuracy\", \"Score\": \"67.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Question Answering\", \"Dataset\": \"TDIUC\", \"Metric\": \"Accuracy\", \"Score\": \"87\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Question Answering\", \"Dataset\": \"Visual7W\", \"Metric\": \"Percentage correct\", \"Score\": \"72.3\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Visual Question Answering\", \"Dataset\": \"VQA v2 test-dev\", \"Metric\": \"Accuracy\", \"Score\": \"68.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Question Answering\", \"Dataset\": \"VQA v2 test-std\", \"Metric\": \"overall\", \"Score\": \"70.3\"}} ]"
    },
    {
        "Context": "Semantic Segmentation for Real Point Cloud Scenes via Bilateral Augmentation and Adaptive Fusion Given the prominence of current 3D sensors, a finegrained analysis on the basic point cloud data is worthy of further investigation. Particularly, real point cloud scenes can intuitively capture complex surroundings in the real world, but due to 3D data's raw nature, it is very challenging for machine perception. In this work, we concentrate on the essential visual task, semantic segmentation, for largescale point cloud data collected in reality. On the one hand, to reduce the ambiguity in nearby points, we augment their local context by fully utilizing both geometric and semantic features in a bilateral structure. On the other hand, we comprehensively interpret the distinctness of the points from multiple resolutions and represent the feature map following an adaptive fusion method at point-level for accurate semantic segmentation. Further, we provide specific ablation studies and intuitive visualizations to validate our key modules. By comparing with state-of-the-art networks on three different benchmarks, we demonstrate the effectiveness of our network. Datasets: In this work, we are targeting the semantic segmentation of real point cloud scenes \u2022 S3DIS: Stanford Large-Scale 3D Indoor Spaces (S3DIS) dataset is collected from indoor working environments In general, there are six sub-areas in the dataset, each containing \u223c50 different rooms We adopt a 6-fold strategy for evaluation Overall, this dataset contains more than four billion points manually marked in eight semantic classes In particular, the dataset has two test sets for online evaluation: the full test set (i.e., semantic-8) has 15 scenes with over 2 billion points, while its subset (i.e., reduced-8) has four selected scenes with \u223c0.1 billion sampled points Particularly, 11 of the 22 sequences are provided with labels, while the results of the other ten sequences (over 20k scans) are for online evaluation Training Settings: We train for 100 epochs on a single GeForce RTX 2080Ti GPU with a batch size between 4 to Table 1: Semantic segmentation (6-fold cross-validation) results (%) on the S3DIS dataset [2]. (mAcc: average class accuracy, OA: overall accuracy, mIoU: mean Intersection-over-Union. \"-\" indicates unknown result.) 2018 2017 indicates unknown result . ) 2020 mIoU OA mAcc average class accuracy , Table 1 : Semantic segmentation ( 6 - fold cross - validation ) results ( mAcc : Table 2: Semantic segmentation (semantic-8) results (%) on the Semantic3D dataset [14]. natural scape scanning vegetation artefacts mIoU cars OA high man - made low buildings hard terrain Table 3: Semantic segmentation (single-scan) results (%) on the SemanticKITTI dataset [4]. sidewalk parking other - vehicle other - ground bicycle truck vegetation traffic - sign trunk pole building mIoU motorcycle bicyclist road car person motorcyclist terrain fence Table 6: Complexity analysis of different semantic segmentation networks on SemanticKITTI. (\"-\" indicates the unknown result.) networks on SemanticKITTI . ( \" - \" indicates the",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"S3DIS Area5\", \"Metric\": \"mAcc\", \"Score\": \"73.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"S3DIS Area5\", \"Metric\": \"mIoU\", \"Score\": \"65.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"S3DIS Area5\", \"Metric\": \"oAcc\", \"Score\": \"88.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Semantic3D\", \"Metric\": \"mIoU\", \"Score\": \"75.4%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Semantic3D\", \"Metric\": \"oAcc\", \"Score\": \"94.9%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"S3DIS\", \"Metric\": \"Mean IoU\", \"Score\": \"72.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"S3DIS\", \"Metric\": \"mAcc\", \"Score\": \"83.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"S3DIS\", \"Metric\": \"oAcc\", \"Score\": \"88.9\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Semantic Segmentation\", \"Dataset\": \"SemanticKITTI\", \"Metric\": \"mIoU\", \"Score\": \"59.9%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"S3DIS Area5\", \"Metric\":\"mAcc\", \"Score\": \"68.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"S3DIS Area5\", \"Metric\":\"mIoU\", \"Score\": \"61.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"S3DIS Area5\", \"Metric\": \"oAcc\", \"Score\": \"87.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"S3DIS\", \"Metric\": \"Mean IoU\", \"Score\": \"68.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"S3DIS\", \"Metric\":\"mAcc\", \"Score\": \"78.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"S3DIS\", \"Metric\": \"oAcc\", \"Score\": \"87.9\"}} ]"
    },
    {
        "Context": "Communication-optimal parallel and sequential QR and LU factorizations: theory and practice We present parallel and sequential dense QR factorization algorithms that are both optimal (up to polylogarithmic factors) in the amount of communication they perform, and just as stable as Householder QR. Our first algorithm, Tall Skinny QR (TSQR), factors m \u00d7 n matrices in a one-dimensional (1-D) block cyclic row layout, and is optimized form n. Our second algorithm, CAQR (Communication-Avoiding QR), factors general rectangular matrices distributed in a two-dimensional block cyclic layout. It invokes TSQR for each block column factorization.The new algorithms are superior in both theory and practice. We have extended known lower bounds on communication for sequential and parallel matrix multiplication to provide latency lower bounds, and show these bounds apply to the LU and QR decompositions. We not only show that our QR algorithms attain these lower bounds (up to polylogarithmic factors), but that existing LAPACK and ScaLAPACK algorithms perform asymptotically more communication. We also point out recent LU algorithms in the literature that attain at least some of these lower bounds.Both TSQR and CAQR have asymptotically lower latency cost in the parallel case, and asymptotically lower latency and bandwidth costs in the sequential case. In practice, we have implemented parallel TSQR on several machines, with speedups of up to 6.7\u00d7 on 16 processors of a Pentium III cluster, and up to 4\u00d7 on 32 processors of a BlueGene/L. We have also implemented sequential TSQR on a laptop for matrices that do not fit in DRAM, so that slow memory is disk. Our out-of-DRAM implementation was as little as 2\u00d7 slower than the predicted runtime as though DRAM were infinite.We have also modeled the performance of our parallel CAQR algorithm, yielding predicted speedups over ScaLAPACK's PDGEQRF of up to 9.7\u00d7 on an IBM Power5, up to 22.9\u00d7 on a model Petascale machine, and up to 5.3\u00d7 on a model of the Grid.  Table 1: Performance models of parallel TSQR and ScaLAPACK's parallel QR factorization PDGEQRF on an m\u00d7n matrix with P processors, along with lower bounds on the number of flops, words, and messages. We assume m/P \u2265 n. Everything (messages, words, and flops) is counted along the critical path. The boldface part of the table highlights TSQR's improvement over ScaLAPACK. \" P Lower bound PDGEQRF TSQR \u0398 n Table 2: Performance models of parallel CAQR and ScaLAPACK's parallel QR factorization PDGEQRF on a m \u00d7 n matrix with P processors, along with lower bounds on the number of flops, words, and messages. The matrix is stored in a 2-D P r \u00d7 P c block cyclic layout with square b \u00d7 b blocks. We choose b, P r , and P c optimally and independently for each algorithm. We assume m \u2265 n. Everything (messages, words, and flops) is counted along the",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "FoveaBox: Beyound Anchor-Based Object Detection We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. In FoveaBox, an instance is assigned to adjacent feature levels to make the model more accurate.We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis. Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO and Pascal VOC object detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance. We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection. The code has been made publicly available at https://github.com/taokong/FoveaBox. Index Terms-Object detection, anchor free, foveabox. For our main results, we report COCO AP on the test-dev split, which has no public labels and requires use of the evaluation server For Pascal VOC, all models are trained on trainval2007 and trainval2012, and evaluated on test2007 subset, following common practice, we conducted additional experiments on Pascal VOC object detection dataset This dataset covers 20 object categories, and the performance is measured by mean average precision (mAP) at IoU = 0.5 All variants are trained on VOC2007 trainval and VOC2012 trainval, and tested on VOC2007 test dataset",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"AP50\", \"Score\": \"58.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"AP75\", \"Score\": \"41.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APL\", \"Score\": \"51.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APM\", \"Score\": \"43.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APS\", \"Score\": \"22.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"box AP\", \"Score\": \"38.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"AP50\", \"Score\": \"57.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"AP75\", \"Score\": \"40.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"box AP\", \"Score\": \"38.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"AP75\", \"Score\": \"40.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APL\", \"Score\": \"52.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APM\", \"Score\": \"42.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APS\", \"Score\": \"19.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"box AP\", \"Score\": \"38\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"AP50\", \"Score\": \"55.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"AP75\", \"Score\": \"37.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APL\", \"Score\": \"50.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APM\", \"Score\": \"39.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"APS\", \"Score\": \"18.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO minival\", \"Metric\": \"box AP\", \"Score\": \"36.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP50\", \"Score\": \"63.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP75\", \"Score\": \"47.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APL\", \"Score\": \"55.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APM\", \"Score\": \"46.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APS\", \"Score\": \"26.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"box AP\", \"Score\": \"43.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"box AP\", \"Score\": \"42.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP50\", \"Score\": \"61.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP75\", \"Score\": \"45.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APM\", \"Score\": \"46.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APS\", \"Score\": \"24.9\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"PASCAL VOC 2007\", \"Metric\": \"MAP\", \"Score\": \"82.6%\"}} ]"
    },
    {
        "Context": "Learning multiple visual domains with residual adapters There is a growing interest in learning data representations that work well for many different types of problems and data. In this paper, we look in particular at the task of learning a single visual representation that can be successfully utilized in the analysis of very different types of images, from dog breeds to stop signs and digits. Inspired by recent work on learning networks that predict the parameters of another, we develop a tunable deep network architecture that, by means of adapter residual modules, can be steered on the fly to diverse visual domains. Our method achieves a high degree of parameter sharing while maintaining or even improving the accuracy of domain-specific representations. We also introduce the Visual Decathlon Challenge, a benchmark that evaluates the ability of representations to capture simultaneously ten very different visual domains and measures their ability to perform well uniformly. In this section we evaluate our method quantitatively against several baselines (section 5.1), investigate the ability of the proposed techniques to learn models for ten very diverse visual domains Table 1: Multiple-domain networks. The figure reports the (top-1) classification accuracy (%) of different models on the decathlon tasks and final decathlon score (S). ImageNet is used to prime the network in every case, except for the networks trained from scratch. The model size is the number of parameters w.r.t. the baseline ResNet. The fully-finetuned model, written blue, is used as a baseline to compute the decathlon score. 2515 2118 2503 2643 544 1363 1826 Airc . Flwr OGlt 26k DPed GTSR 30k SVHN 40k 50k C100 70k 9k 7k DTD mean 4k 2k UCF #par . ImNet Table 2: Pairwise forgetting. Each pair of numbers report the top-1 accuracy (%) on the old task (ImageNet) and a new target task after the network is fully finetuned on the latter. We also show the performance of LwF when it is finetuned on the new task with a high and low learning",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Continual Learning\", \"Dataset\": \"visual domain decathlon (10 tasks)\", \"Metric\": \"decathlon discipline (Score)\", \"Score\": \"3131\"}}, {\"LEADERBOARD\": {\"Task\": \"Continual Learning\", \"Dataset\": \"visual domain decathlon (10 tasks)\", \"Metric\": \"decathlon discipline (Score)\", \"Score\": \"2643\"}}, {\"LEADERBOARD\": {\"Task\": \"Continual Learning\", \"Dataset\": \"visual domain decathlon (10 tasks)\", \"Metric\": \"decathlon discipline (Score)\", \"Score\": \"2621\"}}, {\"LEADERBOARD\": {\"Task\": \"Continual Learning\", \"Dataset\": \"visual domain decathlon (10 tasks)\", \"Metric\": \"decathlon discipline (Score)\", \"Score\": \"2503\"}}, {\"LEADERBOARD\": {\"Task\": \"Continual Learning\", \"Dataset\": \"visual domain decathlon (10 tasks)\", \"Metric\": \"decathlon discipline (Score)\", \"Score\": \"2118\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"STL-10\", \"Metric\": \"Percentage correct\", \"Score\": \"94.41\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"STL-10\", \"Metric\": \"Percentage correct\", \"Score\": \"88.80\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"STL-10\", \"Metric\": \"Percentage correct\", \"Score\": \"87.36\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"STL-10\", \"Metric\": \"Percentage correct\", \"Score\": \"74.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"STL-10\", \"Metric\": \"Percentage correct\", \"Score\": \"91.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"Stanford Cars\", \"Metric\": \"Accuracy\", \"Score\": \"96.32%\"}}, { \"LEADERBOARD\": { \"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"Stanford Cars\", \"Metric\": \"Accuracy\", \"Score\": \"32.76%\"}}, {"
    },
    {
        "Context": "Optimal and Cut-free Tableaux for Propositional Dynamic Logic with Converse We give an optimal (exptime), sound and complete tableaubased algorithm for deciding satisfiability for propositional dynamic logic with converse (CPDL) which does not require the use of analytic cut. Our main contribution is a sound method to combine our previous optimal method for tracking least fix-points in PDL with our previous optimal method for handling converse in the description logic ALCI. The extension is non-trivial as the two methods cannot be combined naively. We give sufficient details to enable an implementation by others. Our OCaml implementation seems to be the first theorem prover for CPDL.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Syntax-Enhanced Neural Machine Translation with Syntax-Aware Word Representations Syntax has been demonstrated highly effective in neural machine translation (NMT). Previous NMT models integrate syntax by representing 1-best tree outputs from a welltrained parsing system, e.g., the representative Tree-RNN and Tree-Linearization methods, which may suffer from error propagation. In this work, we propose a novel method to integrate source-side syntax implicitly for NMT. The basic idea is to use the intermediate hidden representations of a well-trained end-to-end dependency parser, which are referred to as syntax-aware word representations (SAWRs). Then, we simply concatenate such SAWRs with ordinary word embeddings to enhance basic NMT models. The method can be straightforwardly integrated into the widelyused sequence-to-sequence (Seq2Seq) NMT models. We start with a representative RNNbased Seq2Seq baseline system, and test the effectiveness of our proposed method on two benchmark datasets of the Chinese-English and English-Vietnamese translation tasks, respectively. Experimental results show that the proposed approach is able to bring significant BLEU score improvements on the two datasets compared with the baseline, 1.74 points for Chinese-English translation and 0.80 point for English-Vietnamese translation, respectively. In addition, the approach also outperforms the explicit Tree-RNN and Tree-Linearization methods.  Previous Work 38 . 83 / +1 . 74 34 . 30 / +2 . 59 38 . 41 / +1 . 32 MT05 MT06 Average / \u2206 MT03 MT04 Table 3: The influence of fine-tuning parser parameters in the SAWR system. MT05 Average MT06 MT03 MT04 Table 4: Ensemble performances, where the Hybrid model denotes SAWR + Tree-RNN + Tree-Linearization. 42 . 27 / +1 . 03 42 . 60 / +1 . 36 42 . 18 / +0 . 94 MT05 MT06 Average / \u2206 MT03 MT04 Table 5: Final results based on the transformer. Only the SAWR results are significantly better (p < 0.05). 41 . 42 / +0 . 68 41 . 78 / +1 . 04 MT05 MT06 Average / \u2206 MT03 MT04",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"IWSLT2015 English-Vietnamese\", \"Metric\": \"BLEU\", \"Score\": \"29.09\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Dependency Parsing\", \"Dataset\": \"Penn Treebank\", \"Metric\": \"LAS\", \"Score\": \"90.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Dependency Parsing\", \"Dataset\": \"Penn Treebank\", \"Metric\": \"UAS\", \"Score\": \"94.9\"}} ]"
    },
    {
        "Context": "From Causality Semantics to Duration Timed Models The interleaving semantics is not compatible with both action refinement and durational actions. Since many true concurrency semantics are congruent w.r.t. action refinement, notably the causality and the maximality ones [Cos93,Gla90], this has challenged us to study the dense time behavior -where the actions are of arbitrary fixed durationwithin the causality semantics of Da Costa [Cos93].We extend the causal transition systems with the clocks and the timed constraints, and thus we obtain an over class of timed automata where the actions need not to be atomic. We define areal time extension of the formal description technique CSP, called duration-CSP, by attributing the duration to actions. We give the operational timed causal semantics of duration-CSP as well as its denotational semantics over the class of timed causal transition systems. Afterwards, we prove that the two semantics are equivalent. Finally we extend the duration-CSP language with a refinement operator \u03c1 -that allows to replace an action with a process -and prove that it preserves the timed causal bisimulation.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "DFNets: Spectral CNNs for Graphs with Feedback-Looped Filters We propose a novel spectral convolutional neural network (CNN) model on graph structured data, namely Distributed Feedback-Looped Networks (DFNets). This model is incorporated with a robust class of spectral graph filters, called feedbacklooped filters, to provide better localization on vertices, while still attaining fast convergence and linear memory requirements. Theoretically, feedback-looped filters can guarantee convergence w.r.t. a specified error bound, and be applied universally to any graph without knowing its structure. Furthermore, the propagation rule of this model can diversify features from the preceding layers to produce strong gradient flows. We have evaluated our model using two benchmark tasks: semi-supervised document classification on citation networks and semi-supervised entity classification on a knowledge graph. The experimental results show that our model considerably outperforms the state-of-the-art methods in both benchmark tasks overall datasets. We evaluate our models on two benchmark tasks: (1) semi-supervised document classification in citation networks, and (2) semi-supervised entity classification in a knowledge graph Datasets We use three citation network datasets Cora, Citeseer, and Pubmed for semi-supervised document classification, and one dataset NELL for semi-supervised entity classification We evaluate our feedback-looped filters using three different spectral CNN models: (i) DFNet: a densely connected spectral CNN with feedback-looped filters, (ii) DFNet-ATT: a self-attention based densely connected spectral CNN with feedback-looped filters, and (iii) DF-ATT: a self-attention based spectral CNN model with feedback-looped filters.: Hyperparameter settings for citation network datasets We use the same data splitting for each dataset as in Yang et al. summarizes the hyperparameter settings for citation network datasets The same hyperparameters are applied to the NELL dataset except for L2 regularization (i.e., 9e-2 for DFNet and DFnet-ATT, and 9e-4 for DF-ATT) The parameters p = 5, q = 3 Table 2: Dataset statistics. %Labeled Nodes contains dataset statistics [ 33 ] . Table 3: Hyperparameter settings for citation network datasets. \u03bb cut Dropout Table 4. We can see that our feedback-looped filters perform best, no matter whether or not the dense architecture is used. - - Table 4: Accuracy (%) averaged over 10 runs (* was obtained using a different data splitting in - - Table 5: Accuracy (%) averaged over 10 runs. Cora dataset . Citeseer It shows that feedback - looped filters improve The experimental results are presented in table 5 . Cora Pubmed Table 6: Accuracy (%) averaged over 10 runs on the Cora dataset. Table 6 : Accuracy ( % ) averaged over 10 runs on the Cora dataset . 2 . LNet GCN Training Split DFNet GAT AdaLNet Chebyshev Table 7: Accuracy (%) averaged over 10 runs on the Citeseer dataset. Table 7 : Accuracy",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"Citeseer\", \"Metric\": \"Accuracy\", \"Score\": \"74.7%\"}}, {\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"Pubmed\", \"Metric\": \"Accuracy\", \"Score\": \"85.2%\"}}, {\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"Cora\", \"Metric\": \"Accuracy\", \"Score\": \"86%\"}}, {\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"NELL\", \"Metric\": \"Accuracy\", \"Score\": \"68.8%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Citeseer\", \"Metric\": \"Accuracy\", \"Score\": \"73.5%\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Pubmed\", \"Metric\": \"Accuracy\", \"Score\": \"82.6%\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Cora\", \"Metric\": \"Accuracy\", \"Score\": \"82.3%\"}} ]"
    },
    {
        "Context": "Modeling of a Reconfigurable OFDM IP Block Family For an RF System Simulator The idea of design domain specific Mother Model of IP block family as abase of modeling of system integration is presented here. A common reconfigurable Mother Model for ten different standardized digital OFDM transmitters has been developed. By means of a set of parameters, the mother model can be reconfigured to any of the ten selected standards. So far the applicability of the proposed reconfiguration and analog-digital co-modeling methods have been proved by modeling the function of the digital parts of three, 802.11a, ADSL and DRM, transmitters in an RF system simulator. The model is intended to be used as signal source template in RF system simulations. The concept is not restricted to signal sources, it can be applied to any IP block development.The idea of the Mother Model will be applied in other design domains to prove that in certain application areas, OFDM transceivers in this case, the design process can progress simultaneously in different design domainsmixed signal, system and RTL-architectural -without the need of high-level synthesis. Only the Mother Models of three design domains are needed to be formally proved to function as specified.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "A Secure Wireless Routing Protocol Using Enhanced Chain Signatures We propose a routing protocol for wireless networks. Wireless routing protocols allow hosts within a network to have some knowledge of the topology in order to know when to forward a packet (via broadcast) and when to drop it. Since a routing protocol forms the backbone of a network, it is a lucrative target for many attacks, all of which attempt to disrupt network traffic by corrupting routing tables of neighboring routers using false updates. Secure routing protocols designed for wired networks (such as S-BGP) are not scalable in an ad-hoc wireless environment because of two main drawbacks: (1) the need to maintain knowledge about all immediate neighbors (which requires a discovery protocol), and (2) the need to transmit the same update several times, one for each neighbor. Although information about neighbors is readily available in a fairly static and wired network, such information is often not updated or available in an ad-hoc wireless network with mobile devices. Our protocol is a variant of S-BGP called SS-BGP and allows a single broadcast for routing updates without having the need to be aware of every neighboring router. The protocol is based on a novel authentication primitive called Enhanced Chain Signatures (ECS).",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Cutting a Convex Polyhedron Out of a Sphere \u22c6 Given a convex polyhedron P of n vertices inside a sphere Q, we give an O(n 3 )-time algorithm that cuts P out of Q by using guillotine cuts and has cutting cost O(log 2 n) times the optimal.The problem of cutting a convex polygon P out of apiece of planar material Q (P is already drawn on Q) with minimum total cutting length is a well studied problem in computational geometry. The problem was first introduced by Overmars and Welzl in 1985 [13] but has been extensively studied in the last decades [2-5, 8, 9, 11, 13-15]  with several variations, such as P and Q are convex or non-convex polygons, Q is a circle, and the cuts are line cuts or ray cuts. This type of cutting problems have many industrial applications such as in metal sheet cutting, paper cutting, furniture manufacturing, ceramic industries, fabrication, ornaments, and leather industries. Some of their variations also fall under stock cutting problems [4].If Q is another convex polygon with m edges, this problem with line cuts has been approached in various ways [3-6, 9, 10, 13, 14]. If the cuts are allowed only along the edges of P , Overmars and Welzl [13] proposed an O(n 3 + m)-time algorithm for this problem with optimal cutting length, where n is the number of edges of P . The problem is more difficult if the cuts are more general, i.e., they are not restricted to touch only the edges of P . In that case, Bhadury and Chandrasekaran showed that the problem has optimal solutions that lie in the algebraic extension of the input data field [4], and due to this algebraic nature of this problem, an approximation scheme is the best that one can achieve [4]. They also gave an approximation scheme with pseudo-polynomial running time [4].After the indication of Bhadury and Chandrasekaran [4] to the hardness of the problem, many people have given polynomial time approximation algorithms. Dumitrescu proposed an O(log n)-approximation algorithm with O(mn+n log n) running time [9,10]. Then, Daescu and Luo [6] gave the first constant factor approximation algorithm with ratio 2.5 + ||Q||/||P ||, where ||P || and ||Q|| are the \u22c6 An earlier version appeared in Proc. WALCOM 2010, LNCS, Springer, 2010",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Buffer Management Algorithm Design and Implementation Based on Network Processors To solve the parameter sensitive issue of the traditional RED (random early detection) algorithm, an adaptive buffer management algorithm called PAFD (packet adaptive fair dropping) is proposed. This algorithm supports DiffServ (differentiated services) model of QoS (quality of service). In this algorithm, both of fairness and throughput are considered. The smooth buffer occupancy rate function is adopted to adjust the parameters. By implementing buffer management and packet scheduling on Intel IXP2400, the viability of QoS mechanisms on NPs (network processors) is verified. The simulation shows that the PAFD smoothes the flow curve, and achieves better balance between fairness and network throughput. It also demonstrates that this algorithm meets the requirements of fast data packet processing, and the hardware resource utilization of NPs is higher.Florida. He has been involved in theoretical works on control theory and on parallel simulation algorithms development for real-time applications in the past several years. In the same periods, he has also participated in several industry supported projects on real-time data processing and microprocessor-based control system designs. Currently, his research interests are in the security related issues and performance improvement of computer networks.Deng Pan received his Ph.D. and M.S.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Toward Scalable Neural Dialogue State Tracking Model The latency in the current neural based dialogue state tracking models prohibits them from being used efficiently for deployment in production systems, albeit their highly accurate performance. This paper proposes anew scalable and accurate neural dialogue state tracking model, based on the recently proposed Global-Local Self-Attention encoder (GLAD) model by Zhong et al. (2018) which uses global modules to share parameters between estimators for different types (called slots) of dialogue states, and uses local modules to learn slot-specific features. By using only one recurrent networks with global conditioning, compared to (1 + # slots) recurrent networks with global and local conditioning used in the GLAD model, our proposed model reduces the latency in training and inference times by 35% on average, while preserving performance of belief state tracking, by 97.38% on turn request and 88.51% on joint goal and accuracy. Evaluation on Multi-domain dataset (Multi-WoZ) also demonstrates that our model outperforms GLAD on turn inform and joint goal accuracy. In this section, we evaluate the proposed encoder for the task of dialogue state tracking om single and multi-domain dialogue state tracking Wizard of oz (WoZ) restaurant reservation dataset is chosen for single-domain, and the performance is compared with the recent neural belief tracking models Moreover, we also evaluate on recen;t proposed multi-domain dataset, Multi-WoZ, which consists of seven domains, i.e The evaluation metric is based on joint goal and turn-level request and joint goal tracking accuracy Multi-Domain: shows the evauation on Multi-Woz dataset which consists of 10k dialogues Table 2: Time complexity for each batch of turn, and train and test epoch on WoZ dataset. Each batch contains 50 turns. All numbers are in second. 76 Test ( sec . ) Turn Train ( sec . ) Table 3: Performance on Multi-Domain dataset, Multi-WoZ (Budzianowski et al., 2018). 76 Test ( sec . ) Turn Train ( sec . )",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Dialogue State Tracking\", \"Dataset\": \"Wizard-of-Oz\", \"Metric\": \"Joint\", \"Score\": \"88.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Dialogue State Tracking\", \"Dataset\": \"Wizard-of-Oz\", \"Metric\": \"Request\", \"Score\": \"97.4\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Dialogue State Tracking\", \"Dataset\": \"Multi-Domain\", \"Metric\": \"MOTA\", \"Score\": \"83.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Dialogue State Tracking\", \"Dataset\": \"Multi-Domain\", \"Metric\": \"MOTP\", \"Score\": \"83.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Dialogue State Tracking\", \"Dataset\": \"Multi-Domain\", \"Metric\": \"MOTP\", \"Score\": \"83.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Dialogue State Tracking\", \"Dataset\": \"Multi-Domain\", \"Metric\": \"MOTP\", \"Score\": \"80.0\"}} ]"
    },
    {
        "Context": "Reading documents on mobile devices is challenging. Not only are screens small and difficult to read, but also navigating an environment using limited visual attention can be difficult and potentially dangerous. Reading content aloud using text-tospeech (TTS) processing can mitigate these problems, but only for content that does not include rich visual information. In this paper, we introduce anew technique, SeeReader, that combines TTS with automatic content recognition and document presentation control that allows users to listen to documents while also being notified of important visual content. Together, these services allow users to read rich documents on mobile devices while maintaining awareness of their visual environment. We ran a within subjects, dual-task study as a preliminary evaluation of the core features of the SeeReader interface",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "SkeleMotion: A New Representation of Skeleton Joint Sequences Based on Motion Information for 3D Action Recognition Due to the availability of large-scale skeleton datasets, 3D human action recognition has recently called the attention of computer vision community. Many works have focused on encoding skeleton data as skeleton image representations based on spatial structure of the skeleton joints, in which the temporal dynamics of the sequence is encoded as variations in columns and the spatial structure of each frame is represented as rows of a matrix. To further improve such representations, we introduce a novel skeleton image representation to be used as input of Convolutional Neural Networks (CNNs), named SkeleMotion. The proposed approach encodes the temporal dynamics by explicitly computing the magnitude and orientation values of the skeleton joints. Different temporal scales are employed to compute motion values to aggregate more temporal dynamics to the representation making it able to capture longrange joint interactions involved in actions as well as filtering noisy motion values. Experimental results demonstrate the effectiveness of the proposed representation on 3D action recognition outperforming the state-of-the-art on NTU RGB+D 120 dataset. To isolate only the contribution brought by SkeleMotion to the action recognition problem, all other representations were tested on the same datasets with the same split of training and testing data and using the same CNN The NTU RGB+D 60 is publicly available 3D action recognition dataset The dataset provides four different data information: (i) RGB frames; (ii) depth maps; (iii) 395 infrared sequences; and (iv) skeleton joints There are two different evaluation protocols: cross-subject, which split the 40 subjects into training and testing; and cross-view, which uses samples from one camera for testing and the other two for training The performance is evaluated by computing the average recognition across all classes The NTU RGB+D 120 is a large-scale 3D action recognition dataset captured under various environmental conditions As in NTU RGB+D 60, the dataset provides RGB frames, depth maps, infrared sequences and skeleton joints There are two different evaluation protocols: Table 1. Action recognition accuracy (%) results on a subset of NTU RGB+D 60 [25] dataset by applying temporal scale aggre- gation (TSA) on our SkeleMotion representation. Three Temporal Scales Acc . ( % ) Magnitude Orientation Table 2. Action recognition accuracy (%) results on NTU RGB+D 60 [25] dataset. Results for the baselines were obtained running each method implementation. Cross - view Acc . ( % ) subject Table 3. Comparison between late and early fusion techniques on NTU RGB+D 60 [25] dataset. Acc . ( % ) Cross - subject Cross - view Table 4. Action recognition accuracy (%) results on NTU RGB+D 120 [15] dataset. Results for literature methods were obtained from [15]. Cross - setup Acc . ( % ) Cross - subject",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D 120\", \"Metric\": \"Accuracy (Cross-Setup)\", \"Score\": \"66.9%\"}}, {\"LEADERBOARD\": {\"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D 120\", \"Metric\": \"Accuracy (Cross-Subject)\", \"Score\": \"67.7%\"}}, {\"LEADERBOARD\": {\"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D 120\", \"Metric\": \"Accuracy (Cross-Setup)\", \"Score\": \"63.0%\"}}, {\"LEADERBOARD\": {\"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D 120\", \"Metric\": \"Accuracy (Cross-Subject)\", \"Score\": \"62.9%\"}}, {\"LEADERBOARD\": {\"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"Accuracy (CS)\", \"Score\": \"76.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"Accuracy (CV)\", \"Score\": \"84.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"NTU RGB+D 120\", \"Metric\": \"Accuracy (Cross-Setup)\", \"Score\": \"66.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"NTU RGB+D 120\", \"Metric\": \"Accuracy (Cross-Subject)\", \"Score\": \"67.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"Accuracy (CS)\", \"Score\": \"76.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"Accuracy (CV)\", \"Score\": \"84.7\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D 120\", \"Metric\": \"Accuracy (Cross-Setup)\", \"Score\": \"88.4%\"}}, { \"LEADERBOARD\": { \"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D 120\", \"Metric\": \"Accuracy (Cross-Subject)\", \"Score\": \"86.9%\"}}, { \"LEADERBOARD\": { \"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"Accuracy (CS)\", \"Score\": \"90.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"Accuracy (CV)\", \"Score\": \"96.3\"}} ]"
    },
    {
        "Context": "Meta-DETR: Few-Shot Object Detection via Unified Image-Level Meta-Learning Few-shot object detection aims at detecting novel objects with only a few annotated examples. Prior works have proved meta-learning a promising solution, and most of them essentially address detection by meta-learning over regions for their classification and location fine-tuning. However, these methods substantially rely on initially welllocated region proposals, which are usually hard to obtain under the few-shot settings. This paper presents a novel meta-detector framework, namely Meta-DETR, which eliminates region-wise prediction and instead meta-learns object localization and classification at image level in a unified and complementary manner. Specifically, it first encodes both support and query images into category-specific features and then feeds them into a category-agnostic decoder to directly generate predictions for specific categories. To facilitate meta-learning with deep networks, we design a simple but effective Semantic Alignment Mechanism (SAM), which aligns high-level and low-level feature semantics to improve the generalization of meta-learned representations. Experiments over multiple few-shot object detection benchmarks show that Meta-DETR outperforms state-of-the-art methods by large margins. Concretely, two widely used few-shot object detection benchmarks are evaluated in our experiments We use trainval 07+12 for training and perform evaluations on test 07 Mean average precision (mAP) at IoU threshold 0.5 is used as the evaluation metric Results are averaged over 10 randomly sampled support datasets MS COCO is a more challenging object detection dataset, which contains 80 categories including those 20 categories in Pascal VOC We adopt the 20 shared categories as novel categories, and adopt the remaining 60 categories in MS COCO dataset as base categories We use train 2017 for training, and perform evaluations on val 2017 Standard evaluation metrics for MS COCO are adopted Results are averaged over 5 randomly sampled support datasets For Pascal VOC, mean average precision (mAP) at IoU threshold 0.5 is used as the evaluation metric Evaluation with Multiple Repeated Runs To address this issue, following and, our results, as reported Table 1. Few-shot detection performance (mAP@0.5) on Pascal VOC test 07 set for novel categories. Results are averaged over multiple repeated runs with different randomly sampled support datasets. \u2020 indicates results are re-evaluated using official codes for multiple runs since original results are evaluated with a single run. since original results are evaluated with a single run . 1 2 3 Novel 5 Split 3 \u2020 indicates results are re - evaluated using official codes for multiple runs Base 10 Table 2. Few-shot detection performance (mAP@0.5) for base and novel categories on category split 1 of Pascal VOC. Results are averaged over multiple runs. \u2020 indicates re-evaluated results. since original results are evaluated with a single run . Novel \u2020 indicates results are re - evaluated using official codes for multiple runs Table 1 . Few - shot detection performance ( mAP@0 . 5 ) on Pascal VOC test 07 set",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Few-Shot Object Detection\", \"Dataset\": \"MS-COCO (30-shot)\", \"Metric\": \"AP\", \"Score\": \"22.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Few-Shot Object Detection\", \"Dataset\": \"MS-COCO (30-shot)\", \"Metric\": \"AP\", \"Score\": \"21.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Few-Shot Object Detection\", \"Dataset\": \"MS-COCO (10-shot)\", \"Metric\": \"AP\", \"Score\": \"17.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Few-Shot Object Detection\", \"Dataset\": \"MS-COCO (10-shot)\", \"Metric\": \"AP\", \"Score\": \"16.7\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Few-Shot Object Detection\", \"Dataset\": \"MS-COCO (30-shot)\", \"Metric\": \"AP\", \"Score\": \"9.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Few-Shot Object Detection\", \"Dataset\": \"MS-COCO (10-shot)\", \"Metric\": \"AP\", \"Score\": \"5.6\"}} ]"
    },
    {
        "Context": "Forecasting directional movements of stock prices for intraday trading using LSTM and random forests We employ both random forests and LSTM networks (more precisely CuDNNLSTM) as training methodologies to analyze their effectiveness in forecasting out-of-sample directional movements of constituent stocks of the S&P 500 from January 1993 till December 2018 for intraday trading. We introduce a multi-feature setting consisting not only of the returns with respect to the closing prices, but also with respect to the opening prices and intraday returns. As trading strategy, we use Krauss et al. (2017) and Fischer & Krauss (2018) as benchmark and, on each trading day, buy the 10 stocks with the highest probability and sell short the 10 stocks with the lowest probability to outperform the market in terms of intraday returns -all with equal monetary weight. Our empirical results show that the multi-feature setting provides a daily return, prior to transaction costs, of 0.64% using LSTM networks, and 0.54% using random forests. Hence we outperform the single-feature setting in Fischer & Krauss (2018) and Krauss et al. (2017) consisting only of the daily returns with respect to the closing prices, having corresponding daily returns of 0.41% and of 0.39% with respect to LSTM and random forests, respectively. 1 We follow the procedure of and divide the dataset consisting of 29 years starting from January 1990 till December 2018, using a 4-year window and 1-year stride, where each study period is divided into a training period of approximately 756 days (\u2248 3 years) and a trading period of approximately 252 days (\u2248 1 year) Table 2: Average performance metrics of the simulations before transaction cost 3 - Feature LSTM RF IntraDay 1 - Feature NextDay Index SP500 Table 3: Average performance metrics of the simulations after transaction cost 3 - Feature LSTM RF IntraDay 1 - Feature Table 2 : Average performance metrics of the simulations before transaction cost NextDay Index SP500",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Stock Market Prediction\", \"Dataset\": \"S&P 500\", \"Metric\": \"Average daily returns\", \"Score\": \"0.635%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Inspec\", \"Dataset\": \"NYU Corpus\", \"Metric\": \"AUC\", \"Score\": \"0.942\"}}, { \"LEADERBOARD\": { \"Task\": \"Inspec\", \"Dataset\": \"STL-10\", \"Metric\": \"Average Accuracy\", \"Score\": \"0.863\"}} ]"
    },
    {
        "Context": "YOLACT++ Better Real-time Instance Segmentation We present a simple, fully-convolutional model for real-time (> 30 fps) instance segmentation that achieves competitive results on MS COCO evaluated on a single Titan Xp, which is significantly faster than any previous state-of-the-art approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn't depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. We also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty. Finally, by incorporating deformable convolutions into the backbone network, optimizing the prediction head with better anchor scales and aspect ratios, and adding a novel fast mask re-scoring branch, our YOLACT++ model can achieve 34.1 mAP on MS COCO at 33.5 fps, which is fairly close to the state-of-the-art approaches while still running at real-time.  S AP 50 AP L FPS Time M 75 AP YOLACT FPS Time AP AP mask AP bbox FPS Time Table 1. In addition to our base 550 \u00d7 550 image size model, we train 400 \u00d7 400 (YOLACT-400) and 700 \u00d7 700 (YOLACT-700) models, adjusting the anchor scales accordingly (s x = s 550 /550 * x). Lowering the image size results in a large decrease in performance, demonstrating that instance segmenta- tion naturally demands larger images. Then, raising the image size decreases speed significantly but also increases performance, as expected. In addition to our base backbone of ResNet-101 With the proposed enhancements , YOLACT++ obtains a huge high speed . In particular , our YOLACT++ - ResNet - 50 model runs mAP r FPS Time Method mAP S AP 50 \u00b7 FPS Time L M 75 AP FPS Time AP Table 7. With these two upgrades for object detection,",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Instance Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP50\", \"Score\": \"53.8%\"}}, {\"LEADERBOARD\": {\"Task\": \"Instance Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP75\", \"Score\": \"36.9%\"}}, {\"LEADERBOARD\": {\"Task\": \"Instance Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APL\", \"Score\": \"55.1%\"}}, {\"LEADERBOARD\": {\"Task\": \"Instance Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APM\", \"Score\": \"36.8%\"}}, {\"LEADERBOARD\": {\"Task\": \"Instance Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APS\", \"Score\": \"11.9%\"}}, {\"LEADERBOARD\": {\"Task\": \"Instance Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"mask AP\", \"Score\": \"34.6%\"}}, {\"LEADERBOARD\": {\"Task\": \"Real-time Instance Segmentation\", \"Dataset\": \"MSCOCO\", \"Metric\": \"AP50\", \"Score\": \"53.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Real-time Instance Segmentation\", \"Dataset\": \"MSCOCO\", \"Metric\": \"AP75\", \"Score\": \"36.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Real-time Instance Segmentation\", \"Dataset\": \"MSCOCO\", \"Metric\": \"APL\", \"Score\": \"55.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Real-time Instance Segmentation\", \"Dataset\": \"MSCOCO\", \"Metric\": \"APM\", \"Score\": \"36.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Real-time Instance Segmentation\", \"Dataset\": \"MSCOCO\", \"Metric\": \"APS\", \"Score\": \"11.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Real-time Instance Segmentation\", \"Dataset\": \"MSCOCO\", \"Metric\": \"Frame (fps)\", \"Score\": \"27.3 (Titan Xp)\"}}, {\"LEADERBOARD\": {\"Task\": \"Real-time Instance Segmentation\", \"Dataset\": \"MSCOCO\", \"Metric\": \"mask AP\", \"Score\": \"34.6\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Instance Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP50\", \"Score\": \"62.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Instance Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP75\", \"Score\": \"43.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Instance Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APL\", \"Score\": \"54.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Instance Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APM\", \"Score\": \"43.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Instance Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APS\", \"Score\": \"25.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Instance Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\":\"mask AP\", \"Score\": \"40.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Real-time Instance Segmentation\", \"Dataset\": \"MSCOCO\", \"Metric\": \"AP50\", \"Score\": \"61.4%\"}}, { "
    },
    {
        "Context": "DEEP ENSEMBLES FOR LOW-DATA TRANSFER LEARNING In the low-data regime, it is difficult to train good supervised models from scratch. Instead practitioners turn to pre-trained models, leveraging transfer learning. Ensembling is an empirically and theoretically appealing way to construct powerful predictive models, but the predominant approach of training multiple deep networks with different random initialisations collides with the need for transfer via pre-trained weights. In this work, we study different ways of creating ensembles from pre-trained models. We show that the nature of pre-training itself is a performant source of diversity, and propose a practical algorithm that efficiently identifies a subset of pre-trained models for any downstream dataset. The approach is simple: Use nearest-neighbour accuracy to rank pre-trained models, fine-tune the best ones with a small hyperparameter sweep, and greedily construct an ensemble to minimise validation cross-entropy. When evaluated together with strong baselines on 19 different downstream tasks (the Visual Task Adaptation Benchmark), this achieves state-of-the-art performance at a much lower inference budget, even when selecting from over 2,000 pre-trained models. We also assess our ensembles on ImageNet variants and show improved robustness to distribution shift. We evaluate our models on the Visual Task Adaptation Benchmark : 19 diverse downstream classification tasks, split into 'natural', 'specialised' and 'structured' categories For the resultant ensemble, we retrain constituent models on the full 1000 data points, and evaluate it on the test data While ImageNet does not match our low-data regime of interest, previous work and additional datasets allow us to conveniently measure robustness and uncertainty metrics Table 1: Test accuracy of our best ensembles against reproduced baselines from Kolesnikov et al. (2019) [ * ]. For each dataset, we take the median of three independent runs. Rows show the average over datasets. Bootstrapped confidence intervals at the 95% level. The source of diversity for ensem- bles is shown: U = upstream (during pre-training) and C = combined (pre-training and fine-tuning). Specialised VTAB 1K Structured Natural Table 2: Upstream diversity gives better ensembles. Test accuracy of different ensembles. For each dataset, we take the median of three independent runs. Rows show the average over datasets. Bootstrapped confidence intervals at the 95% level. The source of diversity is noted as: D = down- stream (during fine-tuning), U = upstream (during pre-training) and C = combined (both). Specialised VTAB 1K Structured Natural Table 3: Ablations. Test accuracy of different ensembles. For each dataset, we take the median of three independent",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"VTAB-1k\", \"Metric\": \"Top-1 Accuracy\", \"Score\": \"77.6\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"STL-10\", \"Metric\": \"Percentage correct\", \"Score\": \"71.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"STL-10\", \"Metric\": \"Percentage correct\", \"Score\": \"69.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"STL-10\", \"Metric\": \"Percentage correct\", \"Score\": \"68.19\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"STL-10\", \"Metric\": \"Percentage correct\", \"Score\": \"63.13\"}} ]"
    },
    {
        "Context": "Analyse des suites al\u00e9atoires engendr\u00e9es par des automates cellulaires et applications\u00e0 la cryptographie On s'int\u00e9resse aux interactions entre la cryptologie et les automates cellulaires. Il a\u00e9t\u00e9 montr\u00e9 r\u00e9cemment qu'il n'existe pas de r\u00e8gle\u00e9l\u00e9mentaire d'automate cellulaire non-lin\u00e9aire robuste\u00e0 la corr\u00e9lation. Ce r\u00e9sultat limite fortement l'usage d'automates cellulaires pour la construction de suites pseudo-al\u00e9atoires servant de cl\u00e9s utilisables en cryptographie\u00e0 cl\u00e9 secr\u00e8te. De plus, pour de tels m\u00e9canismes de g\u00e9n\u00e9ration de suites pseudo-al\u00e9atoires, Meier et Staffelbach ont propos\u00e9 une technique de cryptanalyse efficace. Cependant, des pistes subsistent pour construire des automates cellulaires susceptibles d'engendrer de bonnes suites pseudo-al\u00e9atoires, que nous\u00e9voquerons\u00e0 la fin de cet article.Abstract : This paper considers interactions between cellular automata and cryptology. It is known that non-linear elementary rule which is correlation-immune don't exist. This results limits the use of cellular automata as pseudo-random generators suitable for cryptographic applications. In addition, for this kind of pseudo-random generators, a successful cryptanalysis was proposed by Meier and Staffelbach. However, other ways to design cellular automata capable to generate good pseudo-random sequences remain and will be discussed in the end of this article.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Chain-Based Representations for Solid and Physical Modeling In this paper we show that the (co)chain complex associated with a decomposition of the computational domain, commonly called a mesh in computational science and engineering, can be represented by a block-bidiagonal matrix that we call the Hasse matrix. Moreover, we show that topology-preserving mesh refinements, produced by the action of (the simplest) Euler operators, can be reduced to multilinear transformations of the Hasse matrix representing the complex. Our main result is anew representation of the (co)chain complex underlying field computations, a representation that provides new insights into the transformations induced by local mesh refinements. Our approach is based on first principles and is general in that it applies to most representational domains that can be characterized as cell complexes, without any restrictions on their type, dimension, codimension, orientability, manifoldness, connectedness.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "LiteSeg: A Novel Lightweight ConvNet for Semantic Segmentation Semantic image segmentation plays a pivotal role in many vision applications including autonomous driving and medical image analysis. Most of the former approaches move towards enhancing the performance in terms of accuracy with a little awareness of computational efficiency. In this paper, we introduce LiteSeg, a lightweight architecture for semantic image segmentation. In this work, we explore anew deeper version of Atrous Spatial Pyramid Pooling module (ASPP) and apply short and long residual connections, and depthwise separable convolution, resulting in a faster and efficient model. LiteSeg architecture is introduced and tested with multiple backbone networks as Darknet19, MobileNet, and ShuffleNet to provide multiple trade-offs between accuracy and computational cost. The proposed model LiteSeg, with MobileNetV2 as a backbone network, achieves an accuracy of 67.81% mean intersection over union at 161 frames per second with 640 \u00d7 360 resolution on the Cityscapes dataset.Index Terms-semantic image segmentation, atrous spatial pyramid pooling, encoder decoder, and depthwise separable convolution. In our evaluation of the proposed method, the effectiveness of LiteSeg with different backbone networks is empirically tested and the results are compared with the lightweight state-of-the-art architectures on Cityscapes dataset The Cityscapes dataset is a large-scale dataset for semantic understanding of urban scenes",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Category mIoU\", \"Score\": \"88.29\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"GFlops\", \"Score\": \"103.09\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"70.75%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Category mIoU\", \"Score\": \"86.79\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"GFlops\", \"Score\": \"4.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"67.81%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Category mIoU\", \"Score\": \"85.39\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"GFlops\", \"Score\": \"2.75\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"65.17%\"}}, {\"LEADERBOARD\": {\"Task\": \"Real-Time Semantic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"mIoU\", \"Score\": \"67.8%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"69.8%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\":\"mIoU\", \"Score\": \"75.7%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\":\"mIoU\", \"Score\": \"73.0%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\":\"mIoU\", \"Score\": \"68.0%\"}} ]"
    },
    {
        "Context": "OPTIMIZATION OF A CLASSICAL STAMPING PROGRESSION BY MODAL CORRECTION OF ANISOTROPY EARS This work is a development from the INETFORSMEP European project. It was proposed to realize a global optimization of a deep drawing industrial progression (made of several stages) fora cup manufacture. The objectives of the process were the thickness decrease and the geometrical parameters (especially the height). This paper completes this previous work in the aim of mastering the contour defect. From the optimal configuration, we are looking for minimizing the needed material and the number of forming operations. Our action is focused on the appearance of undesirable undulations (ears) located in the rim of the cups during forming, due to non-uniform crystallographic texture. These undulations can cause a significant amount of scrap, productivity loss and undesired cost during manufacture. In this paper, this phenomenon causes the use of five forming operations for the cup manufacture. The focus is to cut down from five to two forming stages by defining an optimal blank (size and shape). The advantage is to reduce the cost of the tool manufacturing and to minimize the needed material (by suppressing the part flange). The envisaged approach consists in defining a particular description of the ears' part by modal decomposition and then simulating several blank shapes and sizes generated by discrete cosine transformation (DCT). The use of a numerical simulation for the forming operation and the design of experiment technique allows to find mathematical links between the ears' formation and the DCT coefficients. An optimization is then possible by using mathematical links. This original approach leads to reduce the ears' amplitude by ten, with only fifteen numerical experiments. Moreover, we have downsized the number of forming stages from five to two with minimal material use.  Table 1. Numerical parameters of the finite element simulation. Process parameters Shell ( 4 nodes ) 7 3m / s Rigid surface table 1 . 1428 Table 3: variation range of each DCT coefficient s High value Low value Initial value 115 . 5mm 118 . 5mm D : nominal diameter 117mm Table 4: Central composite experimental design. < - target values each simulated part . All these results are presented in table 4 . - 1 . 35E+00 Profile 12 Profile 13 Profile 14 L1 - 1 . 24E+00 L2 L3 - 1 . 22E+00 L4 L5 1 . 39E+00 1 . 28E+00 1 . 82E - 01 Profile 10 1 . 52E+00 Profile 11 - 1 . 12E+00 1 . 34E+00 blank with particular D , A1 and A2 values . The result was a file of points from the bottom contour . Table 5: Optimal configuration and modal",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "3D MRI brain tumor segmentation using autoencoder regularization Automated segmentation of brain tumors from 3D magnetic resonance images (MRIs) is necessary for the diagnosis, monitoring, and treatment planning of the disease. Manual delineation practices require anatomical knowledge, are expensive, time consuming and can be inaccurate due to human error. Here, we describe a semantic segmentation network for tumor subregion segmentation from 3D MRIs based on encoder-decoder architecture. Due to a limited training dataset size, a variational auto-encoder branch is added to reconstruct the input image itself in order to regularize the shared decoder and impose additional constraints on its layers. The current approach won 1st place in the BraTS 2018 challenge.  Table 1. VAE decoder branch structure, where GN stands for group normalization (with group size of 8), Conv -3x3x3 convolution, Conv1 -1x1x1 convolution, AddId - addition of identity/skip connection, UpLinear -3D linear spatial upsampling, Dense -fully connected layer - fully connected layer 256x20x24x16 256x1 128x1 32x160x192x128 128x40x48x32 64x80x96x64 Ops Repeat Table 1 . VAE decoder branch structure , where GN stands for group normalization Table 2. BraTS 2018 validation dataset results. Mean Dice and Hausdorff measure- ments of the proposed segmentation method. EN -enhancing tumor core, WT -whole tumor, TC -tumor core. Hausdorff ( mm ) Dice WT ET TC Table 3. BraTS 2018 testing dataset results. Mean Dice and Hausdorff measurements of the proposed segmentation method. EN -enhancing tumor core, WT -whole tumor, TC -tumor core. Hausdorff ( mm ) Dice WT ET TC",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Brain Tumor Segmentation\", \"Dataset\": \"BRATS 2018\", \"Metric\": \"Dice Score\", \"Score\": \"0.87049\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Medical Image Segmentation\", \"Dataset\": \"TCIA Pancreas-CT\", \"Metric\": \"Dice Score\", \"Score\": \"0.84\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Medical Image Segmentation\", \"Dataset\": \"CT-150\", \"Metric\": \"Dice Score\", \"Score\": \"0.88\"}} ]"
    },
    {
        "Context": "Video Classification with FineCoarse Networks A rich representation of the information in video data can be realized by means of frequency analysis. Fine motion details from the boundaries of moving regions are characterized by high frequencies in the spatio-temporal domain. Meanwhile, lower frequencies are encoded with coarse information containing substantial redundancy, which causes low efficiency for those video models that take as input raw RGB frames. In this work, we propose a Motion Band-pass Module (MBPM) for separating the fine-grained information from coarse information in raw video data. By representing the coarse information with low resolution, we can increase the efficiency of video data processing. By embedding the MBPM into a two-pathway CNN architecture, we define a FineCoarse network. The efficiency of the FineCoarse network is determined by avoiding the redundancy in the feature space processed by the two pathways: one operates on downsampled features of low-resolution data, while the other operates on the fine-grained motion information captured by the MBPM. The proposed FineCoarse network outperforms many recent video processing models on Kinetics400, UCF101 and HMDB51. Furthermore, our approach achieves the state-of-the-art with 57.0% top-1 accuracy on Something-Something V1. In this section, we first introduce the datasets and implementation details Datasets We evaluate our approach on Something-Something V1, Kinetics400, UCF101 and HMDB51 Since Something-Something is widely used for evaluating temporal modeling efficiency, we mainly evaluate FineCoarse networks on this dataset As for other datasets, we utilize the sparse sampling strategy as shown in where a video is equally divided into N segments, and we randomly sample 3 consecutive frames in each segment to constitute a video clip of length T =3N Table 2: Ablation Studies for FineCoarse Networks on Something-Something V1. We show top-1 and top-5 prediction accuracy (%), as well as computational complexity measured in GFLOPs for a single crop & single clip. GFLOPs Top - 5 Top - 1 Table 3: Ablation study on the spatio-temporal input size (width 2 \u00d7time) of the two pathways in FineCoarse network. GFLOPs ( % ) Input size for Fine Top - 5 for Coarse Top - 1 Table 4: Comparison results on Something-Something V1. \"N/A\" indicates the numbers are not available. + refers to averaging the predictions from different models. - Top - 1 ( % ) Top - 5 ( % ) Table 5: Comparison results on Kinetics400. We report the inference cost of multiple \"views\" (spatial crops \u00d7 temporal clips). - FLOPs ( % ) Top - 5 Top - 1 Table 6: Comparison results on HMDB51 and UCF101. We report",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@1\", \"Score\": \"77.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@5\", \"Score\": \"93.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"Something-Something V1\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"57.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"Something-Something V1\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"83.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"UCF101\", \"Metric\": \"3-fold Accuracy\", \"Score\": \"97.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"HMDB-51\", \"Metric\": \"Average accuracy of 3 splits\", \"Score\": \"77.6\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@1\", \"Score\": \"75.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Recognition\", \"Dataset\": \"Something-Something V1\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"49.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Recognition\", \"Dataset\": \"Something-Something V1\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"46.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Recognition\", \"Dataset\": \"Something-Something V1\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"84.5\"}} ]"
    },
    {
        "Context": "Transformer Tracking Correlation acts as a critical role in the tracking field, especially in recent popular Siamese-based trackers. The correlation operation is a simple fusion manner to consider the similarity between the template and the search region. However, the correlation operation itself is a local linear matching process, leading to lose semantic information and fall into local optimum easily, which maybe the bottleneck of designing high-accuracy tracking algorithms. Is there any better feature fusion method than correlation? To address this issue, inspired by Transformer, this work presents a novel attention-based feature fusion network, which effectively combines the template and search region features solely using attention. Specifically, the proposed method includes an ego-context augment module based on self-attention and a cross-feature augment module based on cross-attention. Finally, we present a Transformer tracking (named TransT) method based on the Siamese-like feature extraction backbone, the designed attention-based fusion mechanism, and the classification and regression head. Experiments show that our TransT achieves very promising results on six challenging datasets, especially on largescale LaSOT, TrackingNet, and GOT-10k benchmarks. Our tracker runs at approximatively 50 fps on GPU. Code and models are available at https://github.com/chenxindlut/TransT. We report the detailed comparison results on the large-scale LaSOT, Track-ingNet, and GOT-10k datasets in LaSOT is a recent large-scale dataset with high-quality annotations, which contains 1400 challenging videos: 1120 for training and 280 for testing We follow the one-pass evaluation (Success and Precision) to compare different tracking algorithms on the LaSOT test set reports an attribute-based evaluation of representative stateof-the-art algorithms, illustrating that the TransT performs much better than other competing trackers on all attributes TrackingNet is a large-scale tracking dataset, which covers diverse object classes and scenes We submit our tracker's outputs to the official online evaluation server, and report the Success (AUC) and Precision (P and P N orm ) results in The GOT-10k dataset contains 10k sequences for training and 180 for testing We evaluate our tracker on some commonly used smallscale datasets, including NFS, OTB2015, and UAV123 We evaluate the proposed tracker on the 30 fps Table 1. State-of-the-art comparison on TrackingNet, LaSOT, and GOT-10k. The best two results are shown in red and blue fonts. SR TrackingNet [ 30 ] GOT - 10k [ 19 ] LaSOT [ 14 ] - Source Table 2. Ablation study on TrackingNet, LaSOT, and GOT-10k. The best results are shown in the red font. PNorm P 19 ] SR0 . 5 AO SR0 . 75 AUC Table 1. TransT- GOT denotes training with only the GOT-10k traning set. PNorm P 19 ] SR0 . 5 AO SR0 . 75 AUC Table 3. Comparison with correlation on TrackingNet, LaSOT, and GOT-10k. The best results are shown in the red font. \u221a \u221a",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Visual Object Tracking\", \"Dataset\": \"LaSOT\", \"Metric\": \"AUC\", \"Score\": \"64.9\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Visual Object Tracking\", \"Dataset\": \"TrackingNet\", \"Metric\": \"Accuracy\", \"Score\": \"81.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Object Tracking\", \"Dataset\": \"TrackingNet\", \"Metric\": \"Normalized Precision\", \"Score\": \"85.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Object Tracking\", \"Dataset\": \"TrackingNet\", \"Metric\": \"Precision\", \"Score\": \"80.0\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Object Tracking\", \"Dataset\": \"LaSOT\", \"Metric\": \"AUC\", \"Score\": \"64.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Object Tracking\", \"Dataset\": \"GOT-10k\", \"Metric\": \"Average Overlap\", \"Score\": \"64.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Visual Object Tracking\", \"Dataset\": \"GOT-10k\", \"Metric\": \"Success Rate 0.5\", \"Score\": \"72.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Video Object Segmentation\", \"Dataset\": \"DAVIS 2017 (test-dev-dev)\", \"Metric\":"
    },
    {
        "Context": "Benchmark for Generic Product Detection: A Low Data Baseline for Dense Object Detection Object detection in densely packed scenes is anew area where standard object detectors fail to train well . Dense object detectors like RetinaNet (Lin et al., 2017) trained on large and dense datasets show great performance. We train a standard object detector on a small, normally packed dataset with data augmentation techniques. This dataset is 265 times smaller than the standard dataset, in terms of number of annotations. This low data baseline achieves satisfactory results (mAP=0.56) at standard IoU of 0.5. We also create a varied benchmark for generic SKU product detection by providing full annotations for multiple public datasets. It can be accessed at this URL. We hope that this benchmark helps in building robust detectors that perform reliably across different settings in the wild. The denseness of the datasets depends on two factors SKU110K is by far one of the most dense datasets for object detection An analysis of the denseness of other datasets in the current benchmark is shown in Table 1: Details of the datasets in the benchmark. # represents the count. Object sizes (Mean and Standard Deviation) are relative to the image size. Average Image size is shown in Megapixels Object Size Avg Img Size ( Std ) ( Mean ) Table 3: Details of Holoselecta dataset. # represents the count. Average size of the entire image is shown in megapixels 208 30 #Images 24 Avg Img Size Type of Images Table 4: Statistics of trainset. # represents the count. Num- ber of Annotations is denoted by #Anns #Images #Obj / Img #Anns Table 5: Performance of our Faster-RCNN across different general product datasets. * denotes results obtained using the trained model given at URL as is. - AR 300",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Object Detection\", \"Dataset\": \"COCO 2017\", \"Metric\": \"Mean mAP\", \"Score\": \"3153\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"SKU-110K\", \"Metric\":\"mAP\", \"Score\": \"0.846\"}}, { \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"SKU-110K\", \"Metric\":\"mAP\", \"Score\": \"0.74\"}}, { \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"SKU-110K\", \"Metric\":\"mAP\", \"Score\": \"0.63\"}}, { \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"SKU-110K\", \"Metric\":\"mAP\", \"Score\": \"0.82\"}}, { \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"SKU-110K\", \"Metric\":\"mAP\", \"Score\": \"0.73\"}}, { \"LEADERBOARD\": { \"Task\": \"Object Detection\", \"Dataset\": \"SKU-110K\", \"Metric\":\"mAP\", \"Score\": \"0.54\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Object Detection\", \"Dataset\": \"SKU-110K\", \"Metric\": \"MAP\", \"Score\": \"33.3\"}}, { "
    },
    {
        "Context": "Deep Diving into BitTorrent Locality [Please cite the IEEE INFOCOM'11 version of this paper] A substantial amount of work has recently gone into localizing BitTorrent traffic within an ISP in order to avoid excessive and oftentimes unnecessary transit costs. Several architectures and systems have been proposed and the initial results from specific ISPs and a few torrents have been encouraging. In this work we attempt to deepen and scale our understanding of locality and its potential. Looking at specific ISPs, we consider tens of thousands of concurrent torrents, and thus capture ISP-wide implications that cannot be appreciated by looking at only a handful of torrents. Secondly, we go beyond individual case studies and present results for the top 100 ISPs in terms of number of users represented in our dataset of up to 40K torrents involving more than 3.9M concurrent peers and more than 20M in the course of a day spread in 11K ASes. We develop scalable methodologies that permit us to process this huge dataset and answer questions such as: \"what is the minimum and the maximum transit traffic reduction across hundreds of ISPs? \", \"what are the win-win boundaries for ISPs and their users? \", \"what is the maximum amount of transit traffic that can be localized without requiring finegrained control of inter-AS overlay connections? \", \"what is the impact to transit traffic from upgrades of residential broadband speeds? \". Some points worth noting: The largest torrent has approximately 60K clients in all three datasets Looking at the number of peers and torrents per ISP we see that mn40K has bigger values which is expected since it is a much bigger dataset than the other two and thus contains more and bigger ISPs (notice that in the numbers for mn40K are per snapshot, whereas for the other two are aggregates over a day, i.e., totals from 24 snapshots) If not otherwise stated, our default dataset will be mn40K Seeder/leecher ratios: In dataset pb600 we know exactly if a peer is seeder or leacher but in mn40K and mn3K we do not have this information Then we made a client in our dataset a seeder with probability equal to the seeder/leecher ratio of its torrent We validated this technique with the dataset pb600 obtaining minor variation compared to real seeder distributions Table 1: Torrent sets collected in the period Aug-Oct 2009. For mn40K we collected three ver- sions, with one week in between them. We actu- ally crawled 100K torrents but only around 40K had peers. For mn3K and pb600 we repeated the crawl every hour for one day. The #IPs and #ISPs for mn40K are per snapshot, whereas for mn3K and pb600 are daily totals. pb600 |V ( T ) | |V ( A ) | mn40k 10 Table 2: Results for ISPs EU1-EU3, US1-US3, under different demographic and speed datasets ( a ) Transit traffic reduction under mn40K and Ookla speeds . Degradation LOIF Strict median QoS Locality under mn40K and Ookla speeds . Table 3: Live torrent characteristics Figure 6 : Comparision between LOIF and Local Percentage of Seed Local Remote",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Study of first-order thermal \u03a3\u2206 architecture for convective accelerometers This paper presents the study of an original closed-loop conditioning approach for fully-integrated convective inertial sensors. The method is applied to an accelerometer manufactured on a standard CMOS technology using an auto-aligned bulk etching step. Using the thermal behavior of the sensor as a summing function, a first order sigma-delta modulator is built. This \"electrophysical\" modulator realizes an analog-to-digital conversion of the signal. Besides the feedback scheme should improve the sensor performance.I.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection Mechanical devices such as engines, vehicles, aircrafts, etc., are typically instrumented with numerous sensors to capture the behavior and health of the machine. However, there are often external factors or variables which are not captured by sensors leading to time-series which are inherently unpredictable. For instance, manual controls and/or unmonitored environmental conditions or load may lead to inherently unpredictable time-series. Detecting anomalies in such scenarios becomes challenging using standard approaches based on mathematical models that rely on stationarity, or prediction models that utilize prediction errors to detect anomalies. We propose a Long Short Term Memory Networks based Encoder-Decoder scheme for Anomaly Detection (EncDec-AD) that learns to reconstruct 'normal' time-series behavior, and thereafter uses reconstruction error to detect anomalies. We experiment with three publicly available quasi predictable time-series datasets: power demand, space shuttle, and ECG, and two realworld engine datasets with both predictive and unpredictable behavior. We show that EncDec-AD is robust and can detect anomalies from predictable, unpredictable, periodic, aperiodic, and quasi-periodic time-series. Further, we show that EncDec-AD is able to detect anomalies from short time-series (length as small as 30) as well as long time-series (length as large as 500). We consider four real-world datasets: power demand, space shuttle valve, ECG, and engine (see) The first three are taken from whereas the engine dataset is a proprietary one encountered in a real-life project The engine dataset contains data for two different applications: Engine-P where the time-series is quasi-predictable, Engine-NP where the time-series is unpredictable, for reasons such as mentioned earlier shows the performance of EncDec-AD on all the datasets Power demand dataset contains one univariate time-series with 35, 040 readings for power demand recorded over a period of one year Space shuttle dataset contains periodic sequences with 1000 points per cycle, and 15 such cycles Engine dataset contains readings for 12 sensors such as coolant temperature, torque, accelerator (control variable), etc ECG dataset contains quasi-periodic time-series (duration of a cycle varies from one instance to another) For our experiment, we use the first channel from qtdb/sel102 dataset where the time-series contains Table 1. Nature of datasets. N , Nn and Na is no. of original se- quences, normal subsequences and anomalous subsequences, re- spectively. spectively . \u221e Quasi - periodic Nn 12 Aperiodic 45 F \u03b2 - score Periodicity Dimensions Periodic 240 P 1 \u03b2 R 200 215 20 TPR / FPR Table 2. F \u03b2 -scores and positive likelihood ratios (TPR/FPR). spectively . \u221e Quasi - periodic Nn 12 Aperiodic 45 F \u03b2 - score Periodicity Dimensions Periodic 240 P 1 \u03b2 R 200 215 20 TPR / FPR",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Time Series Classification\", \"Dataset\": \"Physionet 2017 Atrial Fibrillation\", \"Metric\": \"AUC\", \"Score\": \"0.719\"}}, {\"LEADERBOARD\": {\"Task\": \"Outlier Detection\", \"Dataset\": \"ECG5000\", \"Metric\": \"Accuracy\", \"Score\": \"0.934\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Anomaly Detection\", \"Dataset\": \"Automatic Misogynistic Identification\", \"Metric\": \"Accuracy\", \"Score\": \"0.83\"}} ]"
    },
    {
        "Context": "Coupled Generative Adversarial Networks We propose coupled generative adversarial network (CoGAN) for learning a joint distribution of multi-domain images. In contrast to the existing approaches, which require tuples of corresponding images in different domains in the training set, CoGAN can learn a joint distribution without any tuple of corresponding images. It can learn a joint distribution with just samples drawn from the marginal distributions. This is achieved by enforcing a weight-sharing constraint that limits the network capacity and favors a joint distribution solution over a product of marginal distributions one. We apply CoGAN to several joint distribution learning tasks, including learning a joint distribution of color and depth images, and learning a joint distribution of face images with different attributes. For each task it successfully learns the joint distribution without any tuple of corresponding images. We also demonstrate its applications to domain adaptation and image transformation. In,,, and, we show several example images of the training images used for the pair image generation tasks in the experiment section.,, and contain the statistics of the training datasets for the experiments Table 1: Unsupervised domain adaptation performance comparison. The table reported classification accuracies achieved by competing algorithms. to USPS to MNIST [ 20 ] [ 19 ] [ 18 ] [ 17 ] CoGAN Table 3: The table shows the performance of pair generation of digits and corresponding edge images (Task A) with different CoGAN weight-sharing configurations. The results were the average pixel agreement ratios over 10000 images over 5 trials. Avg . pixel agreement ratio 5 5 , 4 , 3 , 2 5 , 4 , 3 Weight - sharing layers in the generative models 5 , 4 Table 4: The table shows the performance of pair generation of digits and corresponding negative images (Task B) with different CoGAN weight-sharing configurations. The results were the average pixel agreement ratios over 10000 images over 5 trials. Avg . pixel agreement ratio 5 5 , 4 , 3 , 2 5 ,",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"Cityscapes Photo-to-Labels\", \"Metric\": \"Class IOU\", \"Score\": \"0.08\"}}, {\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"Cityscapes Photo-to-Labels\", \"Metric\": \"Per-class Accuracy\", \"Score\": \"11%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"Cityscapes Photo-to-Labels\", \"Metric\": \"Per-pixel Accuracy\", \"Score\": \"45%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"Cityscapes Labels-to-Photo\", \"Metric\": \"Class IOU\", \"Score\": \"0.06\"}}, {\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"Cityscapes Labels-to-Photo\", \"Metric\": \"Per-class Accuracy\", \"Score\": \"10%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"Cityscapes Labels-to-Photo\", \"Metric\": \"Per-pixel Accuracy\", \"Score\": \"40%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image-to-Image Translation\", \"Dataset\": \"SYNTHIA-to-Cityscapes\", \"Metric\":\"mIoU (13 classes)\", \"Score\": \"42.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Synthetic-to-Real Translation\", \"Dataset\": \"GTAV-to-Cityscapes Labels\", \"Metric\":\"mIoU\", \"Score\": \"44.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Domain Adaptation\", \"Dataset\": \"SYNSIG-to-Cityscapes\", \"Metric\":\"mIoU\", \"Score\": \"42.8\"}} ]"
    },
    {
        "Context": "Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network . However, attempts to combine existing techniques to create a practical model are still uncommon. In this study, we carryout extensive experiments to validate that carefully assembling these techniques and applying them to basic CNN models (e.g., ResNet and MobileNet) can improve the accuracy and robustness of the models while minimizing the loss of throughput. Our proposed assembled ResNet-50 shows improvements in top-1 accuracy from 76.3% to 82.78%, mCE from 76.0% to 48.9% and mFR from 57.7% to 32.3% on ILSVRC2012 validation set. With these improvements, inference throughput only decreases from 536 to 312. To verify the performance improvement in transfer learning, fine grained classification and image retrieval tasks were tested on several public datasets and showed that the improvement to backbone network performance boosted transfer learning performance significantly. Our approach achieved 1st place in the iFood Competition Fine-Grained Visual Recognition at CVPR 2019 1 , and the source code and trained models will be made publicly available 2 . Top-1 The top-1 is a measure of classification accuracy on the ILSVRC2012 validation dataset The validation dataset consists of 50,000 images of 1,000 classes Table 1. Summary of key results. Top-1 is ILSVRC2012 top-1 validation accuracy. mCE is mean corruption error and mFR is mean flip rate (Lower is better.) 312 536 mFR Top - 1 - mCE Table 2. Result of channel attention with different configurations. R50 is a simple notation for ResNet-50. r is the reduction ratio of SK in the Fuse operation. The piecewise learning rate decay is used in these experiments. 466 402 326 536 382 r SK Top - 1 Table 3. Results for downsampling with anti-aliasing. The perfor- mance of the model was tested with different configurations for downsampling with anti-aliasing. The piecewise learning rate de- cay is used in these experiments. 422 456 536 483 Strided Conv Top - 1 Table 6. Performance comparison of stacking network tweaks. By stacking the ResNet-D, Selective Kernel (SK), BigLittleNet (BL) and downsampling with anti-aliasing (AA), we have steadily improved the ResNet-50",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet ReaL\", \"Metric\": \"Accuracy\", \"Score\": \"88.65%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet ReaL\", \"Metric\": \"Accuracy\", \"Score\": \"87.82%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"84.2%\"}}, {\"LEADERBOARD\": {\"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"Oxford 102 Flowers\", \"Metric\": \"Accuracy\", \"Score\": \"98.9%\"}}, {\"LEADERBOARD\": {\"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"SOP\", \"Metric\": \"Recall@1\", \"Score\": \"85.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"FGVC Aircraft\", \"Metric\": \"Accuracy\", \"Score\": \"92.4%\"}}, {\"LEADERBOARD\": {\"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"Food-101\", \"Metric\": \"Accuracy\", \"Score\": \"92.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"Stanford Cars\", \"Metric\": \"Accuracy\", \"Score\": \"94.4%\"}}, {\"LEADERBOARD\": {\"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"Oxford-IIIT Pets\", \"Metric\": \"Accuracy\", \"Score\": \"94.3%\"}}, {\"LEADERBOARD\": {\"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"Oxford-IIIT Pets\", \"Metric\": \"Top-1 Error Rate\", \"Score\": \"5.7%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of params\", \"Score\": \"57.3M\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"80.1%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"95.1%\"}} ]"
    },
    {
        "Context": "Modeling Multi-Point Transport Protocol in P2P Networks Traditional end-to-end congestion control mechanisms assume data transferring happens between each pair user. In contrast, in a P2P network, many peers may locally keep a copy of a specific data object. If the path between a pair of peers is congested, the requesting peer who wants to download data will switch to another peer in its neighbor peer list to fetch the data instead of decreasing the download rate from the current peer. Thus, it is critical to study the performance in multi-point-to-multi-point (M2M) transport protocol in a P2P network. In this paper, we build a mathematical model for identifying the key parameters for the M2M transport protocol and also the relationships among these parameters. Finally, we conduct simulation experiments to validate our model.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Automatic Generation of Proof Tactics for Finite-Valued Logics * A number of flexible tactic-based logical frameworks are nowadays available that can implement a wide range of mathematical theories using a common higher-order metalanguage. Used as proof assistants, one of the advantages of such powerful systems resides in their responsiveness to extensibility of their reasoning capabilities, being designed over rule-based programming languages that allow the user to build her own 'programs to construct proofs' -the so-called proof tactics.The present contribution discusses the implementation of an algorithm that generates sound and complete tableau systems fora very inclusive class of sufficiently expressive finite-valued propositional logics, and then illustrates some of the challenges and difficulties related to the algorithmic formation of automated theorem proving tactics for such logics. The procedure on whose implementation we will report is based on a generalized notion of analyticity of proof systems that is intended to guarantee termination of the corresponding automated tactics on what concerns theoremhood in our targeted logics.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "WDRN : A Wavelet Decomposed RelightNet for Image Relighting The task of recalibrating the illumination settings in an image to a target configuration is known as relighting. Relighting techniques have potential applications in digital photography, gaming industry and in augmented reality. In this paper, we address the one-to-one relighting problem where an image at a target illumination settings is predicted given an input image with specific illumination conditions. To this end, we propose a wavelet decomposed RelightNet called WDRN which is a novel encoder-decoder network employing wavelet based decomposition followed by convolution layers under a muti-resolution framework. We also propose a novel loss function called gray loss that ensures efficient learning of gradient in illumination along different directions of the ground truth image giving rise to visually superior relit images. The proposed solution won the first position in the relighting challenge event in advances in image manipulation (AIM) 2020 workshop which proves its effectiveness measured in terms of a Mean Perceptual Score which in turn is measured using SSIM and a Learned Perceptual Image Patch Similarity score. The dataset used in the experiments is the Virtual Image Dataset for Illumination Transfer (VIDIT) The dataset contains 390 different scenes which is captured at 40 different illumination settings (8 azimuthal angles and five different colour temperatures 2500K, 4500K etc.) with a total of 15, 600 images For the experiments as part of the challenge, we used 390 image pairs from the dataset, where the input image has a fixed illumination setting \u03b8 1 =North, T 1 = 6500K and the target is set at a different illumination setting \u03b8 2 = East, T 2 = 4500K In addition to the standard evaluation metrics like peak signal to noise ratio (PSNR) and SSIM, the performance of the proposed WDRN is evaluated using rather new perceptual metrics like Learned Perceptual Image Patch Similarity (LPIPS) and mean perceptual score (MPS) Table 1. Performance comparison of WDRN with competing entries in scene relight- ing and illumination estimation challenge, track-1 one-to-one relighting at AIM 2020 workshop. The MPS, used to determine the final ranking, is computed following Eq. (6). 13s 6s MPS SSIM PSNR LPIPS Run - time Table 2. Ablation study of wavelet domain network MPS Table 2 . Ablation study of wavelet domain network SSIM PSNR LPIPS Table 3. Ablation study of wavelet decomposition levels MPS SSIM PSNR Table 3 . Ablation study of wavelet decomposition levels LPIPS",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Relighting\", \"Dataset\": \"VIDIT\u201920 validation set\", \"Metric\": \"LPIPS\", \"Score\": \"0.2771\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Relighting\", \"Dataset\": \"VIDIT\u201920 validation set\", \"Metric\": \"MPS\", \"Score\": \"0.6935\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Relighting\", \"Dataset\": \"VIDIT\u201920 validation set\", \"Metric\": \"PSNR\", \"Score\": \"17.45\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Relighting\", \"Dataset\": \"VIDIT\u201920 validation set\", \"Metric\": \"Runtime(s)\", \"Score\": \"0.05\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Relighting\", \"Dataset\": \"VIDIT\u201920 validation set\", \"Metric\": \"SSIM\", \"Score\": \"0.6642\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Relighting\", \"Dataset\": \"VIDIT\u201920 validation set\", \"Metric\": \"LPIPS\", \"Score\": \"0.3712\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Relighting\", \"Dataset\": \"VIDIT\u201920 validation set\", \"Metric\": \"MPS\", \"Score\": \"0.5992\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Relighting\", \"Dataset\": \"VIDIT\u201920 validation set\", \"Metric\": \"PSNR\", \"Score\": \"17.20\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Relighting\", \"Dataset\": \"VIDIT\u201920 validation set\", \"Metric\": \"Runtime(s)\", \"Score\": \"0.0058\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Relighting\", \"Dataset\": \"VIDIT\u201920 validation set\", \"Metric\": \"SSIM\", \"Score\": \"0.5696\"}} ]"
    },
    {
        "Context": "Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning Our work involves enriching the Stack-LSTM transition-based AMR parser (Ballesteros and Al-Onaizan, 2017) by augmenting training with Policy Learning and rewarding the Smatch score of sampled graphs. In addition, we also combined several AMR-to-text alignments with an attention mechanism and we supplemented the parser with pre-processed concept identification, named entities and contextualized embeddings. We achieve a highly competitive performance that is comparable to the best published results. We show an in-depth study ablating each of the new components of the parser. We start by reimplementing BO 5 and we train models with the most recent dataset (LDC2017T10) 6 All our experiments use beam 10 for decoding and they are the best (when evaluated in the development set) of 5 different random seeds 5 BO reported results on the 2014 dataset Table 1: Results, including comparison with the best systems, in the LDC2017T10 test set (aka AMR 2.0). Results highlighted Concepts Smatch Negations Wikification Experiment Named Entities Id SRL Rentrancies Unlabeled No WSD",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"AMR Parsing\", \"Dataset\": \"LDC2017T10\", \"Metric\": \"Smatch\", \"Score\": \"73.4\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"AMR Parsing\", \"Dataset\": \"LDC2017T10\", \"Metric\": \"Smatch\", \"Score\": \"71.3\"}} ]"
    },
    {
        "Context": "Computer-aided assessment of physical rehabilitation entails evaluation of patient performance in completing prescribed rehabilitation exercises, based on processing movement data captured with a sensory system. Despite the essential role of rehabilitation assessment toward improved patient outcomes and reduced healthcare costs, existing approaches lack versatility, robustness, and practical relevance. In this paper, we propose a deep learning-based framework for automated assessment of the quality of physical rehabilitation exercises. The main components of the framework are metrics for quantifying movement performance, scoring functions for mapping the performance metrics into numerical scores of movement quality, and deep neural network models for generating quality scores of input movements via supervised learning. The proposed performance metric is defined based on the log-likelihood of a Gaussian mixture model, and encodes low-dimensional data representation obtained with a deep autoencoder network. The proposed deep spatio-temporal neural network arranges data into temporal pyramids, and exploits the spatial characteristics of human movements by using sub-networks to process joint displacements of individual body parts. The presented framework is validated using a dataset often rehabilitation exercises. The significance of this work is that it is the first that implements deep neural networks for assessment of rehabilitation performance. For validation of the presented framework, we created the UI-PRMD dataset The dataset consists of skeletal data collected from 10 healthy subjects A detailed description of the UI-PRMD dataset is provided in",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Action Quality Assessment\", \"Dataset\": \"UI-PRMD\", \"Metric\": \"Average mean absolute error\", \"Score\": \"0.0253\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Quality Assessment\", \"Dataset\": \"KIMORE\", \"Metric\": \"Average mean absolute error\", \"Score\": \"0.0379\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Surgical tool detection\", \"Dataset\": \"U-Detnamese Representation-based\", \"Metric\": \"2-Class Accuracy\", \"Score\": \"0.304\"}} ]"
    },
    {
        "Context": "Using Similarity Measures to Select Pretraining Data for NER Word vectors and Language Models (LMs) pretrained on a large amount of unlabelled data can dramatically improve various Natural Language Processing (NLP) tasks. However, the measure and impact of similarity between pretraining data and target task data are left to intuition. We propose three cost-effective measures to quantify different aspects of similarity between source pretraining and target task data. We demonstrate that these measures are good predictors of the usefulness of pretrained models for Named Entity Recognition (NER) over 30 data pairs. Results also suggest that pretrained LMs are more effective and more predictable than pretrained word vectors, but pretrained word vectors are better when pretraining data is dissimilar. Micro average F 1 score is used to evaluate the performance of the tagger The supervised model used for NER is the same BiLSTM-CRF model mentioned above, and we follow the approach proposed by to incorporate the pretrained LMs Table 2: List of the target NER data sets and their specifications. Size is shown in number of tokens. biology and chemistry experi - Physics . Protein , Science , Material Sciences and Protocols Description taxa , Abstract Protein , DNA , RNA , Cell line and Cell type terial ( including corpora , physical materials ) Entity Types Person , Organization , Location , Miscellany Process ( including methods , equipment ) , Task and Ma - Adverse Drug Event , Disease , Drug , Finding , Symp - which is a forum where con - Newswire Biomacromolecular sequence , Entrez gene , Biological vice , Location , Method , Reagent , Speed , Tempera - Table 3: Similarity between source and target data sets (left), and the effectiveness of word vectors and LMs pretrained using different sources for NER (right). Lower PPL or WVV values indicate higher similarity between",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Named Entity Recognition\", \"Dataset\": \"WetLab\", \"Metric\": \"F1\", \"Score\": \"79.62\"}}, {\"LEADERBOARD\": {\"Task\": \"Named Entity Recognition\", \"Dataset\": \"JNLPBA\", \"Metric\": \"F1\", \"Score\": \"74.29\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Drug\u2013drug Interaction Extraction\", \"Dataset\": \"DDI extraction 2013 corpus\", \"Metric\": \"F1\", \"Score\": \"0.8255\"}}, { \"LEADERBOARD\": { \"Task\": \"Drug\u2013drug Interaction Extraction\", \"Dataset\": \"DDI extraction 2013 corpus\", \"Metric\": \"F1\", \"Score\": \"0.805\"}} ]"
    },
    {
        "Context": "COINDUCTIVE FORMAL REASONING IN EXACT REAL ARITHMETIC In this article we present a method for formally proving the correctness of the lazy algorithms for computing homographic and quadratic transformations -of which field operations are special cases-on a representation of real numbers by coinductive streams. The algorithms work on coinductive stream of M\u00f6bius maps and form the basis of the Edalat-Potts exact real arithmetic. We use the machinery of the Coq proof assistant for the coinductive types to present the formalisation. The formalised algorithms are only partially productive, i.e., they do not output provably infinite streams for all possible inputs. We show how to deal with this partiality in the presence of syntactic restrictions posed by the constructive type theory of Coq. Furthermore we show that the type theoretic techniques that we develop are compatible with the semantics of the algorithms as continuous maps on real numbers. The resulting Coq formalisation is available for public download.  Table 1: Various phases of the formalisation. 4596 lines 2099 lines ( percentage of total ) Length 62 K ( 12 . 9% )",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Haptic Sensing for MEMS with Application for Cantilever and Casimir Effect This paper presents an implementation of the Cosserat theory into haptic sensing technologies for real-time simulation of microstructures. Cosserat theory is chosen instead of the classical theory of elasticity fora better representation of stress, especially in the nonlinear regime. The use of Cosserat theory leads to a reduction of the complexity of the modelling and thus increases its capability for real time simulation which is indispensable for haptic technologies. The incorporation of Cosserat theory into haptic sensing technology enables the designer to simulate in real-time the components in a virtual reality environment (VRE) which can enable virtual manufacturing and prototyping. The software tool created as a result of this methodology demonstrates the feasibility of the proposed model. As test demonstrators, a cantilever microbeam and microbridge undergoing bending in VRE are presented.I.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Delaunay Triangulations in Linear Time? (Part I) * We present anew and simple randomized algorithm for constructing the Delaunay triangulation using nearest-neighbor graphs for point location. Under suitable assumptions, it runs in linear expected time for points in the plane with polynomially bounded spread, i.e., if the ratio between the largest and smallest pointwise distance is polynomially bounded. This also holds for point sets with bounded spread in higher dimensions as long as the expected complexity of the Delaunay triangulation of a sample of the points is linear in the sample size. * This was originally part of a manuscript containing further material now in [5]. For the problem at hand, we found an algorithm avoiding the use of the history of the construnction [6].",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Using Mobilize Power Management IP for Dynamic & Static Power Reduction in SoC at 130 nm At 130 nm and 90 nm, power consumption (both dynamic and static) has become a barrier in the roadmap for SoC designs targeting battery powered, mobile applications. This paper presents the results of dynamic and static power reduction achieved implementing Tensilica's 32-bit Xtensa microprocessor core, using Virtual Silicon's Power Management IP. Independent voltage islands are created using Virtual Silicon's VIP PowerSaver standard cells by using voltage level shifting cells and voltage isolation cells to implement power islands. The VIP PowerSaver standard cells are characterized at 1.2V, 1.0V and 0.8V, to accommodate voltage scaling. Power islands can also be turned off completely. Designers can significantly lower both the dynamic power and the quiescent or leakage power of their SoC designs, with very little impact on speed or area using Virtual Silicon's VIP Gate Bias standard cells.  Table 1: Leakage vs. Process 1 . 00E - 05 Proceedings of the Design , Automation and Test in Europe Conference and Exhibition ( DATE ' 05 ) 0 90nm FF 125C - 0 Current",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences atone forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides anew solution to the LSTF problem. We extensively perform experiments on four datasets, including 2 collected real-world datasets for LSTF and 2 public benchmark datasets To explore the granularity on the LSTF problem, we create separate datasets as {ETTh 1 , ETTh 2 } for 1-hourlevel and ETTm 1 for 15-minute-level Due to the missing data, we convert the dataset into hourly consumption of 2 years and set 'MT 320' as the target value Weather 4 : This dataset contains local climatological data for nearly 1,600 U.S To better explore the ProbSparse selfattention's performance in our proposed Informer, we incorporate the canonical self-attention variant (Informer \u2020 ), the efficient variant Reformer and the most related work LogSparse self-attention in the experiments Setup: The input of each dataset is zero-mean normalized and summarize the univariate/multivariate evaluation results of all the methods on 4 datasets Note that we preserves 10% validation data for each dataset, so all the experiments Table 1: Univariate long sequence time-series forecasting results on four datasets (five cases). LogTrans Reformer LSTMa Informer DeepAR ARIMA Informer \u2020 MSE MAE Prophet Table 2: Multivariate long sequence time-series forecasting results on four datasets (five cases). Table 3: Ablation study of the ProbSparse self-attention mechanism. LogTrans Informer Figure 4 : The parameter sensitivity of three components in Informer . L / 2 - scale Dependency ( b ) Sampling Factor . 48 1440 Encoder Input ( horizon=48 ) 2880 - Informer , factor c=8 ( a ) Input length . 168 720 336 Prolong Input Length ( L x , L token ) 624 Informer , factor c=10 L / 4 - scale Dependency Encoder Input Length ( L x ) Encoder Input ( horizon=168 ) MSE L - scale Dependency Informer , factor c=3 Table 5: Ablation study of the self-attention distilling. 960 720 Prediction length 336 1200 480",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (168)\", \"Metric\": \"MAE\", \"Score\": \"0.389\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (168)\", \"Metric\": \"MSE\", \"Score\": \"0.232\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (168)\", \"Metric\": \"MAE\", \"Score\": \"0.595\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (168)\", \"Metric\": \"MSE\", \"Score\": \"2.800\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (168)\", \"Metric\": \"MAE\", \"Score\": \"0.346\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (168)\", \"Metric\": \"MSE\", \"Score\": \"0.183\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (168)\", \"Metric\": \"MAE\", \"Score\": \"0.504\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (168)\", \"Metric\": \"MSE\", \"Score\": \"0.396\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (336)\", \"Metric\": \"MAE\", \"Score\": \"0.387\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (336)\", \"Metric\": \"MSE\", \"Score\": \"0.222\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (336)\", \"Metric\": \"MAE\", \"Score\": \"0.593\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (336)\", \"Metric\": \"MSE\", \"Score\": \"0.468\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (48)\", \"Metric\": \"MAE\", \"Score\": \"0.314\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (48)\", \"Metric\": \"MSE\", \"Score\": \"0.155\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (48)\", \"Metric\": \"MAE\", \"Score\": \"0.474\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (48)\", \"Metric\": \"MSE\", \"Score\": \"3.190\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (24)\", \"Metric\": \"MAE\", \"Score\": \"0.247\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (24)\", \"Metric\": \"MSE\", \"Score\": \"0.098\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (24)\", \"Metric\": \"MAE\", \"Score\": \"0.284\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (24)\", \"Metric\": \"MSE\", \"Score\": \"0.108\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (48)\", \"Metric\": \"MAE\", \"Score\": \"0.319\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (48)\", \"Metric\": \"MSE\", \"Score\": \"0.158\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (48)\", \"Metric\": \"MAE\", \"Score\": \"0.424\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (48)\", \"Metric\": \"MSE\", \"Score\": \"0.175\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (336)\", \"Metric\": \"MAE\", \"Score\": \"0.417\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (336)\", \"Metric\": \"MSE\", \"Score\": \"0.263\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (336)\", \"Metric\": \"MAE\", \"Score\": \"0.738\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (336)\", \"Metric\": \"MSE\", \"Score\": \"2.753\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (720)\", \"Metric\": \"MAE\", \"Score\": \"0.435\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (720)\", \"Metric\": \"MSE\", \"Score\": \"0.269\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (720)\", \"Metric\": \"MAE\", \"Score\": \"0.766\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh1 (720)\", \"Metric\": \"MSE\", \"Score\": \"0.659\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (720)\", \"Metric\": \"MAE\", \"Score\": \"0.431\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (720)\", \"Metric\": \"MSE\", \"Score\": \"0.277\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (720)\", \"Metric\": \"MAE\", \"Score\": \"1.044\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (720)\", \"Metric\": \"MSE\", \"Score\": \"2.878\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (24)\", \"Metric\": \"MAE\", \"Score\": \"0.240\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (24)\", \"Metric\": \"MSE\", \"Score\": \"0.093\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (24)\", \"Metric\": \"MAE\", \"Score\": \"0.455\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Forecasting\", \"Dataset\": \"ETTh2 (24)\", \"Metric\": \"MSE\", \"Score\": \"3.554\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Sequential Image Classification\", \"Dataset\": \"Sequential CIFAR-10\", \"Metric\": \"Unpermuted Accuracy\", \"Score\": \"73.2\"}} ]"
    },
    {
        "Context": "Dual Variational Generation for Low Shot Heterogeneous Face Recognition Heterogeneous Face Recognition (HFR) is a challenging issue because of the large domain discrepancy and alack of heterogeneous data. This paper considers HFR as a dual generation problem, and proposes a novel Dual Variational Generation (DVG) framework. It generates large-scale new paired heterogeneous images with the same identity from noise, for the sake of reducing the domain gap of HFR. Specifically, we first introduce a dual variational autoencoder to represent a joint distribution of paired heterogeneous images. Then, in order to ensure the identity consistency of the generated paired heterogeneous images, we impose a distribution alignment in the latent space and a pairwise identity preserving in the image space. Moreover, the HFR network reduces the domain discrepancy by constraining the pairwise feature distances between the generated paired heterogeneous images. Extensive experiments on four HFR databases show that our method can significantly improve state-of-the-art results. The related code is available at https://github.com/BradyFU/DVG.Recently, the great progress of high-quality face synthesis [38,5,33,39] has made \"recognition via generation\" possible. TP- GAN [16]  and CAPG- GAN [13]  introduce face synthesis to improve the quantitative performance of large pose face recognition. For HFR, [32] proposes a two-path model to synthesize VIS images from NIR images.[36] utilizes a GAN based multi-stream feature fusion technique to generate VIS images from polarimetric thermal faces. However, all these methods are * Equal Contribution  Table 1: Experimental analyses on the CASIA NIR-VIS 2.0 database. The backbone is LightCNN-9. FID MD Rank - 1 Table 2: Comparisons with other state-of-the-art deep HFR methods on the CASIA NIR-VIS 2.0, the Oulu-CASIA NIR-VIS, the BUAA-VisNir and the IIIT-D Viewed Sketch databases. DVR VisNir databases , respectively Further , when backbone DVG IDR Method - state - of - the - art methods . We first employ LightCNN - 9 as the backbone to perform DVG , which 10 5 WCNN 10 4 10 1 False Positive Rate 10 2 - Rank - 1 FAR=0 . 1% ( c ) BUAA - VisNir ROC IIIT - D Viewed Sketch Oulu - CASIA NIR - VIS BUAA - VisNir FAR=1% 10 0 ( b ) Oulu - CASIA NIR - VIS ROC",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Face Verification\", \"Dataset\": \"Oulu-CASIA NIR-VIS\", \"Metric\": \"TAR @ FAR=0.001\", \"Score\": \"92.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Verification\", \"Dataset\": \"Oulu-CASIA NIR-VIS\", \"Metric\": \"TAR @ FAR=0.01\", \"Score\": \"98.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Verification\", \"Dataset\": \"IIIT-D Viewed Sketch\", \"Metric\": \"TAR @ FAR=0.01\", \"Score\": \"97.86\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Verification\", \"Dataset\": \"CASIA NIR-VIS 2.0\", \"Metric\": \"TAR @ FAR=0.001\", \"Score\": \"99.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Verification\", \"Dataset\": \"BUAA-VisNir\", \"Metric\": \"TAR @ FAR=0.001\", \"Score\": \"97.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Verification\", \"Dataset\": \"BUAA-VisNir\", \"Metric\": \"TAR @ FAR=0.01\", \"Score\": \"98.5\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"Oulu-CASIA NIR-VIS\", \"Metric\": \"TAR @ FAR=0.001\", \"Score\": \"84.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"Oulu-CASIA NIR-VIS\", \"Metric\": \"TAR @ FAR=0.01\", \"Score\": \"97.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"CASIA NIR-VIS 2.0\", \"Metric\": \"TAR @ FAR=0.001\", \"Score\": \"99.6\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"BUAA-VisNir\", \"Metric\": \"TAR @ FAR=0.001\", \"Score\": \"96.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"BUAA-VisNir\", \"Metric\": \"TAR @ FAR=0.01\", \"Score\": \"98.5\"}} ]"
    },
    {
        "Context": "Action Quality Assessment Across Multiple Actions Can learning to measure the quality of an action help in measuring the quality of other actions? If so, can consolidated samples from multiple actions help improve the performance of current approaches? In this paper, we carryout experiments to see if knowledge transfer is possible in the action quality assessment (AQA) setting. Experiments are carried out on our newly released AQA dataset (http: //rtis.oit.unlv.edu/datasets.html) consisting of 1106 action samples from seven actions with quality as measured by expert human judges. Our experimental results show that there is utility in learning a single model across multiple actions. To the best of our knowledge, there are only two publicly available AQA datasets and they have limited number of samples for each individual activity This is partly due to the extra effort required to collect samples as compared to an action recognition dataset In case of action recognition dataset compilation, an annotator might go to video hosting website such as YouTube and run a search query on names of actions While in compiling an AQA dataset, annotator has to mark starting and ending frames of a sample, and note down execution score, difficulty level, final score, etc Additionally, field experts are required to evaluate and score AQA data samples which can be quite difficult in many domains These factors limit dataset size To fill this data void, we introduce Action Quality Assessment 7 (AQA-7) dataset (), comprising samples from seven actions: {singles diving-10m platform, gymnastic vault, big air skiing, big Table 1: Characteristics of AQA-7 dataset. Avg . Seq . Len . 1 8 - 50 2 # Samples Score Range Table 2: All-Action vs. Single-Action models. Performance evaluation of single-action and all-action models in terms of action-wise and average Spearman's rank correlation (higher is better). First two frameworks simply average features to aggregate them and use SVR as the regression module. The bottom two frameworks use LSTM to aggregate features and use a fully-connected layer as the regression module. Our approach can be directly compared with single-action C3D-LSTM [16], since both have the same architecture. C3D - SVR [ 16 ] C3D - LSTM [ 16 ] oarding Snowb - Gymvault Skiing Sync . Dive 3m Diving Dive 10m Avg . Corr . - Table 3: Zero-shot AQA. Performance comparison of randomly-initialized model, single-action models (for e.g., first row shows the results of training on diving action measuring the quality",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Action Quality Assessment\", \"Dataset\": \"AQA-7\", \"Metric\": \"Spearman Correlation\", \"Score\": \"69.37%\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Quality Assessment\", \"Dataset\": \"AQA-7\", \"Metric\": \"Spearman Correlation\", \"Score\": \"64.78%\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Quality Assessment\", \"Dataset\": \"AQA-7\", \"Metric\": \"Spearman Correlation\", \"Score\": \"61.65%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Action Recognition\", \"Dataset\": \"UCF101\", \"Metric\": \"3-fold Accuracy\", \"Score\": \"97.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Recognition\", \"Dataset\": \"HMDB-51\", \"Metric\": \"Average accuracy of 3 splits\", \"Score\": \"71.5\"}} ]"
    },
    {
        "Context": "Distributed Representations of Sentences and Documents Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-ofwords models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks. For sentiment analysis, we use two datasets: Stanford sentiment treebank dataset and IMDB dataset Documents in these datasets differ significantly in lengths: We also test our method on an information retrieval task, where the goal is to decide if a document should be retrieved given a query Dataset: This dataset was first proposed by and subsequently extended by as a benchmark for sentiment analysis The dataset consists of three sets: 8544 sentences for training, 2210 sentences for test and 1101 sentences for validation (or development) Every sentence in the dataset has a label which goes from very negative to very positive in the scale from 0.0 to 1.0 The dataset comes with detailed labels for sentences, and subphrases in the same scale In total, there are 239,232 labeled phrases in the dataset The dataset can be downloaded at: http://nlp.Stanford.edu/sentiment/ We learn the word vectors and paragraph vectors using 75,000 training documents Table 1. The performance of our method compared to other ap- proaches on the Stanford Sentiment Treebank dataset. The error rates of other methods are reported in (Socher et al., 2013b). ( Socher et al . , 2013b ) grained ) Error rate Negative ) ( Fine - ( Positive /",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Question Answering\", \"Dataset\": \"WikiQA\", \"Metric\": \"MAP\", \"Score\": \"0.5976\"}}, {\"LEADERBOARD\": {\"Task\": \"Question Answering\", \"Dataset\": \"WikiQA\", \"Metric\": \"MRR\", \"Score\": \"0.6058\"}}, {\"LEADERBOARD\": {\"Task\": \"Question Answering\", \"Dataset\": \"WikiQA\", \"Metric\": \"MAP\", \"Score\": \"0.5110\"}}, {\"LEADERBOARD\": {\"Task\": \"Question Answering\", \"Dataset\": \"WikiQA\", \"Metric\": \"MRR\", \"Score\": \"0.5160\"}}, {\"LEADERBOARD\": {\"Task\": \"Question Answering\", \"Dataset\": \"QASent\", \"Metric\": \"MAP\", \"Score\": \"0.6762\"}}, {\"LEADERBOARD\": {\"Task\": \"Question Answering\", \"Dataset\": \"QASent\", \"Metric\": \"MRR\", \"Score\": \"0.7514\"}}, {\"LEADERBOARD\": {\"Task\": \"Question Answering\", \"Dataset\": \"QASent\", \"Metric\": \"MAP\", \"Score\": \"0.5213\"}}, {\"LEADERBOARD\": {\"Task\": \"Question Answering\", \"Dataset\": \"QASent\", \"Metric\": \"MRR\", \"Score\": \"0.6023\"}}, {\"LEADERBOARD\": {\"Task\": \"Text Classification\", \"Dataset\": \"IMDb\", \"Metric\": \"Accuracy (10 classes)\", \"Score\": \"-\"}}, {\"LEADERBOARD\": {\"Task\": \"Text Classification\", \"Dataset\": \"IMDb\", \"Metric\": \"Accuracy (2 classes)\", \"Score\": \"92.58\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"IMDb\", \"Metric\": \"Accuracy\", \"Score\": \"55.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"IMDb\", \"Metric\": \"Accuracy\", \"Score\": \"95.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"IMDb\", \"Metric\": \"Accuracy\", \"Score\": \"95.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"IMDb\", \"Metric\": \"Accuracy\", \"Score\": \"95.6\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"IMDb\", \"Metric\": \"Accuracy\", \"Score\": \"90.6\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"IMDb\", \"Metric\": \"Accuracy\", \"Score\": \"88.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"IMDb\", \"Metric\": \"Accuracy\", \"Score\": \"75.4\"}}, { \"LEADERBOARD\":"
    },
    {
        "Context": "Robust Lexical Features for Improved Neural Network Named-Entity Recognition Neural network approaches to Named-Entity Recognition reduce the need for carefully handcrafted features. While some features do remain in state-of-the-art systems, lexical features have been mostly discarded, with the exception of gazetteers. In this work, we show that this is unfair: lexical features are actually quite useful. We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia. From this, we compute -offline -a feature vector representing each word. When used with a vanilla recurrent neural network model, this representation yields substantial improvements. We establish anew state-of-the-art F1 score of 87.95 on ONTONOTES 5.0, while matching state-of-the-art performance with a F1 score of 91.73 on the over-studied CONLL-2003   dataset.This work is licensed under a Creative Commons Attribution 4.0 International License. License details: provides an overview of the two datasets For both datasets, we convert the IOB encoding to BILOU, since found the latter to perform better The four entity types are fairly evenly distributed, and the train/dev/test datasets present a similar type distribution This dataset is annotated with 18 entity types, and is much larger than CONLL Table 1: Topmost similar entity types to a few single-word mentions (left table) and non-entity words (right table directed in Sim Table 2: Statistics of the CONLL-2003 and ONTONOTES 5.0 datasets. #tok stands for the number of tokens, and #ent indicates the number of named-entities gold annotated. Dev #ent CONLL - 2003",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Named Entity Recognition\", \"Dataset\": \"CoNLL 2003 (English)\", \"Metric\": \"F1\", \"Score\": \"91.73\"}}, {\"LEADERBOARD\": {\"Task\": \"Named Entity Recognition\", \"Dataset\": \"Ontonotes v5 (English)\", \"Metric\": \"F1\", \"Score\": \"87.95\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Named Entity Recognition\", \"Dataset\": \"CoNLL 2003 (English)\", \"Metric\": \"F1\", \"Score\": \"84.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Named Entity Recognition\", \"Dataset\": \"CoNLL 2003 (English)\", \"Metric\": \"F1\", \"Score\": \"82.0\"}} ]"
    },
    {
        "Context": "On Distributed Model Checking of MSO on Graphs We consider distributed model-checking of Monadic Second-Order logic (MSO) on graphs which constitute the topology of communication networks. The graph is thus both the structure being checked and the system on which the distributed computation is performed. We prove that MSO can be distributively model-checked with only a constant number of messages sent over each link for planar networks with bounded diameter, as well as for networks with bounded degree and bounded treelength. The distributed algorithms rely on nontrivial transformations of linear time sequential algorithms for tree decompositions of bounded tree-width graphs.In the sequel, we present distributed algorithms to model-check MSO over classes of networks of bounded tree-width. These algorithms are obtained by transforming the centralized linear time algorithm presented in the previous section into distributed ones, which admit low complexity bounds. The challenge lies in two aspects. First, an ordered tree decomposition could be distributively constructed, with only O(1) messages sent over each link. Second, the constructed tree decomposition should be distributively stored in a suitable way, so that the tree automaton obtained from the MSO sentence, can be ran over the rooted labeled tree transformed from the ordered tree decomposition, in a bottom-up way, still with only O(1) messages sent over each link.We consider a message passing model of computation [AW04], based on a communication network whose topology is given by a graph G = (V, E) of diameter \u2206, where E denotes the set of bidirectional communication links between nodes. From now on, we restrict our attention to finite connected graphs.Unless specified explicitly, we assume in this paper that the distributed system is asynchronous and has no failure. The nodes have a unique identifier",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "AN APPLICATION OF THE FEFERMAN-VAUGHT THEOREM TO AUTOMATA AND LOGICS FOR WORDS OVER AN INFINITE ALPHABET We show that a special case of the Feferman-Vaught composition theorem gives rise to a natural notion of automata for finite words over an infinite alphabet, with good closure and decidability properties, as well as several logical characterizations. We also consider a slight extension of the Feferman-Vaught formalism which allows to express more relations between component values (such as equality), and prove related decidability results. From this result we get new classes of decidable logics for words over an infinite alphabet.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Communications Law The ABC of Digital Business Ecosystems Subject: Legal foundations for network-based economies The European Commission has the power to inspire, initiate and sponsor huge transnational projects to an extent impossible for most other entities. These projects can address universal themes and develop well-being models that are valuable across a diversity of societies and economies.It is a universal fact that SMEs in all countries provide a substantial proportion of total employment, and conduct much of a nation's innovative activity. Yet these smaller companies struggle in global markets on afar from level playing field, where large companies have distinct advantages.To redress this imbalance the Commission saw it as a priority to improve the trading capability of the Small and Medium-sized Enterprises (SMEs), and perceived digital platforms as the modern means to this end. They considered that the best operational model fora vibrant Web2.0-based Internet services industry would be by analogy to well-performing biological ecosystems.Open Source Software is adopted in the DBE 1 /OPAALS 2 projects as the best support for sustainability of such complex electronic webs, since it minimises interoperability problems, enables code access for cheaper in-house modification or development of systems, and reduces both capital and operating expenditure. al.respond to the new conditions 3 . Less healthy scenarios are where a dominant effectively produces a monoculture. If such a species is then taken out of the eco-equation, and there are no tail species to provide resilience to changed conditions, the ecosystem may fail.For industrial communities, if one accepts the analogy to biological ecosystems, the presence of super-dominants 4 can be regarded as a configuration that impacts all other 'business species'. Digital Business Ecosystems (DBE) and its sister project OPAALS are major initiatives which, together with their satellite work groups, focus on the health of the European SMEs -the 'business tail' -in the new knowledge-based economy that we are experiencing growing up around us.A key observation is that when living in a marketplace alongside dominant firms, self-organisation by other firms is more difficult to achieve. The current Internet use models fail to support local autonomy for the firm [Dini 2008], and the so-called sharing/cooperation zones on the internet are homogenised in the direction of the interests of the dominant participants. Thus, conformity rather than diversity results 5 . This effectively centralised model further promotes the possibility of single points of failure 6 , and more importantly single points of control from centralised servers. So an ecosystem-oriented approach necessitates a fully distributed peer-to-peer 7 networking model [Briscoe 2007b], which would avoid single points of control and so be immune to single points of failure, as shown in Figure 1.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Searching Beyond MobileNetV3 The evolution of MobileNets has laid a solid foundation for neural network applications on mobile end. With the latest MobileNetV3, neural architecture search again claimed its supremacy in network design. Unfortunately, till today all mobile methods mainly focus on CPU latencies instead of GPU, the latter, however, is much preferred in practice for it has faster speed, lower overhead and less interference. Bearing the target hardware in mind, we propose the first Mobile GPU-Aware (MoGA) neural architecture search in order to be precisely tailored for real-world applications. Further, the ultimate objective to devise a mobile network lies in achieving better performance by maximizing the utilization of bounded resources. Urging higher capability while restraining time consumption is not reconcilable. We alleviate the tension by weighted evolution techniques. Moreover, we encourage increasing the number of parameters for higher representational power. With 200\u00d7 fewer GPU days than MnasNet, we obtain a series of models that outperform MobileNetV3 under the similar latency constraints, i.e., MoGA-A achieves 75.9% top-1 accuracy on ImageNet, MoGA-B meets 75.5% which costs only 0.5 ms more on mobile GPU. MoGA-C best attests GPU-awareness by reaching 75.3% and being slower on CPU but faster on GPU. The models and test code are made publicly here 12 .  Table 2: The architecture of MoGA-A. Note t, c, s refer to expansion rate, output channel size and stride respectively. SE for squeeze-and-excitation, NL for non-linearity. k for the number of categories. Input Ops s t Table 3: Comparison of mobile models on ImageNet. : Our reimplementation. Numbers within the parentheses are reported by its authors, same for below. \u2020 : Based on its published code. \u2021 : Samsung Galaxy S8. * : Samsung Note8. - Latc ( % ) Top - 5 ( M ) Top - 1 ( ms ) Table 3. Apart from the mobile framework we use, CPUs and GPUs differ on inherent microarchitectures, which puts hardware-specific requirements a must for the design of neu- ral architectures. 0 500 FBNet 1000 2000",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of params\", \"Score\": \"5.1M\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"75.9%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"92.8%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Accuracy\", \"Score\": \"75.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"MACs\", \"Score\": \"325M\"}}, { \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Params\", \"Score\": \"4.0M\"}}, { \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top-1 Error Rate\", \"Score\": \"24.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Accuracy\", \"Score\": \"75.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"MACs\", \"Score\": \"238M\"}}, { \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Params\", \"Score\": \"5.5M\"}}, { \"LEADERBOARD\": { \"Task\": \"Neu"
    },
    {
        "Context": "UPSNet: A Unified Panoptic Segmentation Network In this paper, we propose a unified panoptic segmentation network (UPSNet) for tackling the newly proposed panoptic segmentation task. On top of a single backbone residual network, we first design a deformable convolution based semantic segmentation head and a Mask R-CNN style instance segmentation head which solve these two subtasks simultaneously. More importantly, we introduce a parameter-free panoptic head which solves the panoptic segmentation via pixel-wise classification. It first leverages the logits from the previous two heads and then innovatively expands the representation for enabling prediction of an extra unknown class which helps better resolve the conflicts between semantic and instance segmentation. Additionally, it handles the challenge caused by the varying number of instances and permits back propagation to the bottom modules in an end-to-end manner. Extensive experimental results on Cityscapes, COCO and our internal dataset demonstrate that our UPSNet achieves stateof-the-art performance with much faster inference. In this section, we present the experimental results on COCO, Cityscapes and our internal dataset Our Dataset We also use an internal dataset which is similar to Cityscapes and consists of 10235 training, 1139 validation and 1186 test images of ego-centric driving scenarios Our dataset consists of 10 and 17 classes for thing (e.g., car, bus) and stuff (e.g., building, road) respectively Experimental Setup For all datasets, we report results on the validation set To evaluate the performance, we adopt panoptic quality (PQ), recognition quality (RQ) and semantic quality (SQ) as the metrics We set the learning rate and weight decay as 0.02 and 0.0001 for all datasets For our dataset, we train for 36K iterations and apply the same learning rate decay at 24K and 32K iterations For PSPNet, we use \"poly' learning rate schedule as in and train 220K, 18K and 76K on COCO, Cityscapes and our dataset with Table 1: Panoptic segmentation results on COCO. Super- scripts Th and St stand for thing and stuff. '-' means inap- plicable. mIoU PQ - SQ RQ AP Table 2: Panoptic segmentation results on MS-COCO 2018 test-dev. The top 3 rows contain results of top 3 models taken from the official leadboard. PQ SQ RQ Table 3: Panoptic segmentation results on Cityscapes. '- COCO' means the model is pretrained on COCO. '-101' means the model uses ResNet-101 as the backbone. Unless specified, all models use ResNet-50 as the backbone and are pretrained on ImageNet. mIoU PQ - SQ RQ AP PQ Th Table 4: Panoptic segmentation results on our dataset. mIoU PQ SQ RQ AP Table 6: Ablation study on COCO dataset. 'Pano.', 'Loss Bal.', 'Unk.' and 'ICA' stand for training with panoptic loss, loss balance, unknown prediction and instance class assign- ment respectively. PQ St Seg . Th GT",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"PQ\", \"Score\": \"46.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"PQst\", \"Score\": \"36.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"PQth\", \"Score\": \"53.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Indian Driving Dataset\", \"Metric\": \"PQ\", \"Score\": \"47.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"AP\", \"Score\": \"39.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"PQ\", \"Score\": \"61.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"PQst\", \"Score\": \"64.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"PQth\", \"Score\": \"57.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"mIoU\", \"Score\": \"79.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"AP\", \"Score\": \"37.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"PQ\", \"Score\": \"60.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"PQst\", \"Score\": \"63.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"PQth\", \"Score\": \"57.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"mIoU\", \"Score\": \"77.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"AP\", \"Score\": \"33.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"PQ\", \"Score\": \"59.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"PQst\", \"Score\": \"62.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"PQth\", \"Score\": \"54.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"mIoU\", \"Score\": \"75.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"KITTI Panoptic Segmentation\", \"Metric\": \"PQ\", \"Score\": \"39.9\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"PQ\", \"Score\": \"48.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"PQst\", \"Score\": \"37.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Panoptic Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"PQth\", \"Score\": \"55.5\"}} ]"
    },
    {
        "Context": "SESAME: Semantic Editing of Scenes by Adding, Manipulating or Erasing Objects input labe Fig. 1. We assess SESAME on three tasks (a) image editing with free form semantic drawings (first row) (b) semantic layout driven semantic editing (second row) (c) layout to image generation with SESAME discriminator (third row)Abstract. Recent advances in image generation gave rise to powerful tools for semantic image editing. However, existing approaches can either operate on a single image or require an abundance of additional information. They are not capable of handling the complete set of editing operations, that is addition, manipulation or removal of semantic concepts. To address these limitations, we propose SESAME, a novel generator-discriminator pair for Semantic Editing of Scenes by Adding, Manipulating or Erasing objects. In our setup, the user provides the semantic labels of the areas to be edited and the generator synthesizes the corresponding pixels. In contrast to previous methods that employ a discriminator that trivially concatenates semantics and image as an input, the SESAME discriminator is composed of two input streams that arXiv:2004.04977v2 [cs.CV] 8 Oct 2020 2 E. Ntavelis et al.independently process the image and its semantics, using the latter to manipulate the results of the former. We evaluate our model on a diverse set of datasets and report state-of-the-art performance on two tasks: (a) image manipulation and (b) image generation conditioned on semantic labels. require the whole semantics while we use only the semantics of the box -In order to quantify the performance of our network we follow the data preparation and evaluation steps of Hong et al ., for generating and removing objects based on a given semantic layout Datasets The dataset contains 3,000 street-level view images of 50 different cities in Europe for the training set and 500 images for the validation set The whole dataset is used for the generation task For manipulation, following Hong et al ., we experiment on a subset of the ADE20K dataset comprised of bedroom scenes In total we consider 49 semantic categories for training and evaluation -Flickr-Landscapes Datasets Similar to SPADE, we first scrapped 200,000 images from flickr with only landscape constraint After post-processing, our curated dataset consists of 7367 training and 500 validation images with their corresponding segmentation for 17 different semantic classes The user Table 1. Addition Results for Cityscapes and ADE20k dataset. We ablate on the Generator and Discriminator architecture as well as the semantic availability. For the SSIM, accuracy and mIoU higher is better, while for FID, lower is better. FID ADE20k Labels SSIM accu mIoU SSIM accu mIoU Cityscapes Table 2. Removal results for Cityscapes and ADE20k datasets. For the SSIM, accuracy and mIoU higher is better, while for FID, lower is better. ADE20k accu\u2191 Cityscapes SSIM\u2191 accu\u2191 mIoU\u2191 FID\u2193 SSIM\u2191 mIoU\u2191 FID\u2193 Table 3. Comparison in number of parameters Parameters in millions while for FID , lower is better ADE20k Discriminator mIoU accu FID mIoU accu FID Generator Discriminator Cityscapes Table 3 . Comparison in number of parameters Table 5. User Study Results: Which image is the most photorealistic? The first study invited the users to choose between Hong et al . User Study II Discriminator : SESAME PatchGAN User Study",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"Cityscapes Labels-to-Photo\", \"Metric\": \"FID\", \"Score\": \"54.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"Cityscapes Labels-to-Photo\", \"Metric\": \"Per-pixel Accuracy\", \"Score\": \"82.5%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"Cityscapes Labels-to-Photo\", \"Metric\": \"mIoU\", \"Score\": \"66\"}}, {\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"ADE20K Labels-to-Photos\", \"Metric\": \"Accuracy\", \"Score\": \"85.5%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"ADE20K Labels-to-Photos\", \"Metric\": \"FID\", \"Score\": \"31.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"ADE20K Labels-to-Photos\", \"Metric\": \"mIoU\", \"Score\": \"49\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"Binarized MNIST\", \"Metric\": \"nats\", \"Score\": \"8.250\"}}, { \"LEADERBOARD\": { \"Task\": \"Density Estimation\", \"Dataset\": \"UCF101\", \"Metric\": \"Log-ll-auto-100k\", \"Score\": \"11.94\"}}, { \"LEADERBOARD\": { \"Task\": \"Density Estimation\", \"Dataset\": \"UCF101\", \"Metric\": \"Log-ll-auto-100k\", \"Score\": \"8.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Density Estimation\", \"Dataset\": \"UCF101\", \"Metric\": \"Log-ll-auto-100k\", \"Score\": \"93.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Density Estimation\", \"Dataset\": \"UCF101\", \"Metric\":\"mean Dice\", \"Score\": \"0.80\"}}, { \"LEADERBOARD\": { \"Task\": \"Density Estimation\", \"Dataset\": \"NYU Depth v2\", \"Metric\": \"MAP\", \"Score\": \"0.76\"}}, { \"LEADERBOARD\": { \"Task\": \"Density Estimation\", \"Dataset\": \"ScanNetV2\", 'Metric"
    },
    {
        "Context": "Note on Maximal Bisection above Tight Lower Bound Ina graph G = (V, E), a bisection (X, Y ) is a partition of V into sets X and Y such that |X| \u2264 |Y | \u2264 |X|+1. The size of (X, Y ) is the number of edges between X and Y . In the Max Bisection problem we are given a graph G = (V, E) and are required to find a bisection of maximum size. It is not hard to see that \u2308|E|/2\u2309 is a tight lower bound on the maximum size of a bisection of G.We study parameterized complexity of the following parameterized problem called Max Bisection above Tight Lower Bound (Max-Bisec-ATLB): decide whether a graph G = (V, E) has a bisection of size at least \u2308|E|/2\u2309 + k, where k is the parameter. We show that this parameterized problem has a kernel with O(k 2 ) vertices and O(k 3 ) edges, i.e., every instance of Max-Bisec-ATLB is equivalent to an instance of Max-Bisec-ATLB on a graph with at most O(k 2 ) vertices and O(k 3 ) edges.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Influence of Memory Hierarchies on Predictability for Time Constrained Embedded Software*",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Multi-granularity Generator for Temporal Action Proposal Temporal action proposal generation is an important task, aiming to localize the video segments containing human actions in an untrimmed video. In this paper, we propose a multi-granularity generator (MGG) to perform the temporal action proposal from different granularity perspectives, relying on the video visual features equipped with the position embedding information. First, we propose to use a bilinear matching model to exploit the rich local information within the video sequence. Afterwards, two components, namely segment proposal producer (SPP) and frame actionness producer (FAP), are combined to perform the task of temporal action proposal at two distinct granularities. SPP considers the whole video in the form of feature pyramid and generates segment proposals from one coarse perspective, while FAP carries out a finer actionness evaluation for each video frame. Our proposed MGG can be trained in an end-to-end fashion. By temporally adjusting the segment proposals with fine-grained frame actionness information, MGG achieves the superior performance over state-of-the-art methods on the public THUMOS-14 and ActivityNet-1.3 datasets. Moreover, we employ existing action classifiers to perform the classification of the proposals generated by MGG, leading to significant improvements compared against the competing methods for the video detection task. The whole dataset consists of 19,994 videos with 200 classes annotated, with 50% for training, 25% for validation, and the rest 25% for testing., SST, TURN, BSN, TAG, and CTAP on THUMOS-14 in terms of AR@AN Table 1: Performance comparisons with DAPs [10], SCNN- prop - Table 2: Performance comparisons with TCN [9], MSRA [46], Prop-SSAD [25], CTAP [13], and BSN [26] on the validation and testing splits of ActivityNet-1.3. MSRA Prop - SSAD BSN MGG TCN CTAP Table 3: Ablation studies on the validation set of ActivityNet-1.3 in terms of AUC and AR@AN. AUC ( val ) @80 @50 @100 @30 Table 4: Recall rates of MGG-U and MGG on generated proposals of different temporal extents on the validation set of ActivityNet-1.3, where AN and tIoU thresholds are set to 100 and 0.75, respectively. 0 - 5s 10 - 15s 40 - 45s 5 - 10s 35 - 40s 15 - 20s 25 - 30s Table 5: Performance comparisons of the two-stage TBA on the validation set of ActivityNet-1.3 in both end-to-end training and stagewise training manners. Stage II Stagewise End - to - end Table",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"THUMOS\u201914\", \"Metric\": \"mAP@0.3\", \"Score\": \"53.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"THUMOS\u201914\", \"Metric\": \"mAP@0.4\", \"Score\": \"46.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"THUMOS\u201914\", \"Metric\": \"mAP@0.5\", \"Score\": \"37.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Temporal Action Proposal Generation\", \"Dataset\": \"ActivityNet-1.3\", \"Metric\": \"AR@100\", \"Score\": \"74.54\"}}, {\"LEADERBOARD\": {\"Task\": \"Temporal Action Proposal Generation\", \"Dataset\": \"ActivityNet-1.3\", \"Metric\": \"AUC (val)\", \"Score\": \"66.43\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Temporal Action Proposal Generation\", \"Dataset\": \"ActivityNet-1.3\", \"Metric\": \"Average Accuracy\", \"Score\": \"73.17\"}}, { \"LEADERBOARD\": { \"Task\": \"Temporal Action Proposal Generation\", \"Dataset\": \"ActivityNet-1.3\", \"Metric\": \"Average Accuracy\", \"Score\": \"65.72\"}} ]"
    },
    {
        "Context": "AtlasNet: A Papier-M\u00e2ch\u00e9 Approach to Learning 3D Surface Generation . Given input as either a 2D image or a 3D point cloud (a), we automatically generate a corresponding 3D mesh (b) and its atlas parameterization (c). We can use the recovered mesh and atlas to apply texture to the output shape (d) as well as 3D print the results (e).We introduce a method for learning to generate the surface of 3D shapes. Our approach represents a 3D shape as a collection of parametric surface elements and, in contrast to methods generating voxel grids or point clouds, naturally infers a surface representation of the shape. Beyond its novelty, our new shape generation framework, AtlasNet, comes with significant advantages, such as improved precision and generalization capabilities, and the possibility to generate a shape of arbitrary resolution without memory issues. We demonstrate these benefits and compare to strong baselines on the ShapeNet benchmark for two applications: (i) autoencoding shapes, and (ii) single-view reconstruction from a still image. We also provide results showing its potential for other applications, such as morphing, parametrization, super-resolution, matching, and co-segmentation.  Table 1. 3D reconstruction. Comparison of our approach against a point-generation baseline (\"CD\" -Chamfer distance, multiplied by 10 3 , computed on 2500 points; \"Metro\" values are multiplied by 10). Note that our approach can be directly evaluated by Metro while the baseline requires performing PSR - CD Metro Table 2. Generalization across object categories. Comparison of our approach with varying number of patches against the point- generating baseline to generate a specific category when training on all other ShapeNet categories. Chamfer distance is reported, multi- plied by 10 3 , computed on 2500 points. Notice that our approach with 125 patches out-performs all baselines when generalizing to the new category. For reference, we also show performance when we train over all categories. Points Ours 125 patches baseline 1 patch Table 3. Single-View Reconstruction (per category). The mean is taken category-wise. The Chamfer Distance reported is computed on 1024 points, after",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Point Cloud Completion\", \"Dataset\": \"Completion3D\", \"Metric\": \"Chamfer Distance\", \"Score\": \"17.77\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Shape Reconstruction\", \"Dataset\": \"Pix3D\", \"Metric\": \"CD\", \"Score\": \"0.125\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Shape Reconstruction\", \"Dataset\": \"Pix3D\", \"Metric\": \"EMD\", \"Score\": \"0.128\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Shape Reconstruction\", \"Dataset\": \"Pix3D\", \"Metric\": \"IoU\", \"Score\": \"N/A\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Reconstruction\", \"Dataset\": \"Data3DR2N2\", \"Metric\": \"3DIoU\", \"Score\": \"0.575\"}} ]"
    },
    {
        "Context": "Succinct Geometric Indexes Supporting Point Location Queries We propose to design data structures called succinct geometric indexes of negligible space (more precisely, o(n) bits) that, by taking advantage of then points in the data set permuted and stored elsewhere as a sequence, to support geometric queries in optimal time. Our first and main result is a succinct geometric index that can answer point location queries, a fundamental problem in computational geometry, on planar triangulations in O(lg n) time 3 . We also design three variants of this index. The first supports point location using lg n + 2 \u221a lg n + O(lg 1/4 n) point-line comparisons.The second supports point location in o(lg n) time when the coordinates are integers bounded by U . The last variant can answer point location in O(H + 1) expected time, where H is the entropy of the query distribution. These results match the query efficiency of previous point location structures that use O(n) words or O(n lg n) bits, while saving drastic amounts of space. We then generalize our succinct geometric index to planar subdivisions, and design indexes for other types of queries. Finally, we apply our techniques to design the first implicit data structures that support point location in O(lg 2 n) time.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Decoupled Spatial-Temporal Attention Network for Skeleton-Based Action Recognition Dynamic skeletal data, represented as the 2D/3D coordinates of human joints, has been widely studied for human action recognition due to its high-level semantic information and environmental robustness. However, previous methods heavily rely on designing handcrafted traversal rules or graph topologies to draw dependencies between the joints, which are limited in performance and generalizability. In this work, we present a novel decoupled spatial-temporal attention network (DSTA-Net) for skeleton-based action recognition. It involves solely the attention blocks, allowing for modeling spatial-temporal dependencies between joints without the requirement of knowing their positions or mutual connections. Specifically, to meet the specific requirements of the skeletal data, three techniques are proposed for building attention blocks, namely, spatial-temporal attention decoupling, decoupled position encoding and spatial global regularization. Besides, from the data aspect, we introduce a skeletal data decoupling technique to emphasize the specific characteristics of space/time and different motion scales, resulting in a more comprehensive understanding of the human actions. To test the effectiveness of the proposed method, extensive experiments are conducted on four challenging datasets for skeleton-based gesture and action recognition, namely, SHREC, DHG, NTU-60 and NTU-120, where DSTA-Net achieves state-of-the-art performance on all of them. To verify the generalization of the model, we use two datasets for hand gesture recognition (DHG and SHREC) and two datasets for human action recognition (NTU-60 and NTU-120) Then, we evaluate our model on all four datasets to compare with the state-of-the-art methods DHG: DHG dataset contains 2800 video sequences of 14 hand gestures performed 5 times by 20 subjects It uses the leave-one-subject-out cross-validation strategy for evaluation SHREC: SHREC dataset contains 2800 gesture sequences performed 1 and 10 times by 28 participants in two ways like the DHG dataset This dataset is used for the competition of SHREC'17 in conjunction with the Euro-graphics 3DOR'2017 Workshop NTU-60: NTU-60 is a most widely used in-door-captured action recognition dataset, which contains 56,000 action clips in 60 action classes This dataset provides 25 joints for each subject in the skeleton sequences Table 1. Ablation studies for architectures of the model on the SHREC dataset. ST-ATT-c denotes the spatial temporal attention networks with attention type c introduced in Accuracy Table 2. Ablation studies for feature fusion on the SHREC dataset. Spatial-temporal denotes the raw data, i.e., the joint coordinates. Other types of features are introduced in Sec. 3.6. Accuracy Table 3. Recognition accuracy comparison of our method and state-of-the-art methods on SHREC dataset and DHG dataset. DHG 14 gestures Year SHREC 28 gestures Table 4. Recognition accuracy comparison of our method and state-of-the-art methods on NTU-60 dataset. CS and CV denote the cross-subject and cross-view benchmarks, respectively. CS ( % ) CV ( % ) Table 5. Recognition accuracy comparison of our method and state-of-the-art methods on NTU-120 dataset. CS and CE denote the cross-subject and cross-setup benchmarks, respectively. CE ( % ) CS ( % )",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D 120\", \"Metric\": \"Accuracy (Cross-Setup)\", \"Score\": \"89.0 %\"}}, {\"LEADERBOARD\": {\"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D 120\", \"Metric\": \"Accuracy (Cross-Subject)\", \"Score\": \"86.6%\"}}, {\"LEADERBOARD\": {\"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"Accuracy (CS)\", \"Score\": \"91.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"Accuracy (CV)\", \"Score\": \"96.4\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"Accuracy (CS)\", \"Score\": \"92.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Skeleton Based Action Recognition\", \"Dataset\": \"NTU RGB+D\", \"Metric\": \"Accuracy (CV)\", \"Score\": \"96.1\"}} ]"
    },
    {
        "Context": "LAMBDANETWORKS: MODELING LONG-RANGE INTERACTIONS WITHOUT ATTENTION We present lambda layers -an alternative framework to self-attention -for capturing long-range interactions between an input and structured contextual information (e.g. a pixel surrounded by other pixels). Lambda layers capture such interactions by transforming available contexts into linear functions, termed lambdas, and applying these linear functions to each input separately. Similar to linear attention, lambda layers bypass expensive attention maps, but in contrast, they model both content and position-based interactions which enables their application to large structured inputs such as images. The resulting neural network architectures, LambdaNetworks, significantly outperform their convolutional and attentional counterparts on ImageNet classification, COCO object detection and COCO instance segmentation, while being more computationally efficient. Additionally, we design LambdaResNets, a family of hybrid architectures across different scales, that considerably improves the speed-accuracy tradeoff of image classification models. LambdaResNets reach excellent accuracies on ImageNet while being 3.2 -4.4x faster than the popular EfficientNets on modern machine learning accelerators. When training with an additional 130M pseudo-labeled images, LambdaResNets achieve up to a 9.5x speed-up over the corresponding Efficient-Net checkpoints 1 . In subsequent experiments, we evaluate lambda layers on standard computer vision benchmarks: ImageNet classification (Deng et al., 2009), COCO object detection and instance segmentation Table 3: Comparison of the lambda layer and attention mechanisms on ImageNet classification with a ResNet50 architecture. The lambda layer strongly outperforms attention alternatives at a fraction of the parameter cost. All models are trained in mostly similar setups (see Appendix E.2) and we include the reported improvements compared to the convolution baseline in parentheses. See Appendix B.4 for a description of the |u| hyperparameter. \u2020 Our implementation. top - 1 Params ( M ) Table 4: The lambda layer reaches higher ImageNet accuracies while being faster and more memory-efficient than self-attention alternatives. Memory is reported assuming full precision for a batch of 128 inputs using default hyperparameters. The memory cost for storing the lambdas matches the memory cost of activations in the rest of the network and is therefore ignored. b: batch size, h: number of heads/queries, n: input length, m: context length, k: query/key depth, l: number of layers.",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of params\", \"Score\": \"42M\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"84.3%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of params\", \"Score\": \"35M\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"84.0%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of params\", \"Score\": \"78.9M\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"80.72%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"93.52%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of params\", \"Score\": \"83.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"83.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"83.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Number of params\", \"Score\": \"84.9M\"}}]"
    },
    {
        "Context": "ACE: Adaptive Confusion Energy for Natural World Data Distribution With the development of deep learning, standard classification problems have achieved good results. However, conventional classification problems are often too idealistic. Most data in the natural world usually have imbalanced distribution and fine-grained characteristics. Recently, many state-of-the-art approaches tend to focus on one or another separately, but rarely on both. In this paper, we introduce a novel and adaptive batch-wise regularization based on the proposed Adaptive Confusion Energy (ACE) to flexibly address the nature world distribution, which usually involves fine-grained and long-tailed properties at the same time. ACE increases the difficulty of the training process and further alleviates the overfitting problem. Through the datasets with the technical issue in fine-grained (CUB, CAR, AIR) and long-tailed (ImageNet-LT), or comprehensive issues (CUB-LT, iNaturalist), the result shows that the ACE is not only competitive to some state-ofthe-art on performance but also demonstrates the effectiveness of training. We conduct extensive experiments to evaluate our approach on three balanced benchmark FGVC datasets, imbalanced datasets, and the natural world distribution dataset We first evaluate the effectiveness of the proposed approach on three standard fine-grained visual classification datasets, namely, CUB-200-2011, Stanford Cars, and FGVC-Aircraft The class distribution of the three datasets is nearly balanced, which can be used to measure the proposed method's performance only in the fine-grained scenario with the adaptive matrix\u00c2 approximating identity matrix Compared with other datasets for the large-scale visual classification task, these three FGVC datasets have fewer training data for each category Next, we go through the experiments on the imbalanced datasets, ImageNet-LT The former is a long-tailed distribution with a low fine-grained factor, confirming whether the proposed approach will adjust on the purely imbalanced dataset The latter is a fine-grained dataset that also has a long-tailed property Finally, we then focus on the natural world Table 1. Head-to-head comparisons of the confusion energy scenarios on the standard FGVC datasets CUB-200-2011 (CUB), Stanford Cars (Cars), and FGVC-Aircraft (Aircraft). Model CUB ResNet - 50 CAR ResNeXt - 50 AIR DenseNet - 161 ResNeXt - 101 Table 2. Compare the results with the typical state-of-the-art. The CNN backbone is ResNet-50. CUB CAR AIR Table 3. Following the approach (Kang et al., 2020) on ImageNet- LT, the proposed ACE gains a significant improvement. Total Median Few Many Table 4. The comparison with some recent stat-of-the-art works Total Median Few Many on the iNaturalist 2018 .",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"FGVC Aircraft\", \"Metric\": \"Accuracy\", \"Score\": \"93.5%\"}}, {\"LEADERBOARD\": {\"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"Stanford Cars\", \"Metric\": \"Accuracy\", \"Score\": \"94.8%\"}}, {\"LEADERBOARD\": {\"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"CUB-200-2011\", \"Metric\": \"Accuracy\", \"Score\": \"89.2%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"CUB-200-2011\", \"Metric\": \"Accuracy\", \"Score\": \"89.6%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"iNaturalist\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"75.3%\"}}, { \"LEADERBOARD\": { \"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"CUB-200-2011\", \"Metric\": \"Accuracy\", \"Score\": \"96%\"}}, { \"LEADERBOARD\": { \"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"FGVC Aircraft\", \"Metric\": \"Accuracy\", \"Score\": \"94.5%\"}}, { \"LEADERBOARD\": { \"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"Stanford Cars\", \"Metric\": \"Accuracy\", \"Score\": \"95.63%\"}}, { \"LEADERBOARD\": { \"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"CUB-200-2011\", \"Metric\": \"Accuracy\", \"Score\": \"88.1%\"}}, { \"LEADERBOARD\": { \"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"FGVC Aircraft\", \"Metric\": "
    },
    {
        "Context": "CommunityGAN: Community Detection with Generative Adversarial Nets Community detection refers to the task of discovering groups of vertices sharing similar properties or functions so as to understand the network data. With the recent development of deep learning, graph representation learning techniques are also utilized for community detection. However, the communities can only be inferred by applying clustering algorithms based on learned vertex embeddings. These general cluster algorithms like K-means and Gaussian Mixture Model cannot output much overlapped communities, which have been proved to be very common in many real-world networks. In this paper, we propose CommunityGAN, a novel community detection framework that jointly solves overlapping community detection and graph representation learning. First, unlike the embedding of conventional graph representation learning algorithms where the vector entry values have no specific meanings, the embedding of CommunityGAN indicates the membership strength of vertices to communities. Second, a specifically designed Generative Adversarial Net (GAN) is adopted to optimize such embedding. Through the minimax competition between the motif-level generator and discriminator, both of them can alternatively and iteratively boost their performance and finally output a better community structure. Extensive experiments on synthetic data and real-world tasks demonstrate that CommunityGAN achieves substantial community detection performance gains over the state-of-the-art methods. In this section, in order to evaluate the effectiveness of Commu-nityGAN, we conduct a series of experiments based on synthetic datasets The The availability of ground-truth communities allows us to quantitatively evaluate the performance of community detection algorithms Table 2: The occurrence probability of cliques for vertices sampled from all vertices or from one community. R: from all vertices. C: from one community. Dataset 3 - Clique C 2 - Clique 4 - Clique Table 4: Synthetic Graphs statistics. V : number of vertices, E: number of edges, C: number of communities, A: average number of community memberships per vertex, P: percent- age of communities that have overlapping with others 100% age of communities that have overlapping with others A Table 4 : Synthetic Graphs statistics . V : number of vertices , Table 4. We can see that the percentages of overlapped communities (denoted by P) are all 100%, indicating that all the communities overlap more or less with others. Besides, with the decrease of \u03b2 1 and \u03b2 2 , the average number of community memberships per vertex (denoted by A) increases, which means the density of",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Community Detection\", \"Dataset\": \"Amazon\", \"Metric\": \"F1-score\", \"Score\": \"0.091\"}}, {\"LEADERBOARD\": {\"Task\": \"Community Detection\", \"Dataset\": \"DBLP\", \"Metric\": \"F1-Score\", \"Score\": \"0.153\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Community Detection\", \"Dataset\": \"Commons\", \"Metric\": \"Evg F1\", \"Score\": \"Commons\"}} ]"
    },
    {
        "Context": "Adaptive Dynamics of Realistic Small-World Networks Continuing in the steps of Jon Kleinberg's and others celebrated work on decentralized search in small-world networks, we conduct an experimental analysis of a dynamic algorithm that produces small-world networks. We find that the algorithm adapts robustly to a wide variety of situations in realistic geographic networks with synthetic test data and with real world data, even when vertices are uneven and non-homogeneously distributed.We investigate the same algorithm in the case where some vertices are more popular destinations for searches than others, for example obeying power-laws. We find that the algorithm adapts and adjusts the networks according to the distributions, leading to improved performance. The ability of the dynamic process to adapt and create small worlds in such diverse settings suggests a possible mechanism by which such networks appear in nature.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Cross-lingual Entity Alignment with Incidental Supervision Much research effort has been put to multilingual knowledge graph (KG) embedding methods to address the entity alignment task, which seeks to match entities in different languagespecific KGs that refer to the same real-world object. Such methods are often hindered by the insufficiency of seed alignment provided between KGs. Therefore, we propose an incidentally supervised model, JEANS , which jointly represents multilingual KGs and text corpora in a shared embedding scheme, and seeks to improve entity alignment with incidental supervision signals from text. JEANS first deploys an entity grounding process to combine each KG with the monolingual text corpus. Then, two learning processes are conducted: (i) an embedding learning process to encode the KG and text of each language in one embedding space, and (ii) a selflearning based alignment learning process to iteratively induce the matching of entities and that of lexemes between embeddings. Experiments on benchmark datasets show that JEANS leads to promising improvement on entity alignment with incidental supervision, and significantly outperforms state-of-the-art methods that solely rely on internal information of KGs. 1 In this section, we evaluate JEANS on two benchmark datasets for cross-lingual entity alignment, and compare against a wide selection of recent baseline methods Table 1: Entity alignment results. Baselines are separated in accord with the three groups described in Section 4.1. \u2020 indicates results obtained from (Sun et al., 2020a), and \u2021 indicates those from (Pei et al., 2019b). Results of KECG, GCN-JE, MMR, HMAN, KDCoE and NAEA are from original papers. Hyphens denote not available. MRR were not reported by GCN-JE, MMR and HMAN. Top results (incl. w/ and w/o seed lexicon) are boldfaced. Note that results by GCN-JE, GMN and HMAN are reported only for the versions where the extra cross-lingual alignment information (such as machine translation) is removed, so as to conduct fair comparison with all the rest models that are trained using only the alignment labels in the benchmark training sets. \u2212 \u2212 Table 2: The ablation study results for components of JEANS based on DBP15k En\u2212F r and DBP15k En\u2212Ja . Note that the additional seed lexicon is not",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Entity Alignment\", \"Dataset\": \"DBP15k zh-en\", \"Metric\": \"Hits@1\", \"Score\": \"0.719\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Entity Alignment\", \"Dataset\": \"DBP15k zh-en\", \"Metric\": \"Hits@1\", \"Score\": \"83.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Entity Alignment\", \"Dataset\": \"DBP15k zh-en\", \"Metric\": \"Hits@1\", \"Score\": \"83.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Entity Alignment\", \"Dataset\": \"dbp15k ja-en\", \"Metric\": \"Hits@1\", \"Score\": \"80.6\"}}, { \"LEADERBOARD\": { \"Task\": \"Entity Alignment\", \"Dataset\": \"dbp15k fr-en\", \"Metric\": \"Hits@1\", \"Score\": \"8370\"}} ]"
    },
    {
        "Context": "Unsupervised Learning using Pretrained CNN and Associative Memory Bank Deep Convolutional features extracted from a comprehensive labeled dataset, contain substantial representations which could be effectively used in anew domain. Despite the fact that generic features achieved good results in many visual tasks, fine-tuning is required for pretrained deep CNN models to be more effective and provide state-of-the-art performance. Fine tuning using the backpropagation algorithm in a supervised setting, is a time and resource consuming process. In this paper, we present anew architecture and an approach for unsupervised object recognition that addresses the above mentioned problem with fine tuning associated with pretrained CNN-based supervised deep learning approaches while allowing automated feature extraction. Unlike existing works, our approach is applicable to general object recognition tasks. It uses a pretrained (on a related domain) CNN model for automated feature extraction pipelined with a Hopfield network based associative memory bank for storing patterns for classification purposes. The use of associative memory bank in our framework allows eliminating backpropagation while providing competitive performance on an unseen dataset. The experiments focus on three popular object classification datasets: Caltech101, Caltech256, and CIFAR-10 We start with the details of these datasets and then we contrast the results obtained using our framework with the state-of-the-art The images in the dataset vary in the degree of shape and scale 1000 images from each class have been randomly selected creating a test dataset of 10000 images and the remaining were used for training We first evaluate the performance of our framework with respect to the number of core patterns The results shown in are based on the Caltech101, Caltech256, and CIFAR-10 datasets from ResNet-50 and VGG-16 models It also showed similar behavior on Caltech256 and CIFAR-10 datasets but relatively more stable after increasing initially The confusion matrix for CIFAR-10 dataset is shown in Since Caltech101 and Caltech256 datasets have a large number of categories compared to the CIFAR-10 dataset, instead of computing the confusion",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"Caltech-256 5-way (1-shot)\", \"Metric\": \"Accuracy\", \"Score\": \"74.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Few-Shot Image Classification\", \"Dataset\": \"CIFAR100 5-way (1-shot)\", \"Metric\": \"Accuracy\", \"Score\": \"89.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Percentage correct\", \"Score\": \"83.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"Caltech-101\", \"Metric\": \"Top-1 Error Rate\", \"Score\": \"91.00%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"Caltech-256, 1024 Labels\", \"Metric\": \"Accuracy\", \"Score\": \"77.40%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"Caltech-101, 202 Labels\", \"Metric\": \"Accuracy\", \"Score\": \"91.00%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"CIFAR-10, 40 Labels\", \"Metric\": \"Percentage error\", \"Score\": \"16.90\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"Caltech-256\", \"Metric\": \"Accuracy\", \"Score\": \"77.40%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"Caltech-101\", \"Metric\": \"Accuracy\", \"Score\": \"91.00%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Percentage correct\", \"Score\": \"74.9\"}} ]"
    },
    {
        "Context": "GRAPH WAVELET NEURAL NETWORK We present graph wavelet neural network (GWNN), a novel graph convolutional neural network (CNN), leveraging graph wavelet transform to address the shortcomings of previous spectral graph CNN methods that depend on graph Fourier transform. Different from graph Fourier transform, graph wavelet transform can be obtained via a fast algorithm without requiring matrix eigendecomposition with high computational cost. Moreover, graph wavelets are sparse and localized in vertex domain, offering high efficiency and good interpretability for graph convolution. The proposed GWNN significantly outperforms previous spectral graph CNNs in the task of graph-based semi-supervised classification on three benchmark datasets: Cora, Citeseer and Pubmed. To evaluate the proposed GWNN, we apply GWNN on semi-supervised node classification, and conduct experiments on three benchmark datasets, namely, Cora, Citeseer and Pubmed In the three citation network datasets, nodes represent documents and edges are citation links Details of these datasets are demonstrated in Following the experimental setup of GCN (Kipf & Welling, 2017), we fetch 20 labeled nodes per class in each dataset to train the model We train a two-layer graph wavelet neural network with 16 hidden units, and prediction accuracy is evaluated on a test set of 1000 labeled samples The partition of datasets is the same as GCN (Kipf & Welling, 2017) with an additional validation set of 500 labeled samples to determine hyper-parameters Table 1. Here, the label rate denotes the proportion of labeled nodes used for training. Following the experimental setup of GCN (Kipf & Welling, 2017), we fetch 20 labeled nodes per class in each dataset to train the model. Table 1 : The Statistics of Datasets Label Rate Table 1: The Statistics of Datasets Table 1 : The Statistics of Datasets Label Rate Table 2: Results of Detaching Feature Transformation from Convolution Citeseer Table 2 : Results of Detaching Feature Transformation from Convolution Cora Pubmed Table 3: Results of Node Classification Citeseer 81 . 7\u00b10 . 5% 78 . 8\u00b10 . 3% Table 3 : Results of Node Classification Cora Pubmed - Table 4: Statistics of wavelet transform and Fourier transform on Cora Transform Matrix Projected Signal Statistical Property 205 , 774 wavelet transform Number of Non - zero Elements Fourier transform Table 4 : Statistics of wavelet transform and Fourier",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"Citeseer\", \"Metric\": \"Accuracy\", \"Score\": \"71.7%\"}}, {\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"Pubmed\", \"Metric\": \"Accuracy\", \"Score\": \"79.1%\"}}, {\"LEADERBOARD\": {\"Task\": \"Node Classification\", \"Dataset\": \"Cora\", \"Metric\": \"Accuracy\", \"Score\": \"81.6%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Citeseer\", \"Metric\": \"Accuracy\", \"Score\": \"71.2%\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Pubmed\", \"Metric\": \"Accuracy\", \"Score\": \"83.5%\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"Cora\", \"Metric\": \"Accuracy\", \"Score\": \"84.3%\"}} ]"
    },
    {
        "Context": "A Novel Bi-hemispheric Discrepancy Model for EEG Emotion Recognition The neuroscience study [1] has revealed the discrepancy of emotion expression between left and right hemispheres of human brain. Inspired by this study, in this paper, we propose a novel bi-hemispheric discrepancy model (BiHDM) to learn the asymmetric differences between two hemispheres for electroencephalograph (EEG) emotion recognition. Concretely, we first employ four directed recurrent neural networks (RNNs) based on two spatial orientations to traverse electrode signals on two separate brain regions, which enables the model to obtain the deep representations of all the EEG electrodes' signals while keeping the intrinsic spatial dependence. Then we design a pairwise subnetwork to capture the discrepancy information between two hemispheres and extract higher-level features for final classification. Besides, in order to reduce the domain shift between training and testing data, we use a domain discriminator that adversarially induces the overall feature learning module to generate emotion-related but domain-invariant feature, which can further promote EEG emotion recognition. We conduct experiments on three public EEG emotional datasets, and the experiments show that the new state-of-the-art results can be achieved. To evaluate the proposed BiHDM model, in this section, we will conduct experiments on three public EEG emotional datasets All the three datasets were collected when the participants satin front of a monitor comfortably and watched emotional video clips The detailed information of these datasets are described as follows: (1) SEED SEED dataset contains 15 subjects, and each subject has three sessions SEED-IV dataset also contains 15 subjects, and each subject has three sessions MPED dataset contains 30 subjects and each subject has one session To evaluate the proposed BiHDM model adequately, we design two kinds of experiments including the subject-dependent and subject-independent ones Thus the sizes d \u00d7 N of the input sample X tare 5 \u00d7 62, 5 \u00d7 62 and 1 \u00d7 62 for these three datasets, respectively The mean accuracy (ACC) and standard deviation (STD) are used as the final evaluation metrics for all the subjects in",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"EEG\", \"Dataset\": \"SEED-IV\", \"Metric\": \"Accuracy\", \"Score\": \"74.35\"}}, {\"LEADERBOARD\": {\"Task\": \"Emotion Recognition\", \"Dataset\": \"MPED\", \"Metric\": \"Accuracy\", \"Score\": \"40.34\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Emotion Recognition\", \"Dataset\": \"SEED-IV\", \"Metric\": \"Accuracy\", \"Score\": \"56.61\"}}, { \"LEADERBOARD\": { \"Task\": \"Emotion Recognition\", \"Dataset\": \"SEED-IV\", \"Metric\": \"Accuracy\", \"Score\": \"79.37\"}} ]"
    },
    {
        "Context": "Automated Synthesis of Assertion Monitors using Visual Specifications properties plays a significant role in assertion-based verification. We present here a methodology to synthesize assertion monitors from visual specifications given in CESC (Clocked Event Sequence Chart). CESC is a visual language designed for specifying system level interactions involving single and multiple clock domains. It has well-defined graphical and textual syntax and formal semantics based on synchronous language paradigm enabling formal analysis of specifications. In this paper we provide an overview of CESC language with few illustrative examples. The algorithm for automated synthesis of assertion monitors from CESC specifications is described. A few examples from standard bus protocols (OCP-IP and AMBA) are presented to demonstrate the application of monitor synthesis algorithm.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "SiamVGG: Visual Tracking using Deeper Siamese Networks Recently, we have seen a rapid development of Deep Neural Network (DNN) based visual tracking solutions. Some trackers combine the DNN-based solutions with Discriminative Correlation Filters (DCF) to extract semantic features and successfully deliver the state-of-the-art tracking accuracy. However, these solutions are highly compute-intensive, which require long processing time, resulting unsecured real-time performance. To deliver both high accuracy and reliable real-time performance, we propose a novel tracker called SiamVGG 1 . It combines a Convolutional Neural Network (CNN) backbone and a cross-correlation operator, and takes advantage of the features from exemplary images for more accurate object tracking. The architecture of SiamVGG is customized from VGG-16, with the parameters shared by both exemplary images and desired input video frames. We demonstrate the proposed SiamVGG on OTB-2013/50/100 and VOT 2015/2016/2017 datasets with the state-ofthe-art accuracy while maintaining a decent real-time performance of 50 FPS running on a GTX 1080Ti. Our design can achieve 2% higher Expected Average Overlap (EAO) compared to the ECO [1] and C-COT [2] in VOT2017 Challenge. In this section, we first start an ablation study of our proposed SiamVGG on OTB100 dataset and then we demonstrate our approach in six different public datasets with the state-of-the-art performance Thus, we introduce the Youtube-BB dataset including more than 100,000 videos annotated once in every 30 frames for more training material By combining these two datasets in a particular ratio, the performance of our design has improved from 0.637 to 0.654 (AUC of success plots for OPE) in OTB-100 Table 1. The backbone architecture of SiamVGG's. All the convolutional layers are integrated with ReLU except the last one working for generating outputs. 'MP' stands for the maxpooling layer. The channel map indicates the number of output and input channels using the format outputchannel \u00d7 inputchannel. 57 \u00d7 57 125 \u00d7 125 Stride Chan . Map 25 \u00d7 25 28 \u00d7 28 22 \u00d7 22 127 \u00d7 127 123 \u00d7 123 23 \u00d7 23 26 \u00d7 26 Activation Size 24 \u00d7 24 21 \u00d7 21 For Exemplar 61 \u00d7 61 59 \u00d7 59 11 \u00d7 11 Kernel Size Table 2. AUC value for recently published real-time trackers using Siamese networks. Datas highlighted in red , blue, and green color stand for the first, second, and third place of each benchmarks, respectively. OTB - 2013 OTB - 50 OTB - 100 - Table 3. We compare our tracker to top 10 trackers",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Visual Object Tracking\", \"Dataset\": \"OTB-2013\", \"Metric\": \"AUC\", \"Score\": \"0.665\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Object Tracking\", \"Dataset\": \"OTB-50\", \"Metric\": \"AUC\", \"Score\": \"0.61\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Object Tracking\", \"Dataset\": \"OTB-2015\", \"Metric\": \"AUC\", \"Score\": \"0.654\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Object Tracking\", \"Dataset\": \"VOT2016\", \"Metric\": \"Expected Average Overlap (EAO)\", \"Score\": \"0.351\"}}, {\"LEADERBOARD\": {\"Task\": \"Visual Object Tracking\", \"Dataset\": \"VOT2017\", \"Metric\": \"Expected Average Overlap (EAO)\", \"Score\": \"0.286\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Visual Object Tracking\", \"Dataset\": \"VOT2017/18\", \"Metric\": \"Expected Average Overlap (EAO)\", \"Score\": \"0.337\"}} ]"
    },
    {
        "Context": "Dilated SpineNet for Semantic Segmentation Scale-permuted networks have shown promising results on object bounding box detection and instance segmentation. Scale permutation and cross-scale fusion of features enable the network to capture multi-scale semantics while preserving spatial resolution. In this work, we evaluate this meta-architecture design on semantic segmentationanother vision task that benefits from high spatial resolution and multi-scale feature fusion at different network stages. By further leveraging dilated convolution operations, we propose SpineNet-Seg, a network discovered by NAS that is searched from the DeepLabv3 system. SpineNet-Seg is designed with a better scale-permuted network topology with customized dilation ratios per block on a semantic segmentation task. SpineNet-Seg models outperform the DeepLabv3/v3+ baselines at all model scales on multiple popular benchmarks in speed and accuracy. In particular, our SpineNet-S143+ model achieves the new state-ofthe-art on the popular Cityscapes benchmark at 83.04% mIoU and attained strong performance on the PASCAL VOC2012 benchmark at 85.56% mIoU. SpineNet-Seg models also show promising results on a challenging Street View segmentation dataset. Code and checkpoints will be opensourced.semantic segmentation as the pixel-wise classification task benefits from detailed spatial information and aggregation of features from multiple scales.To solve these problems, researches have proposed better network operations and architecture designs. The dilated convolution operator [29,48,3,4,5,6] is one of the most popular methods that overcome the challenge of preserving feature resolution. The 'convolution with holes' design allows the network to use upsampled convolution kernels to extract abstract semantics without reducing feature resolution. Recently, scale-permuted networks discovered by neural architecture search (NAS) [14,13] have shown promising results on the task of object detection. Scale permutation for the intermediate building blocks enables the network to capture strong semantics and retain high feature resolution throughout network stages. Cross-scale feature fusion aggregates multi-scale semantics that helps the network to recognize objects at different scales. We evaluate our Spinenet-Seg models on the PASCAL VOC2012 benchmark, the Cityscapes benchmark and a challenging large-scale Street View dataset For pretraining, we use ImageNet and COCO datasets Search dataset is important since the evaluation signals influence the quality of the searched architectures For instance, performance on the PASCAL VOC2012 dataset depends heavily on the quality of the pretrained checkpoint, hence it is difficult to use for NAS that trains proxty tasks from scratch We decide to use COCO dataset since it is diverse, and unlike Cityscapes it has significantly smaller images Training proxy tasks from scratch using the COCO dataset usually converges, and eval signals can be used as stable rewards to update the NAS controller Table 1. A performance comparison of the original DeepLabv3+ ResNet-50 backbone that downsamples at the end of each stage and our modified ResNet-S50 backbone that downsamples at the beginning of each stage. Results are reported with the DeepLabv3+ system on the PASCAL VOC2012 val dataset. mIoU Table 2. Learned network configurations for the SpineNet-S49 architecture. We show the detailed configurations for each block for the search space components described in Section 3.1. BP: block permutation. CC: cross-scale connection. LA: level adjust- ment. DR: dilation ratio. FD: feature dimension. {3\u00d7L as ResNet - 50 / 101 / 152 S50 ) , while we refer to the original DeepLabv3+ backbones L beginning of each stage . This saves proposes to downsample the features at the end of each net - the effect of such a modification is shown in Tab . 1 . We 9\u00d7L We take two 30% of the computa -",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"mIoU\", \"Score\": \"83.04%\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL VOC 2012 val\", \"Metric\": \"mIoU\", \"Score\": \"85.64%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"82.6%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL VOC 2012 test\", \"Metric\": \"Mean IoU\", \"Score\": \"89.2%\"}} ]"
    },
    {
        "Context": "Weakly-and Semi-Supervised Panoptic Segmentation We present a weakly supervised model that jointly performs both semantic-and instance-segmentation -a particularly relevant problem given the substantial cost of obtaining pixel-perfect annotation for these tasks. In contrast to many popular instance segmentation approaches based on object detectors, our method does not predict any overlapping instances. Moreover, we are able to segment both \"thing\" and \"stuff\" classes, and thus explain all the pixels in the image. \"Thing\" classes are weakly-supervised with bounding boxes, and \"stuff\" with image-level tags. We obtain state-of-the-art results on Pascal VOC, for both full and weak supervision (which achieves about 95% of fullysupervised performance). Furthermore, we present the first weakly-supervised results on Cityscapes for both semantic-and instance-segmentation. Finally, we use our weakly supervised framework to analyse the relationship between annotation quality and predictive performance, which is of interest to dataset creators. Datasets and weak supervision We evaluate on two standard segmentation datasets, Pascal VOC and Cityscapes Following common practice on this dataset, we utilise additional images from the SBD dataset to obtain a training set of 10582 images We evaluate on the validation set, of 1449 images, as the evaluation server is not available for instance segmentation However, we increased the threshold to 50% to obtain higher precision on this more cluttered dataset Moreover, our fullysupervised instance segmentation model outperforms all previous work on this dataset Table 1. Comparison of semantic segmentation performance to recent methods using only weak, bounding-box supervision on Pascal VOC. Note that [12] and [11] use the less accurate VGG network, whilst we and [43] use ResNet-101. \"FS%\" denotes the percentage of fully-supervised performance. With COCO annotations Without COCO annotations Validation set FS% IoU ( full ) IoU ( weak ) Test set - Table 2. Comparison of instance segmentation performance to recent (fully-and weakly- supervised) methods on the VOC 2012 validation set. Weakly supervised without COCO Weakly supervised with COCO Fully supervised without COCO Fully supervised with COCO - PQ vol Method - AP r Table 3. Semantic-and instance-segmentation performance on Pascal VOC with varying lev- els of supervision from the Pascal and COCO datasets. The former is measured by the IoU, and latter by the AP r vol and PQ. PQ vol Dataset COCO IoU AP r Table 4. Semantic segmentation",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"AP\", \"Score\": \"28.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"PQ\", \"Score\": \"53.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"PQst\", \"Score\": \"62.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"PQth\", \"Score\": \"42.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Panoptic Segmentation\", \"Dataset\": \"Cityscapes val\", \"Metric\": \"mIoU\", \"Score\": \"79.8\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"81.3%\"}} ]"
    },
    {
        "Context": "Bilevel Online Adaptation for Out-of-Domain Human Mesh Reconstruction This paper considers anew problem of adapting a pretrained model of human mesh reconstruction to out-ofdomain streaming videos. However, most previous methods based on the parametric SMPL model [36] underperform in new domains with unexpected, domain-specific attributes, such as camera parameters, lengths of bones, backgrounds, and occlusions. Our general idea is to dynamically finetune the source model on test video streams with additional temporal constraints, such that it can mitigate the domain gaps without over-fitting the 2D information of individual test frames. A subsequent challenge is how to avoid conflicts between the 2D and temporal constraints. We propose to tackle this problem using anew training algorithm named Bilevel Online Adaptation (BOA), which divides the optimization process of overall multi-objective into two steps of weight probe and weight update in a training iteration. We demonstrate that BOA leads to state-of-the-art results on two human mesh reconstruction benchmarks 1 . * Equal contribution \u2020 Corresponding authors: Yunbo Wang, Bingbing Ni 1 The project website with code, supplementary materials and video results is at https://sites.google.com/view/humanmeshboa Domain Gap Camera Height (m) Bone Length (m) Focal Length (pixel) Datasets We use the Human3.6M dataset for training the source model and learn to adapt the model to the 3DHP and 3DPW datasets presents the statistics of typical domain gaps among these datasets \u2022 3DHP is the test split of the MPI-INF-3DHP dataset Evaluation metrics Following previous works, we evaluate our model in terms of Mean Per Joint Position Error (MPJPE), Procrustes-Aligned MPJPE (PA-MPJPE), and the Percentage of Correct Keypoints (PCK) with a threshold of 150mm on 3DHP presents atypical showcase of mesh reconstruction on the challenging 3DPW dataset Table 1. Typical domain gaps among datasets in terms of focal length, bone length, camera distance, and camera height [57]. Human3 . 6M dataset and take 3DPW and 3DHP as test sets . 3DPW 3DHP We first train the base model M on the collected Focal len . ( pixel ) H3 . 6M Table 2. Results on 3DPW, including end-to-end approaches (top) and those fine-tuned on the target domain (middle). Figure 4 . A qualitative comparison of mesh reconstruction on 3DPW streaming data . We zoom in on the limbs for better visualization . - PA - MPJPE VIBE MPJPE Table 3. Different protocols of pre-processing the 3DPW data by SPIN does not require access to the training set . In addition , we do PA - MPJPE PCK Method MPJPE Table 4. Results on 3DHP. All models but BOA are trained on the training split of MPI-INF-3DHP, while BOA",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"3DPW\", \"Metric\": \"MPJPE\", \"Score\": \"77.2\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"3DPW\", \"Metric\": \"PA-MPJPE\", \"Score\": \"49.5\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"3DPW\", \"Metric\": \"MPJPE\", \"Score\": \"69.3\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"3DPW\", \"Metric\": \"MPVPE\", \"Score\": \"82.3\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"3DPW\", \"Metric\": \"PA-MPJPE\", \"Score\": \"67.5\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"3DPW\", \"Metric\": \"acceleration error\", \"Score\": \"6.8\"}} ]"
    },
    {
        "Context": "LFFD: A Light and Fast Face Detector for Edge Devices Face detection, as a fundamental technology for various applications, is always deployed on edge devices which have limited memory storage and low computing power. This paper introduces a Light and Fast Face Detector (LFFD) for edge devices. The proposed method is anchorfree and belongs to the one-stage category. Specifically, we rethink the importance of receptive field (RF) and effective receptive field (ERF) in the background of face detection. Essentially, the RFs of neurons in a certain layer are distributed regularly in the input image and theses RFs are natural \"anchors\". Combining RF \"anchors\" and appropriate RF strides, the proposed method can detect a large range of continuous face scales with 100% coverage in theory. The insightful understanding of relations between ERF and face scales motivates an efficient backbone for onestage detection. The backbone is characterized by eight detection branches and common layers, resulting in efficient computation. Comprehensive and extensive experiments on popular benchmarks: WIDER FACE and FDDB are conducted. A new evaluation schema is proposed for application-oriented scenarios. Under the new schema, the proposed method can achieve superior accuracy (WIDER FACE Val/Test -Easy: 0.910/0.896, Medium: 0.881/0.865, Hard: 0.780/0.770; FDDB -discontinuous: 0.973, continuous: 0.724). Multiple hardware platforms are introduced to evaluate the running efficiency. The proposed method can obtain fast inference speed ( NVIDIA TITAN Xp: 131.45 FPS at 640\u00d7480; NVIDIA TX2: 136.99 PFS at 160\u00d7120; Raspberry Pi 3 Model B+: 8.44 FPS at 160\u00d7120) with model size of 9 MB. Firstly, anew evaluation schema is proposed and the evaluation results on benchmarks are presented In this subsection, anew evaluation schema is described at the beginning SIO is proposed to reform the evaluation procedure for real-world applications The conventional evaluation procedure involves some tricky means, such as flips and image pyramids, for achieving higher accuracy We evaluate all methods on two benchmarks: FDDB and WDIER FACE FDDB dataset We show final evaluation results of LFFD on FDDB against above five methods in WIDER FACE dataset Table 1. Accuracy of the top-5 methods on validation set of WIDER FACE. Method Easy Subset Medium Hard",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Face Detection\", \"Dataset\": \"FDDB\", \"Metric\": \"AP\", \"Score\": \"0.973\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Detection\", \"Dataset\": \"WIDER Face (Hard)\", \"Metric\": \"AP\", \"Score\": \"0.770\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Detection\", \"Dataset\": \"WIDER Face (Medium)\", \"Metric\": \"AP\", \"Score\": \"0.865\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Detection\", \"Dataset\": \"WIDER Face (Easy)\", \"Metric\": \"AP\", \"Score\": \"0.896\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Face Detection\", \"Dataset\": \"FDDB\", \"Metric\": \"AP\", \"Score\": \"0.901\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Detection\", \"Dataset\": \"WIDER Face (Hard)\", \"Metric\": \"AP\", \"Score\": \"0.876\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Detection\", \"Dataset\": \"WIDER Face (Medium)\", \"Metric\": \"AP\", \"Score\": \"0.931\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Detection\", \"Dataset\": \"WIDER Face (Easy)\", \"Metric\": \"AP\", \"Score\": \"0.943\"}} ]"
    },
    {
        "Context": "Analytical correlation of routing table length index and routing path length index in hierarchical routing model In Kleinrock and Kamoun's paper, the inverse relation of routing table length index and routing path length index in hierarchical routing model is illustrated. In this paper we give the analytical correlation of routing table length index and routing path length index in hierarchical routing model.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Student-Teacher Feature Pyramid Matching for Unsupervised Anomaly Detection Anomaly detection is a challenging task and usually formulated as an unsupervised learning problem for the unexpectedness of anomalies. This paper proposes a simple yet powerful approach to this issue, which is implemented in the student-teacher framework for its advantages but substantially extends it in terms of both accuracy and efficiency. Given a strong model pre-trained on image classification as the teacher, we distill the knowledge into a single student network with the identical architecture to learn the distribution of anomaly-free images and this one-step transfer preserves the crucial clues as much as possible. Moreover, we integrate the multi-scale feature matching strategy into the framework, and this hierarchical feature alignment enables the student network to receive a mixture of multi-level knowledge from the feature pyramid under better supervision, thus allowing to detect anomalies of various sizes. The difference between feature pyramids generated by the two networks serves as a scoring function indicating the probability of anomaly occurring. Due to such operations, our approach achieves accurate and fast pixel-level anomaly detection. Very competitive results are delivered on three major benchmarks, significantly superior to the state of the art ones. In addition, it makes inferences at a very high speed (with 100 FPS for images of the size at 256\u00d7256), at least dozens of times faster than the latest counterparts. To evaluate the effectiveness of our method, we first conduct experiments on the MVTec Anomaly Detection (MVTec-AD) dataset Our method is then applied to the ShanghaiTech Campus (STC) dataset for unsupervised anomaly pixel-level detection and the CIFAR-10 dataset for one-class classification To strike a balance between detection accuracy and efficiency, we resize all images in the MVTec-AD and STC datasets to 256\u00d7256 For instance, flipped metal nut in the MvTec-AD dataset is regraded as anomalous product Evaluation Metrics Our method is first evaluated on the MVTec-AD dataset, which is specifically created to benchmark algorithms for unsupervised detection of anomalous regions This makes the MVTec-AD dataset more challenging than those previously used in the literature (e.g., MNIST and CIFAR-10) where anomalous images come from other different categories Pixel-precise anomaly detection on the MvTec-AD dataset We further evaluate our method for pixel-level anomaly detection on the STC dataset This dataset is originally created Table 1. Image-level anomaly detection on the MvTec-AD dataset. The performance is measured by average AUC-ROC across 15 cat- egories. The best results are highlighted by boldface. Results for all approaches except ours are quoted from [11]. Ours SPADE Geom GANomaly ITAE 2 - AE Table 2. Pixel-precise anomaly detection on the MvTec-AD dataset. The performance is measured by the pixel-level AUC- ROC for each category. The best results are highlighted in bold- face. Results for all approaches except ours are quoted from Ours SPADE CNN - Dict SSIM - AE AnoGAN Table 3. Pixel-level anomaly detection on the MvTec-AD dataset. The performance is measured by the normalized area under the Per- Region-Overlap (PRO) curve up to an average false positive rate (FPR) of 30% for each category. The best results are highlighted in boldface. Results for all approaches except ours are quoted from STAD Ours K - means SPADE CNN",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Anomaly Detection\", \"Dataset\": \"MVTec AD\", \"Metric\": \"Detection AUROC\", \"Score\": \"95.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Anomaly Detection\", \"Dataset\": \"MVTec AD\", \"Metric\": \"Segmentation AUROC\", \"Score\": \"97.0\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Anomaly Detection\", \"Dataset\": \"MVTec AD\", \"Metric\": \"Detection AUROC\", \"Score\": \"95.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Anomaly Detection\", \"Dataset\": \"MVTec AD\", \"Metric\": \"Segmentation AUROC\", \"Score\": \"95.9\"}} ]"
    },
    {
        "Context": "Unsupervised Training for 3D Morphable Model Regression We present a method for training a regression network from image pixels to 3D morphable model coordinates using only unlabeled photographs. The training loss is based on features from a facial recognition network, computed onthe-fly by rendering the predicted faces with a differentiable renderer. To make training from features feasible and avoid network fooling effects, we introduce three objectives: a batch distribution loss that encourages the output distribution to match the distribution of the morphable model, a loopback loss that ensures the network can correctly reinterpret its own output, and a multi-view identity loss that compares the features of the predicted 3D face and the input photograph from multiple viewing angles. We train a regression network using these objectives, a set of unlabeled photographs, and the morphable model itself, and demonstrate state-of-the-art results. nition network [25] into identity parameters for the Basel 2017 Morphable Face Model [8].to-image autoencoder with a fixed, morphable-model-based decoder and an image-based loss [28]. This paper presents a method for training a regression network that removes both the need for supervised training data and the reliance on inverse rendering to reproduce image pixels. Instead, the network learns to minimize a loss based on the facial identity features produced by a face recognition network such as VGG-Face [17] or Google's FaceNet [25]. These features are robust to pose, expression, lighting, and even non-photorealistic inputs. We exploit this 1 arXiv:1806.06098v1 [cs.CV] We then evaluate our method quantitatively by comparing reconstruction error against scanned 3D face geometry (Sec Results on the MoFA-Test dataset Table 1. Mean Error on MICC Dataset using point-to-plane dis- tance after ICP alignment of video-averaged outputs with isotropic scale estimation. Our errors lower on average and in variance, both within individual subjects and as conditions change. Mean Cooperative Std . Outdoor Indoor Mean Std . Table 2. Earth mover's distance between distributions of VGG- Face \u03c6( t) similarity and distributions of same and different iden- tities on LFW. A low distance for \"Same\" means the similarity scores between a photo and its associated 3D rendering are close to the scores of same identity photos in LFW, while a low distance for \"Diff.\" means the scores are close to the scores of different identity photos. ters were fixed for all renderings . Same Diff . MICC MoFA - T Figure 5 . Distributions of cosine similarity between VGG - Face LFW Table 3. Identity Clustering Recall using VGG-Face distances on MoFA-Test",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"3D Face Reconstruction\", \"Dataset\": \"Florence\", \"Metric\": \"Average 3D Error\", \"Score\": \"1.50\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Human Pose Estimation\", \"Dataset\": \"Human3.6M\", \"Metric\": \"Average MPJPE (mm)\", \"Score\": \"65.6\"}} ]"
    },
    {
        "Context": "Unsupervised Image-to-Image Translation Networks Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in https://github.com/mingyuliutw/unit. We used the map dataset (visualized in), which contained corresponding pairs of images in two domains (satellite image and map) useful for quantitative evaluation We did not evaluate the translation from maps to images since the translation was multi-modal, which was difficult to construct a proper evaluation metric In, we showed several example results achieved by applying the proposed framework to translate images between the synthetic images in the SYNTHIA dataset and the real images in the Cityscape dataset We used the images of Husky, German Shepherd, Corgi, Samoyed, and Old English Sheep dogs in the ImageNet dataset to learn to translate dog images between different breeds We also used the images of house cat, tiger, lion, cougar, leopard, jaguar, and cheetah in the ImageNet dataset to learn to translate cat images between different species We used the CelebA dataset for attribute-based face images translation Each face image in the dataset Table 2: Unsupervised domain adaptation performance. The reported numbers are classification accuracies. DTN [ 26 ] SA [ 4 ] DANN [ 5 ] CoGAN UNIT ( proposed )",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Unsupervised Image-To-Image Translation\", \"Dataset\": \"Freiburg Forest Dataset\", \"Metric\": \"PSNR\", \"Score\": \"9.42\"}}, {\"LEADERBOARD\": {\"Task\": \"Multimodal Unsupervised Image-To-Image Translation\", \"Dataset\": \"Edge-to-Shoes\", \"Metric\": \"Diversity\", \"Score\": \"0.011\"}}, {\"LEADERBOARD\": {\"Task\": \"Multimodal Unsupervised Image-To-Image Translation\", \"Dataset\": \"Edge-to-Shoes\", \"Metric\": \"Quality\", \"Score\": \"37.4%\"}}, {\"LEADERBOARD\": {\"Task\": \"Multimodal Unsupervised Image-To-Image Translation\", \"Dataset\": \"Cats-and-Dogs\", \"Metric\": \"CIS\", \"Score\": \"0.115\"}}, {\"LEADERBOARD\": {\"Task\": \"Multimodal Unsupervised Image-To-Image Translation\", \"Dataset\": \"Cats-and-Dogs\", \"Metric\": \"IS\", \"Score\": \"0.826\"}}, {\"LEADERBOARD\": {\"Task\": \"Multimodal Unsupervised Image-To-Image Translation\", \"Dataset\": \"EPFL NIR-VIS\", \"Metric\": \"PSNR\", \"Score\": \"15.33\"}}, {\"LEADERBOARD\": {\"Task\": \"Multimodal Unsupervised Image-To-Image Translation\", \"Dataset\": \"Edge-to-Handbags\", \"Metric\": \"Diversity\", \"Score\": \"0.023\"}}, {\"LEADERBOARD\": {\"Task\": \"Multimodal Unsupervised Image-To-Image Translation\", \"Dataset\": \"Edge-to-Handbags\", \"Metric\": \"Quality\", \"Score\": \"37.3%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image-to-Image Translation\", \"Dataset\": \"GTAV-to-Cityscapes Labels\", \"Metric\":\"mIoU\", \"Score\": \"43.2\"}} ]"
    },
    {
        "Context": "FASTER EXPONENTIALS OF POWER SERIES We describe anew algorithm for computing exp f where f is a power series in C x . If M (n) denotes the cost of multiplying polynomials of degree n, the new algorithm costs (2.1666 . . . + o(1))M (n) to compute exp f to order n. This improves on the previous best result, namely (2.333 . . . + o(1))M (n).",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Mockingbird: Defending Against Deep-Learning-Based Website Fingerprinting Attacks with Adversarial Traces Website Fingerprinting (WF) is a type of traffic analysis attack that enables a local passive eavesdropper to infer the victim's activity even when the traffic is protected by encryption, a VPN, or an anonymity system like Tor. Leveraging a deep-learning classifier, a WF attacker can gain over 98% accuracy on Tor traffic. Existing WF defenses are either very expensive in terms of bandwidth and latency overheads (e.g. two-to-three times as large or slow) or ineffective against the latest attacks. In this paper, we explore a novel defense, Mockingbird, based on the idea of adversarial examples that have been shown to undermine machine-learning classifiers in other domains. Since the attacker gets to design his classifier based on the defense design, we first demonstrate that at least one technique for generating adversarial-example based traces fails to protect against an attacker using adversarial training for robust classification. We then propose Mockingbird, a technique for generating traces that resists adversarial training by moving randomly in the space of viable traces and not following more predictable gradients. The technique drops the accuracy of the state-of-the-art attack hardened with adversarial training from 98% to as low as 29% while incurring only 56% bandwidth overhead. The attack accuracy is generally lower than state-of-the-art defenses, and much lower when considering Top-2 accuracy, while incurring lower overheads inmost settings. In our experiments, we use burst sequences for both FD and HD datasets Full-Duplex (FD) : We use the FD traffic traces provided by Sirinam et al.Their dataset has 95 classes with 1000 instances each We also use the FD openworld (OW) dataset from Sirinam et al We process the OW dataset as well Half-Duplex (HD) : We use the HD dataset provided by Sirinam et al. This dataset contains 100 sites which are also from top 100 sites in Alexa.com [1], with 900 instances for each class We preprocess this dataset in the same way we processed FD dataset We also process OW dataset We use both of these HD closed-world (CW) and OW datasets for our HD evaluations In order to use these datasets for our defense, further preprocessing is required before we can use those in our models In consequence, we must determine an ideal size of input Table 1: Dataset Split: Adv Set & Detector Set. FD: Full-Duplex, HD: Half-Duplex, NC: Number of Classes, NI: Number of Instances, CW: Closed-World, OW: Open-World. Bandwidth 40 , 000 Adv Set A Total Case I OW Overhead Case II 40 , 716 ( NC \u00d7 N I ) HD FD Table 2: The Evaluation of the Mockingbird against DF, AWF, CUMUL, k-FP, and k-NN Attacks & Comparison against WTF-PAD and W-T Defenses. S\u00d7I: Sites\u00d7Instances, BWO: Bandwidth Overhead, FD: Full-Duplex, HD: Half-Duplex. iterations . Bandwidth Case I k - NN [ 38 ] \u03b1 =7 DF [ 36 ] \u03b1 =5 k - FP [ 15 ] ( b ) Case II Number of Iterations 100 200 300 500 ( a ) Case I CUMUL [ 26 ] AWF [ 34 ] Overhead BWO Table 3: Top-2 Accuracy of DF Attack against Mocking- bird and W-T. FD: Full-duplex, HD: Half-Duplex. DF [",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Website Fingerprinting Defense\", \"Dataset\": \"Website Traffic Data on Tor\", \"Metric\": \"Accuracy (%)\", \"Score\": \"42\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Adversarial Defense\", \"Dataset\": \"ImageNet (non-targeted PGD, max perturbation=4)\", \"Metric\": \"Accuracy\", \"Score\": \"7.0%\"}} ]"
    },
    {
        "Context": "GRDN:Grouped Residual Dense Network for Real Image Denoising and GAN-based Real-world Noise Modeling Recent research on image denoising has progressed with the development of deep learning architectures, especially convolutional neural networks. However, real-world image denoising is still very challenging because it is not possible to obtain ideal pairs of ground-truth images and real-world noisy images. Owing to the recent release of benchmark datasets, the interest of the image denoising community is now moving toward the real-world denoising problem. In this paper, we propose a grouped residual dense network (GRDN), which is an extended and generalized architecture of the state-of-the-art residual dense network (RDN). The core part of RDN is defined as grouped residual dense block (GRDB) and used as a building module of GRDN. We experimentally show that the image denoising performance can be significantly improved by cascading GRDBs. In addition to the network architecture design, we also develop anew generative adversarial network-based real-world noise modeling method. We demonstrate the superiority of the proposed methods by achieving the highest score in terms of both the peak signal-to-noise ratio and the structural similarity in the NTIRE2019 Real Image Denoising Challenge -Track 2:sRGB. We used the training and validation images of NTIRE 2019 Real Image Denoising Challenge, which is a subset of SIDD dataset Let ChDB denote the dataset we used for our experiment Because the ground-truth images of the test dataset are not publicly available, we report the performance of image denoising models using the validation dataset in this Section Table 1: Comparison of image denoising models. Table 2: Performance comparison on the test dataset of NTIRE 2019 Real Image Denoising Challenge - Track2:sRGB. in the electronic version . ( b ) PSNR SSIM",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Color Image Denoising\", \"Dataset\": \"NTIRE 2019 Real Image Denoising Challenge (sRGB)\", \"Metric\": \"PSNR\", \"Score\": \"39.931743\"}}, {\"LEADERBOARD\": {\"Task\": \"Color Image Denoising\", \"Dataset\": \"NTIRE 2019 Real Image Denoising Challenge (sRGB)\", \"Metric\": \"SSIM\", \"Score\": \"0.973589\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Denoising\", \"Dataset\": \"SIDD\", \"Metric\": \"PSNR (sRGB)\", \"Score\": \"39.28\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Denoising\", \"Dataset\": \"SIDD\", \"Metric\": \"SSIM (sRGB)\", \"Score\": \"0.956\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Denoising\", \"Dataset\": \"DND\", \"Metric\": \"PSNR (sRGB)\", \"Score\": \"39.38\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Denoising\", \"Dataset\": \"DND\", \"Metric\": \"SSIM (sRGB)\", \"Score\": \"0.952\"}} ]"
    },
    {
        "Context": "ZHAI, WU: CLASSIFICATION IS A STRONG BASELINE FOR DEEP METRIC LEARNING Classification is a Strong Baseline for Deep Metric Learning Deep metric learning aims to learn a function mapping image pixels to embedding feature vectors that model the similarity between images. Two major applications of metric learning are content-based image retrieval and face verification. For the retrieval tasks, the majority of current state-of-the-art (SOTA) approaches are triplet-based nonparametric training. For the face verification tasks, however, recent SOTA approaches have adopted classification-based parametric training. In this paper, we look into the effectiveness of classification based approaches on image retrieval datasets. We evaluate on several standard retrieval datasets such as CAR-196, CUB-200-2011, Stanford Online  Product, and  In-Shop datasets for image retrieval and clustering, and establish that our classification-based approach is competitive across different feature dimensions and base feature networks. We further provide insights into the performance effects of subsampling classes for scalable classification-based training, and the effects of binarization, enabling efficient storage and computation for practical applications. We follow the same evaluation protocol commonly used in image retrieval tasks with the standard train/test split on four datasets: CARS-196, CUB-200-2011, Stanford Online Products (SOP), and In-shop Clothes Retrieval Finally in Section 4.6, we show that our method outperforms state-of-the-art methods on several retrieval datasets Table 1: Recall@K and NMI across standard retrieval tasks. All methods are trained using GoogleNet for a fair comparison. - 1 100 2 CUB - 200 4 SOP CARS - 196 8 NMI - 10 Table 2: Recall@K and NMI across standard retrieval tasks. All methods are trained using BNInception for a fair comparison - 1 100 2 CUB - 200 4 SOP CARS - 196 8 NMI 10 Table 3: Recall@K and NMI across standard retrieval tasks. All methods are trained using ResNet50 for a fair comparison BNInception for a fair comparison 1 100 2 CUB - 200 4 SOP CARS - 196 8 Table 2 : Recall@K and NMI across standard retrieval tasks . All methods are trained using NMI 10 Table 5: Recall@K on Stanford Online Products (SOP) and In-Shop. R -ResNet, G - GoogleNet, B -BNInception, \u2020 refers to refers to additional attention parameters, LMCL * is",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Retrieval\", \"Dataset\": \"SOP\", \"Metric\": \"R@1\", \"Score\": \"79.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Retrieval\", \"Dataset\": \"CUB-200-2011\", \"Metric\": \"R@1\", \"Score\": \"65.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Retrieval\", \"Dataset\": \"CARS196\", \"Metric\": \"R@1\", \"Score\": \"89.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Retrieval\", \"Dataset\": \"In-Shop\", \"Metric\": \"R@1\", \"Score\": \"89.4\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Retrieval\", \"Dataset\": \"SOP\", \"Metric\": \"R@1\", \"Score\": \"84.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Retrieval\", \"Dataset\": \"In-Shop\", \"Metric\": \"R@1\", \"Score\": \"91.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Retrieval\", \"Dataset\": \"CUB-200-2011\", \"Metric\": \"R@1\", \"Score\": \"74.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Retrieval\", \"Dataset\": \"CARS196\", \"Metric\": \"R@1\", \"Score\": \"91.5\"}} ]"
    },
    {
        "Context": "Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors Visual features are of vital importance for human action understanding in videos. This paper presents anew video representation, called trajectory-pooled deepconvolutional descriptor (TDD), which shares the merits of both hand-crafted features [31] and deep-learned features [24]. Specifically, we utilize deep architectures to learn discriminative convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional features into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to transform convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity compared with those hand-crafted features; (ii) TDDs take account of the intrinsic characteristics of temporal dimension and introduce the strategies of trajectoryconstrained sampling and pooling for aggregating deeplearned features. We conduct experiments on two challenging datasets: HMDB51 and UCF101. Experimental results show that TDDs outperform previous hand-crafted features [31] and deep-learned features [24]. Our method also achieves superior performance to the state of the art on these datasets 1 . In this section, we first present the details of datasets and their evaluation scheme In order to verify the effectiveness of TDDs, we conduct experiments on two public large datasets, namely HMDB51 and UCF101 The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos The dataset is composed of 6, 766 video clips from 51 action categories, with each category containing at least 100 clips Our experiments follow the original evaluation scheme using three different training/testing splits The UCF101 dataset contains 101 action classes and there are at least 100 video clips for each class The whole dataset contains 13, 320 video clips, which are divided into 25 groups for each action category We follow the evaluation scheme of the THUMOS13 challenge and adopt the three training/testing splits for evaluation As UCF101 is larger than HMDB51, we use the UCF101 dataset to train Table 1. ConvNet Architectures. We use similar architectures to two-stream ConvNets 1 / 16 1 / 4 1 / 8 Table 3. Performance of TDD on the HMDB51 dataset and UCF101 dataset. We compare our proposed TDD with iDT features Table 2. The performance of different layers of spatial nets and temporal nets on the HMDB51 dataset. conv5 conv4 conv3 conv2 conv1 Spatial ConvNets Temporal ConvNets",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"UCF101\", \"Metric\": \"3-fold Accuracy\", \"Score\": \"91.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"HMDB-51\", \"Metric\": \"Average accuracy of 3 splits\", \"Score\": \"65.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Activity Recognition In Videos\", \"Dataset\": \"DogCentric\", \"Metric\": \"Accuracy\", \"Score\": \"76.6\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Action Recognition\", \"Dataset\": \"UCF101\", \"Metric\": \"3-fold Accuracy\", \"Score\": \"94.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Recognition\", \"Dataset\": \"HMDB-51\", \"Metric\": \"Average accuracy of 3 splits\", \"Score\": \"71.5\"}} ]"
    },
    {
        "Context": "Learning by cheating Vision-based urban driving is hard. The autonomous system needs to learn to perceive the world and act in it. We show that this challenging learning problem can be simplified by decomposing it into two stages. We first train an agent that has access to privileged information. This privileged agent cheats by observing the ground-truth layout of the environment and the positions of all traffic participants. In the second stage, the privileged agent acts as a teacher that trains a purely vision-based sensorimotor agent. The resulting sensorimotor agent does not have access to any privileged information and does not cheat. This two-stage training procedure is counter-intuitive at first, but has a number of important advantages that we analyze and empirically demonstrate. We use the presented approach to train a vision-based autonomous driving system that substantially outperforms the state of the art on the CARLA benchmark and the recent NoCrash benchmark. Our approach achieves, for the first time, 100% success rate on all tasks in the original CARLA benchmark, sets anew record on the NoCrash benchmark, and reduces the frequency of infractions by an order of magnitude compared to the prior state of the art.  Table 1: Ablation study on the CoRL2017 bench- mark (CARLA 0.9.5, \"navigation\" condition, test town, test weather). Two key advantages of the presented decomposition -white-box supervision and on-policy trajectories -each substantially im- prove performance and together achieve 100% success rate on the benchmark. on - policy Table 2: Comparison of the success rate of the presented approach (LBC) to the state of the art on the original CARLA benchmark (CoRL2017) in the test town. (The supplement provides results on the training town.) LBC \u2020 denotes our agent trained and evaluated on CARLA 0.9.6. All other agents were evaluated on CARLA 0.8 and 0.9.5. Our approach outperforms all prior work and achieves 100% success rate on all routes in the full-generalization setting (test town, test weather). test train CIL [ 6 ] 100 LBC \u2020 CAL [ 22 ] CILRS [ 7 ] MP [ 8 ] LBC CIRL [ 14 ]",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Autonomous Driving\", \"Dataset\": \"CARLA Leaderboard\", \"Metric\": \"Driving Score\", \"Score\": \"8.94\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Vehicle Re-Identification\", \"Dataset\": \"CARPK\", \"Metric\": \"MAE\", \"Score\": \"6.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Vehicle Re-Identification\", \"Dataset\": \"CARPK\", \"Metric\": \"RMSE\", \"Score\": \"57.55\"}} ]"
    },
    {
        "Context": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system. In this work, we introduce anew type of linear connections, named fastforward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers. Fast-forward connections play an essential role in propagating the gradients and building a deep topology of depth 16. On the WMT'14 Englishto-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT'14 English-to-German task. We evaluate our method mainly on the widely used WMT'14 English-to-French translation task Table 2: The effect of F-F. We list the BLEU scores of Deep-Att with and without F-F. Because of the param- eter exploding problem, we can not list the model per- formance of larger depth without F-F. For n e = 1 and n d = 1, F-F connections only contribute to the represen- tation at interface part (see Eq. 7). d BLEU ne Table 3: BLEU scores with different LSTM layer width in Deep-Att. After using two times larger LSTM layer width of 1024, we can only obtain BLEU score of 33.8. It is still behind the corresponding Deep-Att with F-F. d BLEU ne Table 4: The effect of the interleaved bi-directional en- coder. We list the BLEU scores of our largest Deep-Att and Deep-ED models. The encoder term Bi denotes that the interleaved bi-directional encoder is used. Uni de- notes a model where all LSTM layers work in forward",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-French\", \"Metric\": \"BLEU score\", \"Score\": \"39.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-French\", \"Metric\": \"BLEU score\", \"Score\": \"35.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-German\", \"Metric\": \"BLEU score\", \"Score\": \"20.7\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-French\", \"Metric\": \"BLEU score\", \"Score\": \"35.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-French\", \"Metric\": \"BLEU score\", \"Score\": \"31.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"WMT2016 English-Romanian\", \"Metric\": \"BLEU score\", \"Score\": \"27.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-German\", \"Metric\": \"BLEU score\", \"Score\": \"33.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-German\", \"Metric\": \"BLEU score\", \"Score\": \"30.4\"}} ]"
    },
    {
        "Context": "The C Object System * Using C as a High-Level Object-Oriented Language The C Object System (COS) is a small C library which implements high-level concepts available in CLOS, OBJECTIVE-C and other object-oriented programming languages: uniform object model (class, metaclass and property-metaclass), generic functions, multi-methods, delegation, properties, exceptions, contracts and closures. COS relies on the programmable capabilities of the C programming language to extend its syntax and to implement the aforementioned concepts as first-class objects. COS aims at satisfying several general principles like simplicity, extensibility, reusability, efficiency and portability which are rarely met in a single programming language. Its design is tuned to provide efficient and portable implementation of message multi-dispatch and message multi-forwarding which are the heart of code extensibility and reusability. With COS features in hand, software should become as flexible and extensible as with scripting languages and as efficient and portable as expected with C programming. Likewise, COS concepts should significantly simplify adaptive and aspect-oriented programming as well as distributed and service-oriented computing.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Overlapping Multi-hop Clustering for Wireless Sensor Networks Clustering is a standard approach for achieving efficient and scalable performance in wireless sensor networks. Traditionally, clustering algorithms aim at generating a number of disjoint clusters that satisfy some criteria. In this paper, we formulate a novel clustering problem that aims at generating overlapping multi-hop clusters. Overlapping clusters are useful in many sensor network applications, including inter-cluster routing, node localization, and time synchronization protocols. We also propose a randomized, distributed multi-hop clustering algorithm (KOCA) for solving the overlapping clustering problem. KOCA aims at generating connected overlapping clusters that cover the entire sensor network with a specific average overlapping degree. Through analysis and simulation experiments we show how to select the different values of the parameters to achieve the clustering process objectives. Moreover, the results show that KOCA produces approximately equal-sized clusters, which allows distributing the load evenly over different clusters. In addition, KOCA is scalable; the clustering formation terminates in a constant time regardless of the network size.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Neural Architecture Search with Bayesian Optimisation and Optimal Transport Bayesian Optimisation (BO) refers to a class of methods for global optimisation of a function f which is only accessible via point evaluations. It is typically used in settings where f is expensive to evaluate. A common use case for BO in machine learning is model selection, where it is not possible to analytically model the generalisation performance of a statistical model, and we resort to noisy and expensive training and validation procedures to choose the best model. Conventional BO methods have focused on Euclidean and categorical domains, which, in the context of model selection, only permits tuning scalar hyper-parameters of machine learning algorithms. However, with the surge of interest in deep learning, there is an increasing demand to tune neural network architectures. In this work, we develop NASBOT, a Gaussian process based BO framework for neural architecture search. To accomplish this, we develop a distance metric in the space of neural network architectures which can be computed efficiently via an optimal transport program. This distance might be of independent interest to the deep learning community as it may find applications outside of BO. We demonstrate that NASBOT outperforms other alternatives for architecture search in several cross validation based model selection tasks on multi-layer perceptrons and convolutional neural networks.While there are several approaches to BO, those based on Gaussian processes (GP) [35] are most common in the BO literature. In its most unadorned form, a BO algorithm operates sequentially, starting at time 0 with a GP prior for f ; at time t, it incorporates results of evaluations from 1, . . . , t\u22121 in the form of a posterior for f . It then uses this posterior to construct an acquisition function \u03d5 t , where \u03d5 t (x) is a measure of the value of evaluating fat x at time t if our goal is to maximise f . Our RAND implementation, operates in exactly the same way as NASBOT, except that the EA procedure is fed a random sample from Unif(0, 1) instead of the GP acquisition each time it evaluates an architecture Datasets: We use the following datasets: blog feedback, indoor location, slice localisation, naval propulsion, protein tertiary structure, news popularity, Cifar10 Experimental Set up: Each method is executed in an asynchronously parallel setup of 2-4 GPUs, That is, it can evaluate multiple models in parallel, with each model on a single GPU When the evaluation of one model finishes, the methods can incorporate the result and immediately re-deploy the next job without waiting for the others to finish For the blog, indoor, slice, naval and protein datasets we use 2 GeForce GTX 970 (4GB) GPUs and a computational budget of 8 hours for each method For the news popularity dataset we use 4 GeForce GTX 980 (6GB) Table 3: The first row gives the number of samples N and the dimensionality D of each dataset in the form Method ( 54K , 385 ) ( 12K , 17 ) 150K iters ( 46K , 9 ) ( 60K , 3K ) ( 40K , 61 ) Cifar10 ( 21K , 529 ) Protein ( 60K , 281 ) Slice Naval News Indoor Blog",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Neural Architecture Search\", \"Dataset\": \"NAS-Bench-201, ImageNet-16-120\", \"Metric\": \"Accuracy (Test)\", \"Score\": \"46.37\"}}, {\"LEADERBOARD\": {\"Task\": \"Neural Architecture Search\", \"Dataset\": \"NAS-Bench-201, ImageNet-16-120\", \"Metric\": \"Search time (s)\", \"Score\": \"75600\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Accuracy\", \"Score\": \"75.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"MACs\", \"Score\": \"597M\"}}, { \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Params\", \"Score\": \"5.3M\"}}, { \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top-1 Error Rate\", \"Score\": \"24.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Search Time (GPU days)\", \"Score\": \"0.25\"}}, { \"LEADERBOARD\": { \"Task\": \"Neural Architecture Search\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Top-1 Error Rate\", \"Score\": \"2.39%\"}}, { \"LEADERBOARD\": { \"Task\": \"Node Classification\", \"Dataset\": \"PPI\", \"Metric\": \"F1\", \"Score\": \"99.09\"}}, { \"LEADERBOARD\": "
    },
    {
        "Context": "Devil in the Details: Towards Accurate Single and Multiple Human Parsing Human parsing has received considerable interest due to its wide application potentials. Nevertheless, it is still unclear how to develop an accurate human parsing system in an efficient and elegant way. In this paper, we identify several useful properties, including feature resolution, global context information and edge details, and perform rigorous analyses to reveal how to leverage them to benefit the human parsing task. The advantages of these useful properties finally result in a simple yet effective Context Embedding with Edge Perceiving (CE2P) framework for single human parsing. Our CE2P is end-to-end trainable and can be easily adopted for conducting multiple human parsing. Benefiting the superiority of CE2P, we won the 1st places on all three human parsing tracks in the 2nd Look into Person (LIP) Challenge. Without any bells and whistles, we achieved 56.50% (mIoU), 45.31% (mean AP r ) and 33.34% (AP p 0.5 ) in Track 1, Track 2 and Track 5, which outperform the state-of-the-arts more than 2.06%, 3.81% and 1.87%, respectively. We hope our CE2P will serve as a solid baseline and help ease future research in single/multiple human parsing. Global Context Embedding Module To evaluate the effectiveness of each module, we first conduct experiments by introducing a global context embedding module Comparison with State-of-the-Arts We evaluate the performance of CE2P on the validation dataset of LIP and compare it to other state-of-the-art approaches Table 1: Comparison of CE2P in various module settings on the validation set of LIP. The results are obtained without left-right flipping except for the last row. 'B' means baseline model. 'G', 'H' and 'E' denote global context, high resolution and edge perceiving module, respectively. ( CE2P ) al . 2018b ) , i . e . \" Poly \" learning rate policy with base learning dress l - leg pants scarf r - leg glove socks j - suits glasses r - arm mIoU coat hair face l - arm skirt u - cloth hat l - shoe r - shoe bkg Table 2: Comparison of performance on the validation set of LIP with state-of-arts methods. to the input size . The results are shown in Tab . 1 , and we can mean acc . mIoU pixel acc . Table 3: Comparison of the performance on the test set",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"LIP val\", \"Metric\": \"mIoU\", \"Score\": \"53.10%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"LIP val\", \"Metric\":\"mIoU\", \"Score\": \"65.3\"}} ]"
    },
    {
        "Context": "Towards an Embedding of Graph Transformation in Intuitionistic Linear Logic Linear logics have been shown to be able to embed both rewriting-based approaches and process calculi in a single, declarative framework. In this paper we are exploring the embedding of double-pushout graph transformations into quantified linear logic, leading to a Curry-Howard style isomorphism between graphs / transformations and formulas / proof terms. With linear implication representing rules and reachability of graphs, and the tensor modelling parallel composition of graphs / transformations, we obtain a language able to encode graph transformation systems and their computations as well as reason about their properties.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Information Hiding Using Improper Frame Padding Hiding information in network traffic may lead to leakage of confidential information. In this paper we introduce anew steganographic system: the PadSteg (Padding Steganography). To authors' best knowledge it is the first information hiding solution which represents interprotocol steganography i.e. usage of relation between two or more protocols from the TCP/IP stack to enable secret communication. PadSteg utilizes ARP and TCP protocols together with an Etherleak vulnerability (improper Ethernet frame padding) to facilitate secret communication for hidden groups in LANs (Local Area Networks). Basing on real network traces we confirm that PadSteg is feasible in today's networks and we estimate what steganographic bandwidth is achievable while limiting the chance of disclosure. We also point at possible countermeasures against PadSteg.  Table 1. The traffic was captured with the aid of Dumpcap which is part of the Wireshark sniffer ver. 1.3.3 (www.wireshark.org). The sources of traffic were ordinary computer devices placed in several university laboratories and employees' ones but also peripherals, servers and network equipment. To analyze the captured traffic and calculate statistics TShark (which is also part of Wireshark) was utilized. Statistics were calculated per day, and average results are presented. protocol TCP Others 8 , 945 , 403 Friday UPPER LAYER PROTCOLS AFFECTED WITH ETHERNET ARP ICMP",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Quality Aware Network for Set to Set Recognition This paper targets on the problem of set to set recognition, which learns the metric between two image sets. Images in each set belong to the same identity. Since images in a set can be complementary, they hopefully lead to higher accuracy in practical applications. However, the quality of each sample cannot be guaranteed, and samples with poor quality will hurt the metric. In this paper, the quality aware network (QAN) is proposed to confront this problem, where the quality of each sample can be automatically learned although such information is not explicitly provided in the training stage. The network has two branches, where the first branch extracts appearance feature embedding for each sample and the other branch predicts quality score for each sample. Features and quality scores of all samples in a set are then aggregated to generate the final feature embedding. We show that the two branches can be trained in an end-to-end manner given only the set-level identity annotation. Analysis on gradient spread of this mechanism indicates that the quality learned by the network is beneficial to set-to-set recognition and simplifies the distribution that the network needs to fit. Experiments on both face verification and person re-identification show advantages of the proposed QAN. The source code and network structure can be downloaded at GitHub 1 Based on above knowledge, we evaluate QAN on two human re-identification benchmarks and two unconstrained face verification benchmarks Results of evaluation obeying \"10-fold cross validation\" on PRID2011 and iLIDS-VID are shown in and Benefiting from the large scale training dataset, our CNN+AvePool and CNN+Min(cos) baselines are close to or even better than the state-of-the-art On PRID2011 dataset, QAN increase top-1 matching rate by 11.1% and 29.4% compared with CNN+AvePool and CNN+Min(cos) On iLIDS-VID dataset, inherent noise is much more than that in PRID2011, which significantly influence the accuracy of CNN+Min(cos) since operator \"Min(cos)\" is more sensitive than \"AvePool\" to noisy samples However, QAN achieves more gain on this noisy dataset Based on these two experiments, QAN significantly outperforms two baselines on both datasets The performance gain is more significant on noisy iLIDS-VID dataset, which meets the expectation and proves QAN's ability to deal with images of poor quality To prevent our Table 1. Comparison of QAN, AvePool, Min(cos) and other state-of-the-art methods on PRID2011, where the number repre- sents the cumulative matching rate in CMC curve. CMC5 CMC20 CMC1 90 70 CMC10 95 97 Table 2. Comparison of QAN, AvePool, Min(cos) and other human re-identification methods on iLIDS-VID, where the num- ber represents the cumulative matching rate on CMC curve. PRID2011 CMC5 CMC20 58 CMC1 91 iLIDS - VID CMC10 84 96 Table 3. Cross-dataset performance of QAN on PRID2011, where the number represents the cumulative accuracy on CMC curve. iLIDS - VID 57 CMC5 CMC20 69 28 CMC1 81 CMC10 PRID2011 Table 4. Cross-dataset performance of QAN on iLIDS-VID, where the number represents the cumulative accuracy on CMC curve. iLIDS - VID 57 CMC5 CMC20 69 28 CMC1 81 CMC10 PRID2011 Table 5. Average accuracy and AUC of QAN on YouTube Face dataset, compared with baselines and other state-of-the-arts. - %",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Face Verification\", \"Dataset\": \"YouTube Faces DB\", \"Metric\": \"Accuracy\", \"Score\": \"96.17%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"PRID2011\", \"Metric\": \"Accuracy\", \"Score\": \"94.07%\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"PRID2011\", \"Metric\": \"Accuracy\", \"Score\": \"93.70%\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"PRID2011\", \"Metric\": \"Accuracy\", \"Score\": \"93.27%\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"PRID2011\", \"Metric\": \"Accuracy\", \"Score\": \"87.27%\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"PRID2011\", \"Metric\": \"Accuracy\", \"Score\": \"93.17%\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"PRID2011\", \"Metric\": \"Accuracy\", \"Score\": \"83.14%\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"PRID2011\", \"Metric\": \"Accuracy\", \"Score\": \"80.026\"}}, { \"LEADERBOARD\": { "
    },
    {
        "Context": "Symbolic model checking of tense logics on rational Kripke models We introduce the class of rational Kripke models and study symbolic model checking of the basic tense logic Kt and some extensions of it in models from that class. Rational Kripke models are based on (generally infinite) rational graphs, with vertices labeled by the words in some regular language and transitions recognized by asynchronous two-head finite automata, also known as rational transducers. Every atomic proposition in a rational Kripke model is evaluated in a regular set of states. We show that every formula of Kt has an effectively computable regular extension in every rational Kripke model, and therefore local model checking and global model checking of Kt in rational Kripke models are decidable. These results are lifted to a number of extensions of Kt. We study and partly determine the complexity of the model checking procedures.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "RECON: Relation Extraction using Knowledge Graph Context in a Graph Neural Network In this paper, we present a novel method named RECON, that automatically identifies relations in a sentence (sentential relation extraction) and aligns to a knowledge graph (KG). RECON uses a graph neural network to learn representations of both the sentence as well as facts stored in a KG, improving the overall extraction quality. These facts, including entity attributes (label, alias, description, instance-of) and factual triples, have not been collectively used in the state of the art methods. We evaluate the effect of various forms of representing the KG context on the performance of RECON. The empirical evaluation on two standard relation extraction datasets shows that RECON significantly outperforms all state of the art methods on NYT Freebase and Wikidata datasets. ACM Reference Format:  Table 1: Comparison of RECON and sentential RE models on the Wikidata dataset. Best values are in bold. Each time a KG context is added in a graph neural network, the per- formance has increased, resulting in a significant RECON outperformance against all sentential RE baselines. P R Macro Micro F1 Table 2: Comparison of RECON against baselines (sentential and multi-instance) on the NYT Freebase dataset. Best val- ues are in bold. RECON continues to significantly outper- form sentential RE baselines and also surpasses the perfor- mance of state of the art multi-instance RE approach. Sentential Precision @30% @10% Table 3: The McNemar's test for statistical significance on the results of both datasets. It can be observed that each of the improvement in the RECON configurations is statisti- cally significant independent of the underlying KG. Freebase Wikidata 3699 Statistic p - value Contingency 4417 Table 4: RECON-EAC performance on Wikidata Dataset.",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Relationship Extraction (Distant Supervised)\", \"Dataset\": \"New York Times Corpus\", \"Metric\": \"P@10%\", \"Score\": \"87.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Relationship Extraction (Distant Supervised)\", \"Dataset\": \"New York Times Corpus\", \"Metric\": \"P@30%\", \"Score\": \"74.1\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"WebNLG\", \"Metric\": \"F1\", \"Score\": \"28.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"NYT\", \"Metric\": \"F1\", \"Score\": \"42.0\"}}, { \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"NYT-single\", \"Metric\": \"F1\", \"Score\": \"49.5\"}} ]"
    },
    {
        "Context": "An Accurate SER Estimation Method Based on Propagation Probability In this paper, we present an accurate but very fast soft error rate (SER) estimation technique for digital circuits based on error propagation probability (EPP) computation. Experiments results and comparison of the results with the random simulation technique show that our proposed method is on average within 6% of the random simulation method and four to five orders of magnitude faster.  Table 2. Our approach vs. random simulation 69510 79950 72800 15480 17220 23810 87230 SysT ISP 3133 %Dif 5270 3480 3405 SPT 1695 SimT 9648 12833 12951",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "M\u00e9thode de calcul du rayonnement acoustique de structures complexes Dans l'industrie automobile, pr\u00e9dire le bruit rayonn\u00e9 est une \u00e9tape importante de la conception. Pour r\u00e9soudre les probl\u00e8mes acoustiques, il existe principalement deux familles de m\u00e9thodes : les M\u00e9thodes \u00c9l\u00e9ments Finis (FEM) et les M\u00e9thodes \u00c9l\u00e9ments de Fronti\u00e8res (BEM). Pour calculer le rayonnement acoustique en champ libre, on utilise g\u00e9n\u00e9ralement plut\u00f4t les \u00e9l\u00e9ments de fronti\u00e8res. N\u00e9anmoins ces m\u00e9thodes peuvent induire des singularit\u00e9s, et sont par cons\u00e9quent moins faciles \u00e0 utiliser que les \u00e9l\u00e9ments finis qui eux sont plut\u00f4t adapt\u00e9s \u00e0 l'\u00e9tude des milieux born\u00e9s. La m\u00e9thode d\u00e9crite dans cet article, la SDM, permet de tirer avantage de ces deux m\u00e9thodes en utilisant chacune l\u00e0 o\u00f9 elle est la plus performante. Une nouvelle m\u00e9thode fond\u00e9e sur \u00e9l\u00e9ments finis est \u00e9galement pr\u00e9sent\u00e9e et permet de remplacer avantageusement les \u00e9l\u00e9ments de fronti\u00e8re pour traiter le probl\u00e8me ext\u00e9rieur. L'efficacit\u00e9 de la SDM coupl\u00e9e \u00e0 cette nouvelle m\u00e9thode est discut\u00e9e.ABSTRACT. In the automotive industry, predicting noise during design cycle is a necessary step. Well-known methods exist to answer this issue in low frequency domain. Among these, Finite Element Methods, adapted to closed domains, are quite easy to implement whereas Boundary Element Methods are more adapted to infinite domains, but may induce singularity problems. In this article, the described method, the SDM, allows to use both methods in their best application domain. A new method is also presented to solve the SDM exterior problem.MOTS-CL\u00c9S : SDM, FEM, BEM, rayonnement acoustique, structures \u00e0 g\u00e9om\u00e9tries complexes, algorithme de clonage, sous structuration infinie.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "A finiteness structure on resource terms In our paper \"Uniformity and the Taylor expansion of ordinary lambda-terms\" (with Laurent Regnier), we studied a translation of lambda-terms as infinite linear combinations of resource lambda-terms, from a calculus similar to Boudol's lambda-calculus with resources and based on ideas coming from differential linear logic and differential lambda-calculus. The good properties of this translation wrt. beta-reduction were guaranteed by a coherence relation on resource terms: normalization is \"linear and stable\" (in the sense of the coherence space semantics of linear logic) wrt. this coherence relation. Such coherence properties are lost when one considers non-deterministic or algebraic extensions of the lambda-calculus (the algebraic lambda-calculus is an extension of the lambda-calculus where terms can be linearly combined). We introduce a \"finiteness structure\" on resource terms which induces a linearly topologized vector space structure on terms and prevents the appearance of infinite coefficients during reduction, in typed settings. * This work has been partly funded by the ANR project BLAN07-1 189926 Curry-Howard for Concurrency (CHOCO). 1 It is not really standard to consider dereliction as structural.The only infinite rule of LL is promotion. The potentially infinite duplicating power of contraction is not \"located\" in the contraction rule itself, but in the fact that, for being duplicable by contractions, a proof must be promoted first. This fact can be observed in denotational models but is not clear in the syntax because the structural rules have no other opponents but promotion 2 .The situation is quite different in differential LL (and, implicitly, in differential lambdacalculus and its variants), a system that we introduced recently (see [ER03,ER06b,EL09]). In this system, the \"?\" rules have exact dual rules: there is a cocontraction, a coweakening and a codereliction rules. These rules are logical versions of standard mathematical operations used in elementary differential calculus, whence the name of the system. So in differential LL we have structural and costructural rules and these rules interact in a completely symmetric and finite way, just as in the multiplicative and additive fragment. Promotion remains apart, as the only truly infinite rule of logic. This fact, which in LL could be observed only in denotational models, can be expressed syntactically in differential LL by means of the Taylor expansion of promotion rules.Resource lambda-calculus. This operation is more easily understood in the lambda-calculus (see [Tra08] for the connection between lambda-terms and nets in differential LL). Roughly speaking, the ordinary lambda-calculus correspond to the fragment of LL which contains the multiplicative, structural and promotion rules. But we can also consider a lambda-calculus corresponding to the multiplicative, structural and costructural rules: the resource calculus that we introduced in [ER08]. Similar calculi already existed in the literature, such as Boudol's calculi with multiplicities [Bou93] or with resources [BCL99], and also Kfoury's calculi [Kfo00], introduced with different motivations and with different semantic backgrounds. The intuition behind our calculus with resources is as follows.The first thing to say is that types should bethought of as (topological) vector spaces and not as domains. Consider then a term t : A \u21d2 B which should be seen as a function from A to B. Then imagine that it makes sense to compute the n-the derivative oft at the point 0 of the vector space A: it is a function t (n) (0) : A n \u2192 A, separately linear in each of its argument, and symmetric in the sense that t (n) (0)(s 1 , . . . , s n ) = t (n) (0)(s f (1) , . . . , sf (n) ) for any permutation f \u2208 Sn and any tuple (s 1 , . . . , s n ) \u2208 A n . In our resource calculus, we have an application construction which represents this operation. Given a term t (of type A \u21d2 B if we are in a typed setting) and a finite number s 1 , . . . , s n of terms (of type A), we can \"apply\" t to the multiset S = s 1 \u00b7 \u00b7 \u00b7 s n (the multiset whose elements are s 1 , . . . , s n , taking multiplicities into account) and we denote with s S this operation. We take benefit of the intrinsic commutativity of multisets for implementing the symmetry of the n-th derivative. The other constructions of this calculus are standard: we have variables x, y, . . . and abstractions \u03bbx s. Redexes are terms of the shape \u03bbx s Sand x can have several free occurrences in s, which are all linear. When reducing this redex, one does not duplicate S. Instead, one splits it into as many pieces as there are occurrences of x in s, and since all these occurrences are linear, all these pieces should contain exactly one term. We do that in all possible ways and take the sum of all possible results. When the number of free occurrences of x in sand the size of S do not coincide, the result of this operation is 0.For this to make sense, one must have the possibility of adding terms, and this is compatible with the idea that types are vector spaces.Taylor expansion. Taylor expansion consists in replacing the ordinary application of lambda-calculus with this differential application of the resource calculus. If M : A \u21d2 B and N : A are terms, then the standard Taylor formula should be",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Incremental Topological Ordering and Strong Component Maintenance We present an on-line algorithm for maintaining a topological order of a directed acyclic graph as arcs are added, and detecting a cycle when one is created. Our algorithm takes O(m 1/2 ) amortized time per arc, where m is the total number of arcs. For sparse graphs, this bound improves the best previous bound by a logarithmic factor and is tight to within a constant factor fora natural class of algorithms that includes all the existing ones. Our main insight is that the bidirectional search method of previous algorithms does not require an ordered search, but can be more general. This allows us to avoid the use of heaps (priority queues) entirely. Instead, the deterministic version of our algorithm uses (approximate) median-finding. The randomized version of our algorithm avoids this complication, making it very simple. We extend our topological ordering algorithm to give the first detailed algorithm for maintaining the strong components of a directed graph, and a topological order of these components, as arcs are added. This extension also has an amortized time bound of O(m 1/2 ) per arc.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects. Moreover, they should be able to generalise the acquired world knowledge to new languages, modulo cultural differences. Advances in machine reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, which includes resource-poor languages like Eastern Apur\u00edmac Quechua and Haitian Creole. We evaluate a range of state-of-the-art models on this novel dataset, revealing that the performance of current methods based on multilingual pretraining and zero-shot fine-tuning falls short compared to translation-based transfer. Finally, we propose strategies to adapt multilingual models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. The XCOPA dataset is freely available at github.com/cambridgeltl/xcopa. arXiv:2005.00333v2 [cs.CL] 26 Oct 2020 14 github.com/huggingface/transformers 15 tfhub.dev/google/ universal-sentence-encoder-multilingual-large/ 3 We now benchmark a series of state-of-the-art models on XCOPA to provide baseline scores for future research, as well as to expose the challenging nature of the dataset Table 2: Indices of typological, genealogical, and areal diversity for the language samples of a set of NLU datasets. XQUAD XCOPA TyDiQA XNLI MLQA PAWS - X Table 3: Percentage of annotated labels in each language agreeing with the majority label. Note that the majority label is highly reliable, as we observed a 100% agreement with the development set labels in the original COPA. VI QU SW TH ID IT HT TA TR ZH ET Table 4: Different experimental setups for data sources. CO=COPA; SI=SIQA; ZS=Zero-Shot; TLV=Target Language Validation (Set). All XCOPA USE \u2229 MBERT \u2229 Table 4. We first compare our cross-lingual average XCOPA results in the best setup with the English COPA performance of the monolingual English BERT (Base) reported by Sap et al. (2019), namely 63 accuracy in COPA-only fine-tuning (+7%) and 80 after sequential SIQA + COPA fine-tuning (+17%). This contributes to recent suspicions (Cao et al.,",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Cross-Lingual Transfer\", \"Dataset\": \"XCOPA\", \"Metric\": \"Accuracy\", \"Score\": \"76.05\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Common Sense Reasoning\", \"Dataset\": \"Xplaint\", \"Metric\": \"Accuracy\", \"Score\": \"97.6\"}}, { \"LEADERBOARD\": { \"Task\": \"Common Sense Reasoning\", \"Dataset\": \"Xplaint\", \"Metric\": \"Accuracy\", \"Score\": \"97.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Common Sense Reasoning\", \"Dataset\": \"Xplaint\", \"Metric\": \"F1\", \"Score\": \"77.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Common Sense Reasoning\", \"Dataset\": \"Xplaint\", \"Metric\": \"Accuracy\", \"Score\": \"78.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Common Sense Reasoning\", \"Dataset\": \"Xplaint\", \"Metric\": \"F1\", \"Score\": \"72.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Common Sense Reasoning\", \"Dataset\": \"Xplaint\", \"Metric\": \"Accuracy\", \"Score\": \"88.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Common Sense Reasoning\", \"Dataset\": \"Xplaint\", \"Metric\": \"F1\", \"Score\": \"80.7\"}}, { 'LEADERBO"
    },
    {
        "Context": "Generating Function For Network Delay In this paper correspondence between experimental data for packet delay and two theoretical types of distribution is investigated. Statistical tests have shown that only exponential distribution can be used for the description of packet delays in global network. Precision experimental data to within microseconds are gathered by means of the RIPE Test Box. Statistical verification of hypothesis has shown that distribution parameters remain constants during 500 second intervals at least. In paper cumulative distribution function and generating function for packet delay in network are in an explicit form written down, the algorithm of search of parameters of distribution is resulted.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "You Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal Action Localization Spatiotemporal action localization requires the incorporation of two sources of information into the designed architecture: (1) temporal information from the previous frames and (2) spatial information from the key frame. Current stateof-the-art approaches usually extract these information with separate networks and use an extra mechanism for fusion to get detections. In this work, we present YOWO, a unified CNN architecture for real-time spatiotemporal action localization in video streams. YOWO is a single-stage architecture with two branches to extract temporal and spatial information concurrently and predict bounding boxes and action probabilities directly from video clips in one evaluation. Since the whole architecture is unified, it can be optimized end-to-end. The YOWO architecture is fast providing 34 frames-per-second on 16-frames input clips and 62 frames-per-second on 8-frames input clips, which is currently the fastest stateof-the-art architecture on spatiotemporal action localization task. Remarkably, YOWO outperforms the previous state-of-the art results on J-HMDB-21 and UCF101-24 with an impressive improvement of \u223c3% and \u223c12%, respectively.We make our code and pretrained models publicly available 2 . To evaluate YOWO's performance, two popular and challenging action detection datasets, UCF101-24 and J-HMDB-21 are selected We follow the official evaluation metrics strictly to report the results and compare the performance of our method with the state of the art UCF101-24 is a subset of UCF101, which is originally an action recognition dataset of realistic action videos J-HMDB-21 is a subset of the HMDB-51 dataset and consists of 928 short videos with 21 action categories in daily life Evaluation metrics: We employ two popular metrics used by the most researches in the region of spatio-temporal action detection to generate convincing evaluations Table 1: Frame-mAP @ IoU 0.5 results on datasets UCF101-24 and J-HMDB-21 for different UCF101 - 24 J - HMDB - 21 Table 3: Frame-mAP @ IoU 0.5 results on datasets UCF101-24 and J-HMDB-21 for different UCF101 - 24 J - HMDB - 21 Table 4: Performance comparison on datasets for different 3D backbones UCF101-24 and GFLOPs UCF101 - 24 Model J - HMDB - 21 Table 5: Performance on dataset J-HMDB-21 and comparison with SOTA results by frame- - Frame - mAP Video - mAP Table 6: Performance on dataset UCF101-24 and comparison with SOTA results by frame- - Frame - mAP Video - mAP Table 7: Run time and performance comparison on dataset UCF101-24 for F-mAP and V- V - mAP F - mAP",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Temporal Action Localization\", \"Dataset\": \"UCF101-24\", \"Metric\": \"Frame-mAP\", \"Score\": \"87.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Temporal Action Localization\", \"Dataset\": \"UCF101-24\", \"Metric\": \"Video-mAP 0.5\", \"Score\": \"48.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Temporal Action Localization\", \"Dataset\": \"J-HMDB-21\", \"Metric\": \"Frame-mAP\", \"Score\": \"74.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Temporal Action Localization\", \"Dataset\": \"J-HMDB-21\", \"Metric\": \"Video-mAP 0.2\", \"Score\": \"87.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Temporal Action Localization\", \"Dataset\": \"J-HMDB-21\", \"Metric\": \"Video-mAP 0.5\", \"Score\": \"85.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Temporal Action Localization\", \"Dataset\": \"J-HMDB-21\", \"Metric\": \"Video-mAP 0.75\", \"Score\": \"58.1\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Temporal Action Localization\", \"Dataset\": \"UCF101-24\", \"Metric\": \"Frame-mAP\", \"Score\": \"69.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Temporal Action Localization\", \"Dataset\": \"J-HMDB-21\", \"Metric\": \"Frame-mAP\", \"Score\": \"65.7\"}} ]"
    },
    {
        "Context": "Index Networks We show that existing upsampling operators can be unified using the notion of the index function. This notion is inspired by an observation in the decoding process of deep image matting where indices-guided unpooling can often recover boundary details considerably better than other upsampling operators such as bilinear interpolation. By viewing the indices as a function of the feature map, we introduce the concept of 'learning to index', and present a novel index-guided encoder-decoder framework where indices are learned adaptively from data and are used to guide downsampling and upsampling stages, without extra training supervision. At the core of this framework is anew learnable module, termed Index Network (IndexNet), which dynamically generates indices conditioned on the feature map. IndexNet can be used as a plug-in applicable to almost all convolutional networks that have coupled downsampling and upsampling stages, enabling the networks to dynamically capture variations of local patterns. In particular, we instantiate, investigate five families of IndexNet, highlight their superiority in delivering spatial information over other upsampling operators with experiments on synthetic data, and demonstrate their effectiveness on four dense prediction tasks, including image matting, image denoising, semantic segmentation, and monocular depth estimation. Code and models are available at: https://git.io/IndexNet. Results on the Composition-1k testing dataset are listed in Other implementation details and evaluations are kept consistent with Compared Only HIN ('Nonlinear+Context') is evaluated due to varied feature dimensionality of decoder and multi-level feature fusion Performance of Image Reconstruction on the Fashion - MNIST Dataset MAE PSNR SSIM TABLE 3 MSE Ablation Study of Design Choices SAD Conn TABLE 4 MSE Grad Modelwise O2O DINs M2O DINs Shared Stagewise O2O DINs HINs Unshared Stagewise O2O DINs Results on the Composition - 1k Testing Set GFLOPs \u2206 SAD Conn #Param . MSE TABLE 5 Grad Ablation Study of Different Normalization Choices on Index Maps SAD TABLE 6 Conn MSE Grad Modelwise O2O DINs M2O DINs Shared Stagewise O2O DINs HINs Unshared Stagewise O2O DINs Average PSNR ( dB ) and SSIM Results of Various Noise Levels on the BSD68 and Set12 Image Denoising Benchmarks 31 . 20 / 0 . 9365 31 . 22 / 0 . 9366 GFLOPs 31 . 17 / 0 . 9366 31 . 22 / 0 . 9365 15 \u2206 31 . 25 / 0 . 9368 #Param . 30 . 87",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Monocular Depth Estimation\", \"Dataset\": \"NYU-Depth V2\", \"Metric\": \"RMSE\", \"Score\": \"0.565\"}}, {\"LEADERBOARD\": {\"Task\": \"Scene Segmentation\", \"Dataset\": \"SUN-RGBD\", \"Metric\": \"Mean IoU\", \"Score\": \"33.48\"}}, {\"LEADERBOARD\": {\"Task\": \"Grayscale Image Denoising\", \"Dataset\": \"Set12 sigma15\", \"Metric\": \"PSNR\", \"Score\": \"32.82\"}}, {\"LEADERBOARD\": {\"Task\": \"Grayscale Image Denoising\", \"Dataset\": \"BSD68 sigma50\", \"Metric\": \"PSNR\", \"Score\": \"26.34\"}}, {\"LEADERBOARD\": {\"Task\": \"Grayscale Image Denoising\", \"Dataset\": \"BSD68 sigma25\", \"Metric\": \"PSNR\", \"Score\": \"29.06\"}}, {\"LEADERBOARD\": {\"Task\": \"Grayscale Image Denoising\", \"Dataset\": \"Set12 sigma30\", \"Metric\": \"PSNR\", \"Score\": \"30.43\"}}, {\"LEADERBOARD\": {\"Task\": \"Grayscale Image Denoising\", \"Dataset\": \"Set12 sigma50\", \"Metric\": \"PSNR\", \"Score\": \"27.29\"}}, {\"LEADERBOARD\": {\"Task\": \"Grayscale Image Denoising\", \"Dataset\": \"BSD68 sigma15\", \"Metric\": \"PSNR\", \"Score\": \"31.23\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"S3DIS Area5\", \"Metric\":\"mIoU\", \"Score\": \"61.28\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"S3DIS Area5\", \"Metric\":\"mIoU\", \"Score\": \"59.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"73.66%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"93.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"73.82%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"67.39%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"70.93%\"}}, { "
    },
    {
        "Context": "Simple implementation of deletion from open-address hash table",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Adversarial Autoencoders In this paper, we propose the \"adversarial autoencoder\" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks. On the MNIST dataset we use the sigmoid activation function in the last layer of the autoencoder and on the TFD dataset we use the linear activation function On the Toronto Face dataset, data points are subtracted by the mean and divided by the standard deviation along each input dimension across the whole training set to normalize the contrast Table 1: Log-likelihood of test data on MNIST and Toronto Face dataset. Higher values are better. On both datasets we report the Parzen window estimate of the log-likelihood obtained by drawing 10K or 10M samples from the trained model. For MNIST, we compare against other models on the real-valued version of the dataset. MNIST ( 10K ) - 2057 \u00b1 26 MNIST ( 10M ) Table 2: Semi-supervised classification performance (error-rate) on MNIST and SVHN. - - MNIST ( All ) SVHN ( 1000 )",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Unsupervised MNIST\", \"Dataset\": \"MNIST\", \"Metric\": \"Accuracy\", \"Score\": \"95.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Unsupervised Image Classification\", \"Dataset\": \"MNIST\", \"Metric\": \"Accuracy\", \"Score\": \"95.9\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"MNIST, 250 Labels\", \"Metric\": \"Accuracy\", \"Score\": \"94.7\"}}, { \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"MNIST, 1000 Labels\", \"Metric\": \"Accuracy\", \"Score\": \"91.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Semi-Supervised Image Classification\", \"Dataset\": \"MNIST, 4000 Labels\", \"Metric\": \"Accuracy\", \"Score\": \"95.8\"}} ]"
    },
    {
        "Context": "Distilling Translations with Visual Awareness Previous work on multimodal machine translation has shown that visual information is only needed in very specific cases, for example in the presence of ambiguous words where the textual context is not sufficient. As a consequence, models tend to learn to ignore this information. We propose a translate-and-refine approach to this problem where images are only used by a second stage decoder. This approach is trained jointly to generate a good first draft translation and to improve over this draft by (i) making better use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context. This approach leads to the state of the art results. Additionally, we show that it has the ability to recover from erroneous or missing words in the source language. We build and test our MMT models on the Multi30K dataset The dataset contains 29,000 instances for training, 1,014 for development, and 1,000 for test In addition to the official Multi30K test set (test 2016), we also use the test set from the latest WMT evaluation competition, test 2018 Table 1. We note that RND and PERS are the same for lan- guage pairs as the degradation only depends on the source side, while for AMB the words replaced de- pend on the target language. 2 avg . blanks per sent . Table 1: Statistics of datasets after applying source degradation strategies 2 avg . blanks per sent . Table 2: Results for the test sets 2016 and 2018. M denotes METEOR, B -BLEU; * marks statistically sig- nificant changes for METEOR (p-value \u2264 0.05) as com- pared to base, \u2020 -as compared to del. Bold high- lights statistically significant improvements. We report previous state of the art results for multimodal models from until convergence . test 2018 B and train the model test 2016 M - Table 3: Human ranking results: normalised rank (micro-averaged). Bold highlights best results. base+att del+obj del Table 4: Results for the test sets 2016",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Multimodal Machine Translation\", \"Dataset\": \"Multi30K\", \"Metric\": \"BLEU (EN-DE)\", \"Score\": \"38\"}}, {\"LEADERBOARD\": {\"Task\": \"Multimodal Machine Translation\", \"Dataset\": \"Multi30K\", \"Metric\": \"Meteor (EN-DE)\", \"Score\": \"55.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Multimodal Machine Translation\", \"Dataset\": \"Multi30K\", \"Metric\": \"BLEU (EN-FR)\", \"Score\": \"60.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Multimodal Machine Translation\", \"Dataset\": \"Multi30K\", \"Metric\": \"Meteor (EN-FR)\", \"Score\": \"74.6\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Multimodal Machine Translation\", \"Dataset\": \"Multi30K\", \"Metric\": \"BLEU (EN-DE)\", \"Score\": \"34.0\"}}, { \"LEADERBOARD\": { \"Task\": \"Multimodal Machine Translation\", \"Dataset\": \"Multi30K\", \"Metric\": \"Meteor (EN-DE)\", \"Score\": \"55.1\"}} ]"
    },
    {
        "Context": "A graph polynomial for independent sets of bipartite graphs We introduce anew graph polynomial that encodes interesting properties of graphs, for example, the number of matchings and the number of perfect matchings. Most importantly, for bipartite graphs the polynomial encodes the number of independent sets (#BIS).We analyze the complexity of exact evaluation of the polynomial at rational points and show that for most points exact evaluation is #P-hard (assuming the generalized Riemann hypothesis) and for the rest of the points exact evaluation is trivial.We conjecture that a natural Markov chain can be used to approximately evaluate the polynomial fora range of parameters. The conjecture, if true, would imply an approximate counting algorithm for #BIS, a problem shown, by [10], to be complete (with respect to, so called, AP-reductions) fora rich logically defined sub-class of #P. We give a mild support for our conjecture by proving that the Markov chain is rapidly mixing on trees. As a by-product we show that the \"single bond flip\" Markov chain for the random cluster model is rapidly mixing on constant tree-width graphs. x, y except when The second part of Theorem 2 will be proved by reducing from exact evaluation of the Tutte polynomial Assuming GRH, by Lemma 3 and Theorem 4, we have that exact evaluation of R \u2032 2 at rational point (\u03bb, \u00b5) is #P-hard when \u03bb \u2208 {0, 1/2, 1} and \u00b5 = 0 To show #P-hardness of exact evaluation of R \u2032 2 (G; 1/2, \u00b5) for \u00b5 / \u2208 {\u22121, 0}, we prove a connection between R \u2032 2 and the \"permissive version of #BIS\" (#PBIS) introduced in; #PBIS is a generalization of #BIS where the weight of a set of vertices is determined by the number of pairs of neighboring vertices that are both in the set (in #BIS the weight is zero raised to the number of such pairs)",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "A Formal Verification Methodology for Checking Data Integrity Formal verification techniques have been playing an important role in pre-silicon validation processes. One of the most important points considered in performing formal verification is to define good verification scopes; we should define clearly what to be verified formally upon designs under tests. We considered the following three practical requirements when we defined the scope of formal verification. They are (a) hard to verify (b) small to handle, and (c) easy to understand.Our novel approach is to breakdown generic properties for system into stereotype properties in block level and to define requirements for Verifiable RTL. Consequently, each designer instead of verification experts can describe properties of the design easily, and formal model checking can be applied systematically and thoroughly to all the leaf modules.During the development of a component chip for server platforms, we focused on RAS (Reliability, Availability, and Serviceability) features and described more than 2000 properties in PSL. As a result of the formal verification, we found several critical logic bugs in a short time with limited resources, and successfully verified all of them. This paper presents a study of the functional verification methodology.  Table 2. Number of verified properties 355 Type of Property P0 0 Sub P1 P2 P3 # of 137 6 Type of Property 150 Table 2 . Number of verified properties Table 3. Classification of logic bugs 355 Type of Property P0 0 Sub P1 P2 P3 # of 137 6 Type of Property 150 Table 2 . Number of verified properties",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "FASTER Recurrent Networks for Efficient Video Classification Typical video classification methods often divide a video into short clips, do inference on each clip independently, then aggregate the clip-level predictions to generate the video-level results. However, processing visually similar clips independently ignores the temporal structure of the video sequence, and increases the computational cost at inference time. In this paper, we propose a novel framework named FASTER, i.e., Feature Aggregation for Spatio-TEmporal Redundancy. FASTER aims to leverage the redundancy between neighboring clips and reduce the computational cost by learning to aggregate the predictions from models of different complexities. The FASTER framework can integrate high quality representations from expensive models to capture subtle motion information and lightweight representations from cheap models to cover scene changes in the video. A new recurrent network (i.e., FAST-GRU) is designed to aggregate the mixture of different representations. Compared with existing approaches, FASTER can reduce the FLOPs by over 10\u00d7 while maintaining the state-of-the-art accuracy across popular datasets, such as Kinetics, UCF-101 and HMDB-51. In this section, we describe the experimental setups, i.e., the datasets, training and test protocols for both the cliplevel backbones and FASTER framework.: Accuracy of clip-level backbones on Kinetics Datasets We choose the Kinetics dataset as the major testbed for FASTER Kinetics is among the most popular datasets for video classification To simplify, all reported results on Kinetics are trained from scratch, without pretraining on other datasets (e.g., Sports1M or ImageNet) These datasets are much smaller, thus we use Kinetics for pretraining and report mean accuracy on three testing splits Table 1: Clip-level backbones for extracting the expen- sive and lightweight representations. The FLOPs of R(2+1)D-50 is about 10\u00d7 of R2D-26. R2D : L R ( 2+1 ) D : L\u00d756\u00d756 \uf8f9 1\u00d71\u00d71 , 1024 1\u00d73\u00d73 , 1152 pool1 3\u00d71\u00d71 , 64 R ( 2+1 ) D - 50 3\u00d71\u00d71 , 64 , stride 1 , 1 , 1 8 \u00d7112\u00d7112 1\u00d73\u00d73 , 64 8 \u00d756\u00d756 conv1 1\u00d71\u00d71 , 1024 stride 1 , 2 , 2 8\u00d77\u00d77 , 64 R2D - 26 \uf8ee \uf8ef \uf8f0 stride 8 , 2 , 2 1\u00d77\u00d77 , 45 , stride 1 , 2 , 2 1\u00d73\u00d73 max \uf8fa \uf8fb \u00d73 \u00d74 1\u00d71\u00d71 , 64 \u00d76 Table 3: Comparison of different architectures for ag- gregation on Kinetics. The clip length L is 8. For all the methods, only the first clip is processed by the expen- sive model, and the remaining clips are processed by the",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@1\", \"Score\": \"75.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@1\", \"Score\": \"71.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"UCF101\", \"Metric\": \"3-fold Accuracy\", \"Score\": \"96.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Action Recognition\", \"Dataset\": \"HMDB-51\", \"Metric\": \"Average accuracy of 3 splits\", \"Score\": \"75.7\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@1\", \"Score\": \"75.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@1\", \"Score\": \"81.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@1\", \"Score\": \"71.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@1\", \"Score\": \"83.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@1\", \"Score\": \"77.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vid acc@1\", \"Score\": \"82.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Action Classification\", \"Dataset\": \"Kinetics-400\", \"Metric\": \"Vi"
    },
    {
        "Context": "WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context We present WiC-TSV, anew multi-domain evaluation benchmark for Word Sense Disambiguation. More specifically, we introduce a framework for Target Sense Verification of Words in Context which grounds its uniqueness in the formulation as binary classification task thus being independent of external sense inventories, and the coverage of various domains. This makes the dataset highly flexible for the evaluation of a diverse set of models and systems in and across domains. WiC-TSV provides three different evaluation settings, depending on the input signals provided to the model. We set baseline performance on the dataset using state-of-the-art language models. Experimental results show that even though these models can perform decently on the task, there remains a gap between machine and human performance, especially in out-ofdomain settings. WiC-TSV data is available at https://competitions.codalab. org/competitions/23683 In this section we detail the construction of the dataset In this section we evaluate the performance of different baseline models on our WiC-TSV benchmark Table 2: Statistics of training, development and testing splits of WiC-TSV, including total number of instances (Total), unique number of target words (N w ) and per- centage of positive instances (R + ). ( WNT / WKT ) ( MSH+CTL+CPS ) R w Total + N Table 3: Average human accuracy for native English annotators, on different subsets of the dataset: general purpose, i.e., WNT/WKT, and the domain specific, i.e., MSH, CTL, and CPS. Human Perf . Table 4: Test set performance of the baseline models on WiC-TSV, in terms of accuracy, precision, recall, and F1, on the three different tasks. Baseline True is a naive baseline that always returns True and the human performance is computed as described in Section 3.4. Task - 3 Task - 2 Task - 1 Acc Rec WiC - TSV Prec F1 Table 6: F1 score for the in-domain few-shot analy- sis (Task-3) using",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: all\", \"Score\": \"75.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: domain specific\", \"Score\": \"77.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: general purpose\", \"Score\": \"73.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: all\", \"Score\": \"71.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: domain specific\", \"Score\": \"74.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: general purpose\", \"Score\": \"68.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: all\", \"Score\": \"76.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: domain specific\", \"Score\": \"80.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: general purpose\", \"Score\": \"73.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: all\", \"Score\": \"54.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: domain specific\", \"Score\": \"60.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: general purpose\", \"Score\": \"49.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: all\", \"Score\": \"62.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: domain specific\", \"Score\": \"69.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: general purpose\", \"Score\": \"57.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: all\", \"Score\": \"60.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: domain specific\", \"Score\": \"67.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: general purpose\", \"Score\": \"54.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: all\", \"Score\": \"53.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: domain specific\", \"Score\": \"50.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: general purpose\", \"Score\": \"56.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: all\", \"Score\": \"52.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: domain specific\", \"Score\": \"47.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: general purpose\", \"Score\": \"56.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: all\", \"Score\": \"53.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: domain specific\", \"Score\": \"49.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: general purpose\", \"Score\": \"57.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: all\", \"Score\": \"50.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: domain specific\", \"Score\": \"47.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: general purpose\", \"Score\": \"53.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: all\", \"Score\": \"50.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: domain specific\", \"Score\": \"47.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: general purpose\", \"Score\": \"53.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: all\", \"Score\": \"50.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: domain specific\", \"Score\": \"47.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: general purpose\", \"Score\": \"53.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: all\", \"Score\": \"85.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: domain specific\", \"Score\": \"89.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: general purpose\", \"Score\": \"82.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: all\", \"Score\": \"75.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: domain specific\", \"Score\": \"77.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: general purpose\", \"Score\": \"73.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: all\", \"Score\": \"71.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: domain specific\", \"Score\": \"74.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: general purpose\", \"Score\": \"68.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: all\", \"Score\": \"76.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: domain specific\", \"Score\": \"80.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: general purpose\", \"Score\": \"73.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: all\", \"Score\": \"54.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: domain specific\", \"Score\": \"60.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: general purpose\", \"Score\": \"49.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: all\", \"Score\": \"62.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: domain specific\", \"Score\": \"69.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: general purpose\", \"Score\": \"57.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: all\", \"Score\": \"60.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: domain specific\", \"Score\": \"67.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: general purpose\", \"Score\": \"54.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: all\", \"Score\": \"53.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: domain specific\", \"Score\": \"50.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: general purpose\", \"Score\": \"56.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: all\", \"Score\": \"52.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: domain specific\", \"Score\": \"47.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: general purpose\", \"Score\": \"56.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: all\", \"Score\": \"53.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: domain specific\", \"Score\": \"49.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: general purpose\", \"Score\": \"57.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: all\", \"Score\": \"50.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: domain specific\", \"Score\": \"47.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 1 Accuracy: general purpose\", \"Score\": \"53.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: all\", \"Score\": \"50.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: domain specific\", \"Score\": \"47.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 2 Accuracy: general purpose\", \"Score\": \"53.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: all\", \"Score\": \"50.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: domain specific\", \"Score\": \"47.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: general purpose\", \"Score\": \"53.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: all\", \"Score\": \"85.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: domain specific\", \"Score\": \"89.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Entity Linking\", \"Dataset\": \"WiC-TSV\", \"Metric\": \"Task 3 Accuracy: general purpose\", \"Score\": \"82.1\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WC-TSV\", \"Metric\": \"F1\", \"Score\": \"77.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WC-TSV\", \"Metric\": \"F1\", \"Score\": \"77.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WC-TSV\", \"Metric\": \"F1\", \"Score\": \"77.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WC-TSV\", \"Metric\": \"F1\", \"Score\": \"77.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WC-TSV\", \"Metric\": \"F1\", \"Score\": \"87.5\"}}, { \"LEADERBOARD\": { \"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WC-TSV\", \"Metric\": \"F1\", \"Score\": \"83.6\"}}, { \"LEADERBOARD\": { \"Task\": \"Word Sense Disambiguation\", \"Dataset\": \"WC-TSV\", \"Metric\": \""
    },
    {
        "Context": "A Novel Model for Optimized GSM Network Design GSM networks are very expensive. The network design process requires too many decisions in a combinatorial explosion. For this reason, the larger is the network, the harder is to achieve a totally human based optimized solution. The BSC (Base Station Control) nodes have to be geographically well allocated to reduce the transmission costs. There are decisions of association between BTS and BSC those impacts in the correct dimensioning of these BSC. The choice of BSC quantity and model capable of carrying the cumulated traffic of its affiliated BTS nodes in turn reflects on the total cost. In addition, the last component of the total cost is due to transmission for linking BSC nodes to MSC. These trunks have a major significance since the number of required E1 lines is larger than BTS to BSC link. This work presents an integer programming model and a computational tool for designing GSM (Global System for Mobile Communications) networks, regarding BSS (Base Station Subsystem) with optimized cost.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Quaternion Knowledge Graph Embeddings In this work, we move beyond the traditional complex-valued representations, introducing more expressive hypercomplex representations to model entities and relations for knowledge graph embeddings. More specifically, quaternion embeddings, hypercomplex-valued embeddings with three imaginary components, are utilized to represent entities. Relations are modelled as rotations in the quaternion space. The advantages of the proposed approach are: (1) Latent inter-dependencies (between all components) are aptly captured with Hamilton product, encouraging a more compact interaction between entities and relations; (2) Quaternions enable expressive rotation in four-dimensional space and have more degree of freedom than rotation in complex plane; (3) The proposed framework is a generalization of ComplEx on hypercomplex space while offering better geometrical interpretations, concurrently satisfying the key desiderata of relational representation learning (i.e., modeling symmetry, anti-symmetry and inversion). Experimental results demonstrate that our method achieves state-of-the-art performance on four wellestablished knowledge graph completion benchmarks.  Table 2: Statistics of the data sets used in this paper. avg . #degree #training #test #validation N Table 3: Link prediction results on WN18 and FB15K. Best results are in bold and second best results are underlined. [ \u2020]: Results are taken from [Nickel et al., 2016]; [ ]: Results are taken from [Kadlec et al., 2017]; [ * ]: Results are taken from [Sun et al., 2019]. a-RotatE denotes RotatE with self-adversarial negative sampling. [QuatE 1 ]: without type constraints; [QuatE 2 ]: with N3 regularization and reciprocal learning; [QuatE 3 ]: with type constraints. 1 2 FB15K MR WN18 Hit@10 MRR Hit@1 - Hit@3 Table 4: Link prediction results on WN18RR and FB15K-237. [ \u2020]: Results are taken from [Nguyen et al., 2017]; [ ]: Results are taken from [Dettmers et al., 2018]; [ * ]: Results are taken from [Sun et al., 2019]. Hit@10 WN18RR FB15K -",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18\", \"Metric\": \"Hits@1\", \"Score\": \"0.945\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18\", \"Metric\": \"Hits@10\", \"Score\": \"0.959\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18\", \"Metric\": \"Hits@3\", \"Score\": \"0.954\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18\", \"Metric\": \"MR\", \"Score\": \"162\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18\", \"Metric\": \"MRR\", \"Score\": \"0.95\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k\", \"Metric\": \"Hits@1\", \"Score\": \"0.800\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k\", \"Metric\": \"Hits@10\", \"Score\": \"0.900\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k\", \"Metric\": \"Hits@3\", \"Score\": \"0.859\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k\", \"Metric\": \"MR\", \"Score\": \"17\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k\", \"Metric\": \"MRR\", \"Score\": \"0.833\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@1\", \"Score\": \"0.438\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@10\", \"Score\": \"0.582\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@3\", \"Score\": \"0.508\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"MR\", \"Score\": \"2314\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"MRR\", \"Score\": \"0.488\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"Hits@1\", \"Score\": \"0.248\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"Hits@10\", \"Score\": \"0.550\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"Hits@3\", \"Score\": \"0.382\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"MR\", \"Score\": \"87\"}}, {\"LEADERBOARD\": {\"Task\": \"Link Prediction\", \"Dataset\": \"FB15k-237\", \"Metric\": \"MRR\", \"Score\": \"0.348\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18\", \"Metric\": \"Hits@1\", \"Score\": \"0.957\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18\", \"Metric\": \"Hits@10\", \"Score\": \"0.947\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18\", \"Metric\": \"Hits@3\", \"Score\": \"0.963\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18\", \"Metric\": \"MRR\", \"Score\": \"0.963\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@1\", \"Score\": \"0.452\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@10\", \"Score\": \"0.582\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction\", \"Dataset\": \"WN18RR\", \"Metric\": \"Hits@3\", \"Score\": \"0.516\"}}, { \"LEADERBOARD\": { \"Task\": \"Link Prediction"
    },
    {
        "Context": "ELECTRAMED: A NEW PRE-TRAINED LANGUAGE REPRESENTATION MODEL FOR BIOMEDICAL NLP The overwhelming amount of biomedical scientific texts calls for the development of effective language models able to tackle a wide range of biomedical natural language processing (NLP) tasks. The most recent dominant approaches are domain-specific models, initialized with general-domain textual data and then trained on a variety of scientific corpora. However, it has been observed that for specialized domains in which large corpora exist, training a model from scratch with just in-domain knowledge may yield better results. Moreover, the increasing focus on the compute costs for pre-training recently led to the design of more efficient architectures, such as ELECTRA. In this paper, we propose a pre-trained domain-specific language model, called ELECTRAMed, suited for the biomedical field. The novel approach inherits the learning framework of the general-domain ELECTRA architecture, as well as its computational advantages. Experiments performed on benchmark datasets for several biomedical NLP tasks support the usefulness of ELECTRAMed, which sets the novel state-of-the-art result on the BC5CDR corpus for named entity recognition, and provides the best outcome in 2 over the 5 runs of the 7th BioASQ-factoid Challange for the question answering task.Keywords Pre-trained language models \u00b7 ELECTRA \u00b7 Biomedical NLP * Corresponding authors.While ELMo and BERT architectures pre-trained on general-domain corpora are well-established top performers for general NLP tasks, they might yield poor results in case of scientific or specific domains, since the corpora used for pre-training, such as news articles and Wikipedia [3], might not include the same terminology adopted in the indomain tasks. For specialized contexts past studies showed that general-domain language models can largely benefit from the use of in-domain textual data [4]. As a consequence, recent models for biomedical NLP relied on adapted versions of general-domain approaches. Among these, two of the most noteworthy and successful examples are represented by BioBERT [5]  and BlueBERT [4], which are domain-specific language models initialized with the general-domain BERT, and then pre-trained on a wide range of biomedical and scientific corpora. In principle, these last methods rely on the assumption that initializing the pre-training from general-domain models might improve the overall performance for domain-specific purposes. However, it has been recently observed that for domains in which large corpora exist, like the biomedical field, pre-training language models from scratch yields better results than feeding the pre-training phase with general-domain knowledge [6].To obtain contextualized word embeddings, BERT pre-training is based on masked language modelling (MLM) which aims at predicting a small random subset of masked input tokens, considering only the token context. This approach allows the model to learn bidirectional representations. A different input corruption procedure has been recently proposed, in which instead of masking, and therefore losing, some of the input tokens, these are replaced with plausible alternatives produced by a small generator network. By learning from all the input tokens this novel approach, called ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately), is computationally much more efficient than BERT, and has been shown to outperform the latter in several tasks [7].  Table 1: Hyperparameters used for ELECTRAMed pre-training 1 / 3 2e - 4 Value 10000 768 1e - 6 Linear Table 1 : Hyperparameters used for ELECTRAMed pre - training 3072 Table 2: Description of the corpora used as benchmarks for biomedical NER 5 , 006 CPR : 4 1 , 212 effect 35 , 336 proteins 4 , 409 chemicals N . relations 10 , 028 TOT 10 , 227 TOT N . val 2 , 937 TOT Table 2 : Description of the corpora used as benchmarks for biomedical NER N . test 1 , 983 CPR : 3 N . entities 59 , 963 TOT Table 3: Description of the corpora used as benchmarks for biomedical RE 5 , 006 CPR : 4 1 , 212 effect 35 , 336 proteins 4 , 409 chemicals N . relations 10 , 028 TOT 10 , 227 TOT N .",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Relation Extraction\", \"Dataset\": \"ChemProt\", \"Metric\": \"F1\", \"Score\": \"72.94\"}}, {\"LEADERBOARD\": {\"Task\": \"Drug\u2013drug Interaction Extraction\", \"Dataset\": \"DDI extraction 2013 corpus\", \"Metric\": \"Micro F1\", \"Score\": \"79.13\"}}, {\"LEADERBOARD\": {\"Task\": \"Named Entity Recognition\", \"Dataset\": \"NCBI-disease\", \"Metric\": \"F1\", \"Score\": \"87.54\"}}, {\"LEADERBOARD\": {\"Task\": \"Named Entity Recognition\", \"Dataset\": \"BC5CDR\", \"Metric\": \"F1\", \"Score\": \"90.03\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Natural Language Inference\", \"Dataset\": \"MultiNLI\", \"Metric\": \"Matched\", \"Score\": \"84.0\"}}, { \"LEADERBOARD\": { \"Task\": \"Natural Language Inference\", \"Dataset\": \"MultiNLI\", \"Metric\": \"Mismatched\", \"Score\": \"84.0\"}}, { \"LEADERBOARD\": { \"Task\": \"Natural Language Inference\", \"Dataset\": \"SNLI\", \"Metric\": \"% Test Accuracy\", \"Score\": \"90.0\"}}, { \"LEADERBOARD\": { \"Task\": \"Natural Language Inference\", \"Dataset\": \"SciTail\", \"Metric\": \"Accuracy\", \"Score\": \"89.9%\"}}, { \"LEADERBOARD\": { \"Task\": \"Natural Language Inference\", \"Dataset\": \"WNLI\", \"Metric\": \"Accuracy\", \"Score\": \"94.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Natural Language Inference\", \"Dataset\": \"QNLI\", \"Metric\": \"Accuracy\", \"Score\": \"90.5%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Textual Similarity\", \"Dataset\": \"STS Benchmark\", \"Metric\": \"Pearson Correlation\", \"Score\": \"0.9"
    },
    {
        "Context": "Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing Despite the noticeable progress in perceptual tasks like detection, instance segmentation and human parsing, computers still perform unsatisfactorily on visually understanding humans in crowded scenes, such as group behavior analysis, person re-identification and autonomous driving, etc. To this end, models need to comprehensively perceive the semantic information and the differences between instances in a multi-human image, which is recently defined as the multi-human parsing task. In this paper, we present anew large-scale database \"Multi-Human Parsing (MHP)\" for algorithm development and evaluation, and advances the state-of-the-art in understanding humans in crowded scenes. MHP contains 25,403 elaborately annotated images with 58 fine-grained semantic category labels, involving 2-26 persons per image and captured in real-world scenes from various viewpoints, poses, occlusion, interactions and background. We further propose a novel deep Nested Adversarial Network (NAN) model for multi-human parsing. NAN consists of three Generative Adversarial Network (GAN)-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. These sub-nets form a nested structure and are carefully designed to learn jointly in an end-to-end way. NAN consistently outperforms existing state-of-the-art solutions on our MHP and several other datasets, and serves as a strong baseline to drive the future research for multi-human parsing. In total, there are 25,403 images in the MHP v2.0 dataset data distribution on 59 semantic categories, the average semantic category number per image and the average instance number per image in the MHP v2.0 dataset are illustrated in (a), (b) and (c), respectively The images in the MHP v2.0 dataset contain diverse instance numbers, viewpoints, poses, occlusion, interactions and background complexities We evaluate NAN qualitatively and quantitatively under various settings and granularities for understanding humans in crowded scenes In particular, we evaluate multihuman parsing performance on the MHP v2.0 dataset proposed in this work, as well as the MHP v1.0 and PASCAL-Person-Part benchmark datasets We also evaluate instance-agnostic parsing and instance-aware clustering results on the Buffy benchmark dataset, which are byproducts of NAN Following, we use the Average Precision based on part (AP p ) and Percentage of Correctly parsed semantic Parts (PCP) metrics for multi-human parsing evaluation We further Table 1: Statistics for publicly available human parsing datasets. 56 13 19 Datasets 10 , 000 # Testing PASCAL - Person - Part [ 4 ] # Training 1 , 000 LIP [ 17 ] 1 , 817 - # Total # Validation Table 2: Component analysis on the MHP v2.0 validation set. Upperbound diction are already small and have only little effect on the flow consistency , the superiority of which is verified by and Finally , we also semantic saliency prediction sub - net from NAN , leading human - centic processing . The superiority of incorporating process can be verified by comparing w / o Di , i \u2208 {1 , 2 , 3} results from the 2 refines \u2208 parses In particular , w / o G1 refers to truncating the results , refinement whereas different MH - Parser results complexity , the superiority of which is",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Multi-Human Parsing\", \"Dataset\": \"PASCAL-Part\", \"Metric\": \"AP 0.5\", \"Score\": \"59.70%\"}}, {\"LEADERBOARD\": {\"Task\": \"Multi-Human Parsing\", \"Dataset\": \"MHP v2.0\", \"Metric\": \"AP 0.5\", \"Score\": \"25.14%\"}}, {\"LEADERBOARD\": {\"Task\": \"Multi-Human Parsing\", \"Dataset\": \"MHP v1.0\", \"Metric\": \"AP 0.5\", \"Score\": \"57.09%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Multi-Human Parsing\", \"Dataset\": \"MHP v2 test-dev\", \"Metric\": \"Accuracy\", \"Score\": \"46.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Multi-Human Parsing\", \"Dataset\": \"MHP v2 test-std\", \"Metric\": \"overall\", \"Score\": \"49.4\"}} ]"
    },
    {
        "Context": "Adaloss: Adaptive Loss Function for Landmark Localization Landmark localization is a challenging problem in computer vision with a multitude of applications. Recent deeplearning based methods have shown improved results by regressing likelihood maps (i.e. heatmaps) instead of regressing the coordinates directly. However, setting the precision of these regression targets during the training has been a cumbersome process since it creates a trade-off between trainability vs. localization accuracy. Using precise targets introduces a significant sampling bias and hence makes the training more difficult, whereas using imprecise targets results in inaccurate landmark detectors. In this paper, we introduce \"Adaloss\", an objective function that adapts itself during the training by updating the target precision based on the training statistics. This approach does not require setting problem-specific parameters and shows improved stability in training and better localization accuracy in inference. We demonstrate the effectiveness of our proposed method in three very different applications of landmark localization: 1) the challenging task of precisely detecting catheter tips in medical X-ray images 1 , 2) localizing surgical instruments in endoscopic images 1 , and 3) localizing facial features on in-the-wild images where we show state-of-the-art results on the 300-W benchmark dataset. We evaluated our method on three very different landmark localization problems: catheter tip detection in X-ray images (single instance of a single landmark that needs to be localized very precisely), facial feature localization (single instances of multiple landmarks where the achievable precision is landmark-dependent), and surgical instrument Table 1: Euclidian distance on CathDet testing set for multi- ple values of sigma compared to Adaloss. \u03c3 = 60 Adaloss \u03c3 = 5 \u03c3 = 40 \u03c3 = 25 Table 2. Model trained with Adaloss presents a much smaller NTV (2.80 vs 3.03), indicating a potential impact of using Adaloss on filter smoothness. Adaloss Fixed sigma Table 2: Normalized Total Variation (NTV) score with and without Adaloss. 'all' represents the aggregate NTV on all the layers, and 'last' the NTV on the final convolutional layer. Model trained using Adaloss has smoother filters, indicating potential improvement in network stability. Adaloss Fixed sigma Table 3: Normalized mean errors (NME) on the 300-W dataset. Proposed approach shows best results on all testing sets. Challenging Full Common Table 4: Quantitative results of the Adaloss model on dif- ferent testing set of EndoVis: Recall (%) / Precision (%) / Euclidian distance (px). Left Clasper Right",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Facial Landmark Detection\", \"Dataset\": \"300W\", \"Metric\": \"NME\", \"Score\": \"3.31\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Facial Landmark Detection\", \"Dataset\": \"300W\", \"Metric\": \"NME\", \"Score\": \"3.49\"}} ]"
    },
    {
        "Context": "A Surprisingly Robust Trick for the Winograd Schema Challenge The Winograd Schema Challenge (WSC) dataset WSC273 and its inference counterpart WNLI are popular benchmarks for natural language understanding and commonsense reasoning. In this paper, we show that the performance of three language models on WSC273 consistently and robustly improves when finetuned on a similar pronoun disambiguation problem dataset (denoted WSCR). We additionally generate a large unsupervised WSClike dataset. By fine-tuning the BERT language model both on the introduced and on the WSCR dataset, we achieve overall accuracies of 72.5% and 74.7% on WSC273 and WNLI, improving the previous state-of-theart solutions by 8.8% and 9.6%, respectively. Furthermore, our fine-tuned models are also consistently more accurate on the \"complex\" subsets of WSC273, introduced by Trichelair et al. (2018). The model is trained with a single epoch of the MaskedWiki dataset, using batches of size 64 (distributed on 8 GPUs), Adam optimizer, a learning rate of 5.0 \u00b7 10 \u22126 , and hyperparameter val-ues \u03b1 = 20 and \u03b2 = 0.2 in the loss function) Both BERT and BERT WIKI are fine-tuned on the WSCR training dataset to create BERT WSCR and BERT WIKI WSCR The model BERT WIKI WSCR pairs is obtained by fine-tuning BERT WIKI on half of the WSCR dataset This time, all examples in the subset come in pairs, just like in the unreduced WSCR dataset We evaluate all models on WSC273 and the WNLI test dataset, as well as the various subsets of WSC273, as described in Section 2 Firstly, we note that models that are fine-tuned on the WSCR dataset consistently outperform their non-fine-tuned counterparts Secondly, the results of BERT WIKI seem to indicate Table 1: Results on WSC273 and its subsets. The comparison between each language model and its WSCR-tuned model is given. For each column, the better result of the two is in bold. The best result in the column overall is underlined. Results for the LM ensemble and Knowledge Hunter are taken from Trichelair et al. (2018). All models consistently improve their accuracy when fine-tuned on the WSCR dataset. - WSC273 assoc . consist . non - assoc . switched WNLI unswitched",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Common Sense Reasoning\", \"Dataset\": \"Winograd Schema Challenge\", \"Metric\": \"Score\", \"Score\": \"72.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Common Sense Reasoning\", \"Dataset\": \"Winograd Schema Challenge\", \"Metric\": \"Score\", \"Score\": \"70.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Natural Language Understanding\", \"Dataset\": \"WNLI\", \"Metric\": \"Accuracy\", \"Score\": \"71.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Natural Language Understanding\", \"Dataset\": \"WNLI\", \"Metric\": \"Accuracy\", \"Score\": \"70.5\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"WNLI\", \"Metric\": \"Accuracy\", \"Score\": \"74.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"WNLI\", \"Metric\": \"Accuracy\", \"Score\": \"74.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"WNLI\", \"Metric\": \"Accuracy\", \"Score\": \"74.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"WNLI\", \"Metric\": \"Accuracy\", \"Score\": \"74.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"WNLI\", \"Metric\": \"Accuracy\", \"Score\": \"74.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"WNLI\", \"Metric\": \"Accuracy\", \"Score\": \"81.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Sentiment Analysis\", \"Dataset\": \"WNLI\", \"Metric\": \"Accuracy\", \"Score\": \"84.9\"}}, { \"LEADERBOARD\": { \"Task\":"
    },
    {
        "Context": "Maximum \u2206-edge-colorable subgraphs of class II graphs A graph G is class II, if its chromatic index is at least \u2206 + 1. Let H be a maximum \u2206-edge-colorable subgraph of G. The paper proves best possible lower bounds for |E(H)| |E(G)| , and structural properties of maximum \u2206-edge-colorable subgraphs. It is shown that every set of vertex-disjoint cycles of a class II graph with \u2206 \u2265 3 can be extended to a maximum \u2206-edge-colorable subgraph. Simple graphs have a maximum \u2206-edge-colorable subgraph such that the complement is a matching. Furthermore, a maximum \u2206-edgecolorable subgraph of a simple graph is always class I.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Discriminative Adversarial Domain Adaptation Given labeled instances on a source domain and unlabeled ones on a target domain, unsupervised domain adaptation aims to learn a task classifier that can well classify target instances. Recent advances rely on domain-adversarial training of deep networks to learn domain-invariant features. However, due to an issue of mode collapse induced by the separate design of task and domain classifiers, these methods are limited in aligning the joint distributions of feature and category across domains. To overcome it, we propose a novel adversarial learning method termed Discriminative Adversarial Domain Adaptation (DADA). Based on an integrated category and domain classifier, DADA has a novel adversarial objective that encourages a mutually inhibitory relation between category and domain predictions for any input instance. We show that under practical conditions, it defines a minimax game that can promote the joint distribution alignment. Except for the traditional closed set domain adaptation, we also extend DADA for extremely challenging problem settings of partial and open set domain adaptation. Experiments show the efficacy of our proposed methods and we achieve the new state of the art for all the three settings on benchmark datasets. Datasets and Implementation Details) is a popular benchmark domain adaptation dataset consisting of 4, 110 images of 31 categories collected from three domains: Amazon (A), Webcam (W), and DSLR (D) We evaluate on six settings For open set domain adaptation, we evaluate on Syn2Real-O, which includes two domains Implementation Details We follow standard evaluation protocols for unsupervised domain adaptation: we use all labeled source and all unlabeled target instances as the training data For other tasks of Syn2Real, we evaluate the accuracy of each category based on ResNet-101 and ResNet-152 (for closed and open set domain adaptation respectively) Table 1: Ablation studies using Office-31 based on ResNet-50. Please refer to the main text for how they are defined. Avg Table 2: Results for closed set domain adaptation on Office-31 based on ResNet-50. Note that SimNet is implemented by an unknown framework; MADA and DANN-CA are implemented by Caffe; all the other methods are implemented by PyTorch. Avg Table 3: Results for closed set domain adaptation on Syn2Real-C based on ResNet-101. Note that all compared methods are based on PyTorch implementation. plane bus mcycl truck sktbrd horse bcycl knife car person mean plant train Table 4: Results for open set domain adaptation on Syn2Real-O based on ResNet-152. Known indicates the mean classification result over the known categories whereas Mean also includes the unknown category. The table below shows the results when the Known-to-Unknown Ratio in the target domain is set to 1 : 10. All compared methods are based on",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Synthetic-to-Real Translation\", \"Dataset\": \"Syn2Real-C\", \"Metric\": \"Accuracy\", \"Score\": \"79.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Domain Adaptation\", \"Dataset\": \"Office-31\", \"Metric\": \"Average Accuracy\", \"Score\": \"89\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Domain Adaptation\", \"Dataset\": \"Office-31\", \"Metric\": \"Average Accuracy\", \"Score\": \"88.2\"}}, { \"LEADERBOARD\": { \"Task\": \"Domain Adaptation\", \"Dataset\": \"Office-Home\", \"Metric\": \"Accuracy\", \"Score\": \"72.3\"}} ]"
    },
    {
        "Context": "Asynchronous Remote Medical Consultation for Ghana Computer-mediated communication systems can be used to bridge the gap between doctors in underserved regions with local shortages of medical expertise and medical specialists worldwide. To this end, we describe the design of a prototype remote consultation system intended to provide the social, institutional and infrastructural context for sustained, self-organizing growth of a globally-distributed Ghanaian medical community. The design is grounded in an iterative design process that included two rounds of extended design fieldwork throughout Ghana and draws on three key design principles (social networks as a framework on which to build incentives within a self-organizing network; optional and incremental integration with existing referral mechanisms; and a weakly-connected, distributed architecture that allows fora highly interactive, responsive system despite failures in connectivity). We discuss initial experiences from an ongoing trial deployment in southern Ghana.  Table 1. Categorization of analyzed conversational threads. 10 Social Professional Total",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": ":2) Desharnais, Gupta, Jagadeesan and Panangaden introduced a family of behavioural pseudometrics for probabilistic transition systems. These pseudometrics area quantitative analogue of probabilistic bisimilarity. Distance zero captures probabilistic bisimilarity. Each pseudometric has a discount factor, areal number in the interval (0, 1]. The smaller the discount factor, the more the future is discounted. If the discount factor is one, then the future is not discounted at all. Desharnais et al. showed that the behavioural distances can be calculated up to any desired degree of accuracy if the discount factor is smaller than one. In this paper, we show that the distances can also be approximated if the future is not discounted. A key ingredient of our algorithm is Tarski's decision procedure for the first order theory over real closed fields. By exploiting the Kantorovich-Rubinstein duality theorem we can restrict to the existential fragment for which more efficient decision procedures exist.distance is zero if and only if the states are probabilistic bisimilar, a behavioural equivalence introduced by Larsen and Skou [26].The pseudometrics of Desharnais et al. are defined via real-valued interpretations of Larsen and Skou's probabilistic modal logic. Formulae assume truth values in the interval [0, 1]. Conjunction and disjunction are interpreted using the lattice structure of the unit interval. The modality a is interpreted arithmetically by integration. The behavioural distance between states s 1 and s 2 is then defined as the supremum overall formulae \u03d5 of the difference in the truth value of \u03d5 in s 1 and in s 2 . 1 The definition of the behavioural pseudometrics of Desharnais et al. is parametrized by a discount factor \u03b4, areal number in the interval (0, 1]. The smaller the discount factor, the more (behavioural differences in) the future are discounted. In the case that \u03b4 equals one, the future is not discounted. All differences in behaviour, whether in the near or far future, contribute alike to the distance. For systems that (in principle) run forever, we maybe interested in all these differences and, hence, in the pseudometric that does not discount the future.In [16], Desharnais et al. presented an algorithm to approximate the behavioural distances for \u03b4 smaller than one. The first and third author [7] presented also an approximation algorithm for \u03b4 smaller than one.There is a fundamental difference between pseudometrics that discount the future and the one that does not. This is, for example, reflected by the fact that all pseudometrics that discount the future give rise to the same topology, whereas the pseudometric that does not discount the future gives rise to a different topology (see, for example, [18, page 350]). As a consequence, it may not be surprising that neither approximation algorithm mentioned in the previous paragraph can be modified in an obvious way to handle the case that \u03b4 equals one.The main contribution of this paper is an algorithm that approximates behavioural distances in case the discount factor \u03b4 equals one. Starting from the logical definition of the pseudometric by Desharnais et al., we first give a characterisation of the pseudometric as the greatest (post-)fixed point of a functional on a complete lattice [0, 1] S , where S is the set of states of the probabilistic transition system in question. This functional is closely related to the Kantorovich metric [24] on probability measures. Next, we dualize this characterization exploiting the Kantorovich-Rubinstein duality theorem [25]. Subsequently, we show, exploiting the dual characterization, that a pseudometric being a post-fixed point can be expressed in the existential fragment of the first order theory over real closed fields. Based on the fact that this first order theory is decidable, a result due to Tarski [31], we show how to approximate the behavioural distances. Finally, we discuss an implementation of our algorithm in Mathematica.Exploiting the techniques put forward in this paper, we have also developed an algorithm to approximate the behavioural pseudometric that is presented in [3]. The other algorithm can be found in [30].",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Quasi-Dense Similarity Learning for Multiple Object Tracking Similarity learning has been recognized as a crucial step for object tracking. However, existing multiple object tracking methods only use sparse ground truth matching as the training objective, while ignoring the majority of the informative regions on the images. In this paper, we present Quasi-Dense Similarity Learning, which densely samples hundreds of region proposals on a pair of images for contrastive learning. We can directly combine this similarity learning with existing detection methods to build Quasi-Dense Tracking (QDTrack) without turning to displacement regression or motion priors. We also find that the resulting distinctive feature space admits a simple nearest neighbor search at the inference time. Despite its simplicity, QDTrack outperforms all existing methods on MOT, BDD100K, Waymo, and TAO tracking benchmarks. It achieves 68.7 MOTA at 20.3 FPS on MOT17 without using external training data. Compared to methods with similar detectors, it boosts almost 10 points of MOTA and significantly decreases the number of ID switches on BDD100K and Waymo datasets. 1 . We also like to test on larger scale and more diverse datasets to study the efficacy of quasi-dense similarity learning The dataset contains 7 videos (5,316 images) for training and 7 videos (5,919 images) for testing Only pedestrians are evaluated in this benchmark BDD100K We use BDD100K annotates 482 classes in total, which are the subset of LVIS dataset MOT17 with public detectors Following the strategy in Tracktor and CenterTrack, we evaluate our method with public detectors on MOT17 TAO presents detailed results on the TAO dataset Although QDTrack does not perform zero-shot and few-shot learning for the long-tail categories, our method is still a stronger baseline method on this dataset and paves the way for future studies Table 1: Results on MOT16 and MOT17 object tracking benchmark test set. Note that we do not use extra data for training. \u2191 means higher is better, \u2193 means lower is better. * means external data besides COCO and ImageNet is used. MOTA \u2191 IDF1 \u2191 MOTP \u2191 292 250 296 498 246 258 316 735 637 759 816 Table 2: Results on BDD100K tracking validation and test set. mMOTA and mIDF1 are averages over 8 categories while MOTA and IDF1 indicates the overall performance without considering categories. Our method outperforms the champion of BDD100K 2020 Tracking Challenge (madamada) only with a simple single model. mMOTA \u2191 mIDF1 \u2191 mAP \u2191 MOTA \u2191 IDF1 \u2191 - 292063 209339 Table 3: Results on Waymo tracking validation set using py-motmetrics library (top) 2 and test set using official evaluation. * indicates methods using undisclosed detectors. Miss / L2 \u2193 Miss / L1 \u2193",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Multi-Object Tracking\", \"Dataset\": \"MOT17\", \"Metric\": \"IDF1\", \"Score\": \"66.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Multi-Object Tracking\", \"Dataset\": \"MOT17\", \"Metric\": \"MOTA\", \"Score\": \"68.7\"}}, {\"LEADERBOARD\": {\"Task\": \"Multi-Object Tracking\", \"Dataset\": \"MOT16\", \"Metric\": \"IDF1\", \"Score\": \"67.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Multi-Object Tracking\", \"Dataset\": \"MOT16\", \"Metric\": \"MOTA\", \"Score\": \"69.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Multi-Object Tracking\", \"Dataset\": \"BDD100K val\", \"Metric\": \"MOTA\", \"Score\": \"36.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Multiple Object Tracking\", \"Dataset\": \"Waymo Open Dataset\", \"Metric\": \"MOTA\", \"Score\": \"51.18\"}}, {\"LEADERBOARD\": {\"Task\": \"One-Shot Object Detection\", \"Dataset\": \"PASCAL VOC 2012 val\", \"Metric\": \"MAP\", \"Score\": \"22.1\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Multi-Object Tracking\", \"Dataset\": \"MOT17\", \"Metric\": \"MOTA\", \"Score\": \"61.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Multi-Object Tracking\", \"Dataset\": \"MOT16\", \"Metric\": \"MOTA\", \"Score\": \"68.03\"}}, { \"LEADERBOARD\": { \"Task\": \"Multi-Object Tracking\", \"Dataset\": \"nuScenes\", \"Metric\": \"amota\", \"Score\": \"0.18\"}}, { \"LEADERBOARD\": { \"Task\": \"Multi-Object Tracking\", \"Dataset\": \"Natural Questions (short)\", \"Metric\": \"F1\", \"Score\": \"60.42\"}}, { \"LEADERBOARD\": { \"Task\": \"Multi-Object Tracking\", \"Dataset\": \"Natural Questions (long)\", \"Metric\": \"F1\", \"Score\": \"Open.57\"}}, { \"LEADERBOARD\": { \"Task\": \"Multi-Object Tracking\", \"Dataset\": \"Natural Questions (Easy)\", \"Metric\": \"F1\", \"Score\": \"57.0\"}}, { \"LEADERBOARD\": { \"Task\": \"Multi-Object Tracking\", \"Dataset\": \"Natural Questions (Easy)\", \"Metric\": \"F"
    },
    {
        "Context": "A Simple In-Place Algorithm for In-Shuffle",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Neural Machine Translation of Rare Words with Subword Units Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English\u2192German and English\u2192Russian by up to 1.1 and 1.3 BLEU, respectively.  Table 2: English\u2192German translation performance (BLEU, CHRF3 and unigram F 1 ) on newstest2015. Ens-8: ensemble of 8 models. Best NMT system in bold. Unigram F 1 (with ensembles) is computed for all words (n = 44085), rare words (not among top 50 000 in training set; n = 2900), and OOVs (not in training set; n = 1168). single all ens - 8 unigram F1 ( % ) BLEU rare OOV CHRF3 Table 1: Corpus statistics for German training corpus with different word segmentation tech- niques. #UNK: number of unknown tokens in newstest2013. \u25b3: 0 34 \u25b3 59 \u22c4 1079 32 0 # tokens # UNK 3000 # types Table 3: English\u2192Russian translation performance (BLEU, CHRF3 and unigram F 1 ) on newstest2015. Ens-8: ensemble of 8 models. Best NMT system in bold. Unigram F 1 (with ensembles) is computed for all words (n = 55654), rare words (not among",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"WMT2015 English-Russian\", \"Metric\": \"BLEU score\", \"Score\": \"20.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"WMT2015 English-German\", \"Metric\": \"BLEU score\", \"Score\": \"22.8\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-German\", \"Metric\": \"BLEU score\", \"Score\": \"31.4\"}}, { \"LEADERBOARD\": { \"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-German\", \"Metric\": \"SacreBLEU\", \"Score\": \"34.8\"}} ]"
    },
    {
        "Context": "Disentangling Physical Dynamics from Unknown Factors for Unsupervised Video Prediction Leveraging physical knowledge described by partial differential equations (PDEs) is an appealing way to improve unsupervised video prediction methods. Since physics is too restrictive for describing the full visual content of generic videos, we introduce PhyDNet, a two-branch deep architecture, which explicitly disentangles PDE dynamics from unknown complementary information. A second contribution is to propose anew recurrent physical cell (PhyCell), inspired from data assimilation techniques, for performing PDE-constrained prediction in latent space. Extensive experiments conducted on four various datasets show the ability of PhyDNet to outperform state-of-the-art methods. Ablation studies also highlight the important gain brought out by both disentanglement and PDE-constrained prediction. Finally, we show that PhyDNet presents interesting features for dealing with missing data and long-term forecasting. Datasets We evaluate PhyDNet on four datasets from various origins We give details about all datasets in supplementary 2.1 Metrics are scaled to be in a similar range across datasets to ease comparison Network architectures and training PhyDNet shares a common backbone architecture for all datasets where the physical branch contains 49 PhyCells (7 \u00d7 7 kernel filters) and the residual branch is composed of a 3-layers ConvL-STM with 128 filters in each layer We follow evaluation metrics commonly used in state-of-the-art video prediction methods: the Mean Squared Error (MSE), Mean Absolute Error (MAE) and the Structural Similarity (SSIM) that computes the perceived image quality with respect to a reference Table 1. Quantitative forecasting results of PhyDNet compared to baselines using various datasets. Numbers are copied from original or citing papers. * corresponds to results obtained by running online code from the authors. The first five baseline are general deep models applicable to all datasets, whereas DDPAE MAE MAE / 100 SSIM MSE \u00d710 MSE / 10 MSE \u00d7100 - MSE Table 2. An ablation study shows the consistent performance gain on all datasets of our physically-constrained PhyCell vs the general purpose ConvLSTM, and the additional gain brought up by the disentangling architecture PhyDNet. * corresponds to results obtained by running online code from the authors. MAE MAE / 100 SSIM MSE \u00d7 100 MSE / 10 MSE \u00d7 10 MSE Table 3. Influence of physical regularization for Moving MNIST. MAE SSIM MSE",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Video Prediction\", \"Dataset\": \"Moving MNIST\", \"Metric\": \"MAE\", \"Score\": \"70.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Video Prediction\", \"Dataset\": \"Moving MNIST\", \"Metric\": \"MSE\", \"Score\": \"24.4\"}}, {\"LEADERBOARD\": {\"Task\": \"Video Prediction\", \"Dataset\": \"Moving MNIST\", \"Metric\": \"SSIM\", \"Score\": \"0.947\"}}, {\"LEADERBOARD\": {\"Task\": \"Video Prediction\", \"Dataset\": \"Human3.6M\", \"Metric\": \"MAE\", \"Score\": \"1620\"}}, {\"LEADERBOARD\": {\"Task\": \"Video Prediction\", \"Dataset\": \"Human3.6M\", \"Metric\": \"MSE\", \"Score\": \"369\"}}, {\"LEADERBOARD\": {\"Task\": \"Video Prediction\", \"Dataset\": \"Human3.6M\", \"Metric\": \"SSIM\", \"Score\": \"0.901\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Video Prediction\", \"Dataset\": \"D&D\", \"Metric\": \"MAE\", \"Score\": \"4.35\"}}, { \"LEADERBOARD\": { \"Task\": \"Video Prediction\", \"Dataset\": \"D&D\", \"Metric\": \"MSE\", \"Score\": \"5.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Video Prediction\", \"Dataset\": \"D&D\", \"Metric\": \"SSIM\", \"Score\": \"0.895\"}}, { \"LEADERBOARD\": { \"Task\": \"Video Prediction\", \"Dataset\": \"D&D\", \"Metric\": \"MAE\", \"Score\": \"5.44\"}}, { \"LEADERBOARD\": { \"Task\": \"Video Prediction\", \"Dataset\": \"D&D\", \"Metric\": \"MSE\", \"Score\": \"5.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Video Prediction\", \"Dataset\": \"D&D\", \"Metric\": \"MAE\", \"Score\": \"0.07\"}}, { \"LEADERBOARD\": { \"Task\": \"Video Prediction\", \"Dataset\": \"D&D\", \"Metric\": \"MSE\", \"Score\": \"0.17\"}}, { \"LEADERBOARD\": { \"Task\": \"Video Prediction\", \"Dataset\": "
    },
    {
        "Context": "SphereFace: Deep Hypersphere Embedding for Face Recognition This paper addresses deep face recognition (FR) problem under open-set protocol, where ideal face features are expected to have smaller maximal intra-class distance than minimal inter-class distance under a suitably chosen metric space. However, few existing algorithms can effectively achieve this criterion. To this end, we propose the angular softmax (A-Softmax) loss that enables convolutional neural networks (CNNs) to learn angularly discriminative features. Geometrically, A-Softmax loss can be viewed as imposing discriminative constraints on a hypersphere manifold, which intrinsically matches the prior that faces also lie on a manifold. Moreover, the size of angular margin can be quantitatively adjusted by a parameter m. We further derive specific m to approximate the ideal feature criterion. Extensive analysis and experiments on Labeled Face in the Wild (LFW), Youtube Faces (YTF) and MegaFace Challenge show the superiority of A-Softmax loss in FR tasks. The code has also been made publicly available 1 . CNNs with different depths (4, 10, 20, 36, 64) are used to better evaluate our method We use publicly available web-collected training dataset CASIA-WebFace (after excluding the images of identities appearing in testing sets) to train our CNN models Notice that the scale of our training data (0.49M) is relatively small, especially compared to other private datasets used in DeepFace (4M), VGGFace (2M) and FaceNet (200M) We also use class 1 (blue) and class 2 (dark green) to construct positive and negative pairs to evaluate the angle distribution of features from the same class and different classes Besides visual comparison, we also perform face recognition on LFW and YTF to evaluate the effect of m LFW dataset includes 13,233 face images from 5749 different identities, and YTF dataset includes 3,424 videos from 1,595 different individuals Both datasets contains faces with large variations in pose, expression and illuminations We follow the unrestricted Table 2: Our CNN architectures with different convolutional layers. Conv1.x, Conv2.x and Conv3.x denote convolution units that may contain multiple convolution layers and residual units are shown in double-column brackets. E.g., [3\u00d73, 64]\u00d74 denotes 4 cascaded convolution layers with 64 filters of size 3\u00d73, and S2 denotes stride 2. FC1 is the fully connected layer. \u00d7 16 [ 3\u00d73 , 256 ] \u00d71 , S2 64 - layer CNN [ 3\u00d73 , 128 ] \u00d71 , S2 20 - layer CNN [ 3\u00d73 , 64 ] \u00d71 , S2 10 - layer CNN [ 3\u00d73 , 512 ] \u00d71 , S2 36 - layer CNN Table 3. One can observe that while m becomes larger, the accuracy of A-Softmax loss also be- comes better, which shows that larger angular margin can bring stronger discrimination power. m=1 m=3 m=2 m=4 Original Table 3: Accuracy(%) comparison of different m (A-Softmax loss) and original",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Face Verification\", \"Dataset\": \"Trillion Pairs Dataset\", \"Metric\": \"Accuracy\", \"Score\": \"43.76\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Verification\", \"Dataset\": \"MegaFace\", \"Metric\": \"Accuracy\", \"Score\": \"89.142%\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Verification\", \"Dataset\": \"MegaFace\", \"Metric\": \"Accuracy\", \"Score\": \"85.561%\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Verification\", \"Dataset\": \"CK+\", \"Metric\": \"Accuracy\", \"Score\": \"93.80\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Verification\", \"Dataset\": \"YouTube Faces DB\", \"Metric\": \"Accuracy\", \"Score\": \"95.0%\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Verification\", \"Dataset\": \"Labeled Faces in the Wild\", \"Metric\": \"Accuracy\", \"Score\": \"99.42%\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Identification\", \"Dataset\": \"Trillion Pairs Dataset\", \"Metric\": \"Accuracy\", \"Score\": \"43.89\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Identification\", \"Dataset\": \"MegaFace\", \"Metric\": \"Accuracy\", \"Score\": \"75.766%\"}}, {\"LEADERBOARD\": {\"Task\": \"Face Identification\", \"Dataset\": \"MegaFace\", \"Metric\": \"Accuracy\", \"Score\": \"72.729%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"YouTube Faces DB\", \"Metric\": \"Accuracy\", \"Score\": \"98.12%\"}}, { \"LEADERBOARD\": { \"Task\": \"Face Verification\", \"Dataset\": \"Labeled Faces in the Wild\", \"Metric\": \"Accuracy\", \"Score\": \"99.03%\"}} ]"
    },
    {
        "Context": "RGBD Salient Object Detection via Deep Fusion Numerous efforts have been made to design different low level saliency cues for the RGBD saliency detection, such as color or depth contrast features, background and color compactness priors. However, how these saliency cues interact with each other and how to incorporate these low level saliency cues effectively to generate a master saliency map remain a challenging problem. In this paper, we design anew convolutional neural network (CNN) to fuse different low level saliency cues into hierarchical features for automatically detecting salient objects in RGBD images. In contrast to the existing works that directly feed raw image pixels to the CNN, the proposed method takes advantage of the knowledge in traditional saliency detection by adopting various meaningful and well-designed saliency feature vectors as input. This can guide the training of CNN towards detecting salient object more effectively due to the reduced learning ambiguity. We then integrate a Laplacian propagation framework with the learned CNN to extract a spatially consistent saliency map by exploiting the intrinsic structure of the input image. Extensive quantitative and qualitative experimental evaluations on three datasets demonstrate that the proposed method consistently outperforms state-of-the-art methods. In this section, we evaluate the proposed method on three datasets, NLPR RGBD salient dataset, NJUDS2000 stereo datast, and LFSD dataset NLPR dataset The NLPR RGBD salient dataset, BSCA, MB+, and LEGS are obtained from RGB image while the saliency maps of LMH, ACSD, GP are from RGBD image We split this dataset into two part randomly: 750 for training and 250 for testing NJUDS2000 dataset The NJUDS2000 dataset contains 2000 stereo images, as well as the corresponding depth maps and manually labeled groundtruth We also split this dataset into two part randomly: 1000 for training and 1000 for testing LFSD dataset The LFSD dataset contains 100 images with depth information and manually labeled groundtruth All the images in this dataset are for testing Evaluation metrics We compute the precision-recall (PR) curve, mean of average precision and recall, and F-measure score to evaluate the performance of different saliency detection methods",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"RGB-D Salient Object Detection\", \"Dataset\": \"NJU2K\", \"Metric\": \"Average MAE\", \"Score\": \"0.205\"}}, {\"LEADERBOARD\": {\"Task\": \"RGB-D Salient Object Detection\", \"Dataset\": \"NJU2K\", \"Metric\": \"S-Measure\", \"Score\": \"51.4\"}}, {\"LEADERBOARD\": {\"Task\": \"RGB-D Salient Object Detection\", \"Dataset\": \"NJU2K\", \"Metric\": \"max E-Measure\", \"Score\": \"72.4\"}}, {\"LEADERBOARD\": {\"Task\": \"RGB-D Salient Object Detection\", \"Dataset\": \"NJU2K\", \"Metric\": \"max F-Measure\", \"Score\": \"63.2\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"RGB-D Salient Object Detection\", \"Dataset\": \"NJU2K\", \"Metric\": \"Average MAE\", \"Score\": \"0.07\"}}, { \"LEADERBOARD\": { \"Task\": \"RGB-D Salient Object Detection\", \"Dataset\": \"NJU2K\", \"Metric\": \"S-Measure\", \"Score\": \"91.1\"}} ]"
    },
    {
        "Context": "Learning Geometry-Disentangled Representation for Complementary Understanding of 3D Object Point Cloud In 2D image processing, some attempts decompose images into high and low frequency components for describing edge and smooth parts respectively. Similarly, the contour and flat area of 3D objects, such as the boundary and seat area of a chair, describe different but also complementary geometries. However, such investigation is lost in previous deep networks that understand point clouds by directly treating all points or local patches equally. To solve this problem, we propose Geometry-Disentangled Attention Network (GDANet). GDANet introduces Geometry-Disentangle Module to dynamically disentangle point clouds into the contour and flat part of 3D objects, respectively denoted by sharp and gentle variation components. Then GDANet exploits Sharp-Gentle Complementary Attention Module that regards the features from sharp and gentle variation components as two holistic representations, and pays different attentions to them while fusing them respectively with original point cloud features. In this way, our method captures and refines the holistic and complementary 3D geometric semantics from two distinct disentangled components to supplement the local information. Extensive experiments on 3D object classification and segmentation benchmarks demonstrate that GDANet achieves the state-of-the-arts with fewer parameters. Code is released on https://github.com/mutianxu/GDANet. We evaluate our network on shape classification task and part segmentation task on various datasets Table 1: Segmentation results (%) on ShapeNet Part dataset. motor inst . chair rocket bag lamp pistol aero guitar mIOU knife cap top car phone ear lap skate class mug board table Table 2: Classification accuracy (%) on ModelNet40 dataset. Acc . Table 3: Classification results (%) on ScanObjectNN dataset (noise robustness test). acc drop OBJ ONLY OBJ BG Table 4: Geometry-Disentangled complementary effect to supplement KNN information in GDANet on ModelNet40. 'knn' indicates KNN aggregation, 'self' means the input point cloud is fused with itself by self-attention, 'sharp' and 'gentle' denote the input point cloud is fused with features of sharp and gentle variation, 'voting' is the voting strategy during testing, respectively. hancement of random point dropout during training . acc . ( % ) Density robustness test . ( a ) . Point cloud with ran - Table 5: Classification results (%) of using different point selection methods in",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"3D Part Segmentation\", \"Dataset\": \"ShapeNet-Part\", \"Metric\": \"Class Average IoU\", \"Score\": \"85.0\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Part Segmentation\", \"Dataset\": \"ShapeNet-Part\", \"Metric\": \"Instance Average IoU\", \"Score\": \"86.5\"}}, {\"LEADERBOARD\": {\"Task\": \"3D Point Cloud Classification\", \"Dataset\": \"ModelNet40\", \"Metric\": \"Overall Accuracy\", \"Score\": \"93.8\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Part Segmentation\", \"Dataset\": \"ShapeNet-Part\", \"Metric\": \"Class Average IoU\", \"Score\": \"84.9\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Part Segmentation\", \"Dataset\": \"ShapeNet-Part\", \"Metric\": \"Instance Average IoU\", \"Score\": \"86.4\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Point Cloud Classification\", \"Dataset\": \"ScanObjectNN\", \"Metric\": \"Overall Accuracy\", \"Score\": \"73.7\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Point Cloud Classification\", \"Dataset\": \"ModelNet40\", \"Metric\": \"Overall Accuracy\", \"Score\": \"92.4\"}} ]"
    },
    {
        "Context": "Cognitive MAC Protocols Using Memory for Distributed Spectrum Sharing Under Limited Spectrum Sensing The main challenges of cognitive radio include spectrum sensing at the physical (PHY) layer to detect the activity of primary users and spectrum sharing at the medium access control (MAC) layer to coordinate access among coexisting secondary users. In this paper, we consider a cognitive radio network in which a primary user shares a channel with secondary users that cannot distinguish the signals of the primary user from those of a secondary user. We propose a class of distributed cognitive MAC protocols to achieve efficient spectrum sharing among the secondary users while protecting the primary user from potential interference by the secondary users. By using a MAC protocol with one-slot memory, we can obtain high channel utilization by the secondary users while limiting interference to the primary user at a low level. The results of this paper suggest the possibility of utilizing MAC design in cognitive radio networks to overcome limitations in spectrum sensing at the PHY layer as well as to achieve spectrum sharing at the MAC layer.Cognitive medium access control, cognitive radio networks, protocols with memory, spectrum sensing, spectrum sharing.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "A-CNN: Annularly Convolutional Neural Networks on Point Clouds Analyzing the geometric and semantic properties of 3D point clouds through the deep networks is still challenging due to the irregularity and sparsity of samplings of their geometric structures. This paper presents anew method to define and compute convolution directly on 3D point clouds by the proposed annular convolution. This new convolution operator can better capture the local neighborhood geometry of each point by specifying the (regular and dilated) ring-shaped structures and directions in the computation. It can adapt to the geometric variability and scalability at the signal processing level. We apply it to the developed hierarchical neural networks for object classification, part segmentation, and semantic segmentation in large-scale scenes. The extensive experiments and comparisons demonstrate that our approach outperforms the state-of-the-art methods on a variety of standard benchmark datasets (e.g., ModelNet10, ModelNet40, ShapeNetpart, S3DIS, and ScanNet). We evaluate our A-CNN model on various tasks such as point cloud classification, part segmentation, and largescale scene segmentation 7 show the quantitative results of part segmentation on ShapeNet-part dataset with two different inputs For ShapeNet-part dataset, we visualize more results (besides the segmentation results shown in the paper) in For S3DIS dataset, we pick rooms from all six areas: area 1 (row 1), area 2: The visualization results on S3DIS dataset Table 1: Classification results on ModelNet10 and ModelNet40 datasets. AAC is accuracy average class, OA is overall accuracy. point cloud based methods with 1024 points different methods with additional input or more points OA AAC ModelNet40 ModelNet10 Table 2: Segmentation results on ShapeNet-part, S3DIS, and Scan- Net. \"mean\" is mean IoU (%), OA is overall accuracy. - OA mean without normals S3DIS ShapeNet - part - with normals ScanNet Table 3: Ablation experiments on ModelNet40 dataset. AAC is accuracy average class, OA is overall accuracy. OA AAC Table 4: Experiments on redundancy on ModelNet40 dataset. AAC is accuracy average class, OA is overall accuracy. OA AAC Table 5: Network configurations. inner , Router ] , k is number of neighbors , F is feature map size . For - [ 16 , 48 ] [ [ 64 , 64 , 128 ] , [ 128 , 128 , 256 ] ]",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"3D Point Cloud Classification\", \"Dataset\": \"ModelNet40\", \"Metric\": \"Overall Accuracy\", \"Score\": \"92.6\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"3D Part Segmentation\", \"Dataset\": \"ShapeNet-Part\", \"Metric\": \"Instance Average IoU\", \"Score\": \"84.9\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Point Cloud Classification\", \"Dataset\": \"ScanObjectNN\", \"Metric\": \"Overall Accuracy\", \"Score\": \"78.5\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Point Cloud Classification\", \"Dataset\": \"ModelNet40\", \"Metric\": \"Mean Accuracy\", \"Score\": \"90.6\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Point Cloud Classification\", \"Dataset\": \"ModelNet40\", \"Metric\": \"Overall Accuracy\", \"Score\": \"92.9\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Point Cloud Classification\", \"Dataset\": \"ModelNet40\", \"Metric\": \"Overall Accuracy\", \"Score\": \"92.3\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Point Cloud Classification\", \"Dataset\": \"ModelNet40\", \"Metric\": \"Mean Accuracy\", \"Score\": \"81.9\"}}, { \"LEADERBOARD\": { \"Task\": \"3D Point Cloud Classification\", \"Dataset\": \"ModelNet40\", \"Metric\": \"Overall Accuracy\", "
    },
    {
        "Context": "Self-stabilizing Tiny Interaction Protocols In this paper we present the self-stabilizing implementation of a class of token based algorithms. In the current work we only consider interactions between weak nodes. They are uniform, they do not have unique identifiers, are static and their interactions are restricted to a subset of nodes called neighbours. While interacting, a pair of neighbouring nodes may create mobile agents (that materialize in the current work the token abstraction) that perform traversals of the network and accelerate the system stabilization. In this work we only explore the power of oblivious stateless agents. Our work shows that the agent paradigm is an elegant distributed tool for achieving selfstabilization in Tiny Interaction Protocols (TIP). Nevertheless, in order to reach the full power of classical self-stabilizing algorithms more complex classes of agents have to be considered (e.g. agents with memory, identifiers or communication skills). Interestingly, our work proposes for the first time a model that unifies the recent studies in mobile robots(agents) that evolve in a discrete space and the already established population protocols paradigm.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "LT-GAN: Self-Supervised GAN with Latent Transformation Detection Generative Adversarial Networks (GANs) coupled with self-supervised tasks, have shown promising results in unconditional and semi-supervised image generation. We propose a self-supervised approach (LT-GAN) to improve the generation quality and diversity of images by estimating the GAN-induced transformation (i.e. transformation induced in the generated images by perturbing the latent space of generator). Specifically, given two pairs of images where each pair comprises of a generated image and its transformed version, the self-supervision task aims to identify whether the latent transformation applied in the given pair is same to that of the other pair. Hence, this auxiliary loss encourages the generator to produce images that are distinguishable by the auxiliary network, which in turn promotes the synthesis of semantically consistent images with respect to latent transformations. We show the efficacy of this pretext task by improving the image generation quality in terms of FID on state-of-the-art models for both conditional and unconditional settings on CIFAR-10, CelebA-HQ and ImageNet datasets. Moreover, we empirically show that LT-GAN helps in improving controlled image editing for CelebA-HQ and ImageNet over baseline models. We experimentally demonstrate that our proposed LT self-supervision task can be effectively combined with other state-of-the-art training techniques for added benefits. Consequently, we show that our approach achieves the new state-of-the-art FID score of 9.8 on conditional CIFAR-10 image generation. Datasets We validate our proposed self-supervised task on CIFAR-10, STL-10, CelebA-HQ-128 and ImageNet-2012 datasets FID has been shown to be more consistent with human evaluation of image quality and also helps in detecting intra-class mode collapse We calculate FID between test set images and equal number of generated images for all datasets and report the best FID obtained across 3 runs We use this methodology to discover latent space trajectories corresponding to the image transformations: brightness, scale, horizontal shift and vertical shift for BigGAN conditional model trained on ImageNet dataset CelebA-HQ Dataset InterfaceGAN provides a framework to find the interpretable semantic directions encoded in the latent space of face synthesis GAN models Table 1: Comparison of self-supervised LT-GAN training approach with # FID Table 2: Classification accuracy (%) on separation boundaries in latent LT - GAN SS - GAN Baseline Table 3: Correlation matrix of synthesized attribute distributions of Table 4: FID comparison of LT-GAN with SS-GAN on different datasets STL - 10 CIFAR - 10 CelebA - HQ Table 5: Inception Score for SNDCGAN and BigGAN architectures BigGAN",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Generation\", \"Dataset\": \"CelebA-HQ 128x128\", \"Metric\": \"FID\", \"Score\": \"16.84\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Generation\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"FID\", \"Score\": \"9.80\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"FID\", \"Score\": \"4.30\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"CIFAR-10\", \"Metric\": \"Inception score\", \"Score\": \"8.49\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Generation\", \"Dataset\": \"ImageNet 32x32\", \"Metric\": \"FID\", \"Score\": \"12.3\"}} ]"
    },
    {
        "Context": "EfficientPose: An efficient, accurate and scalable end-to-end 6D multi object pose estimation approach In this paper we introduce EfficientPose, anew approach for 6D object pose estimation. Our method is highly accurate, efficient and scalable over a wide range of computational resources. Moreover, it can detect the 2D bounding box of multiple objects and instances as well as estimate their full 6D poses in a single shot. This eliminates the significant increase in runtime when dealing with multiple objects other approaches suffer from. These approaches aim to first detect 2D targets, e.g. keypoints, and solve a Perspective-n-Point problem for their 6D pose for each object afterwards. We also propose a novel augmentation method for direct 6D pose estimation approaches to improve performance and generalization, called 6D augmentation. Our approach achieves anew state-of-theart accuracy of 97.35% in terms of the ADD(-S) metric on the widely-used 6D pose estimation benchmark dataset Linemod using RGB input, while still running end-to-end at over 27 FPS. Through the inherent handling of multiple objects and instances and the fused single shot 2D object detection as well as 6D pose estimation, our approach runs even with multiple objects (eight) end-to-end at over 26 FPS, making it highly attractive to many real world scenarios. Code will be made publicly available at In this section we describe the experiments we did, our experimental setup with implementation details as well as the evaluation metrics we use We evaluate our approach on two popular benchmark datasets which are described in this subsection We evaluate our approach with the commonly used ADD(-S) metric Symmetric objects are evaluated using the ADD-S metric which is given by the following equation Method YOLO6D Pix2Pose PVNet DPOD DPOD+ CDPN Hybrid-Pose Ours \u03c6 = 0 Quantitative evaluation and comparison on the Linemod dataset in terms of the ADD(-S) metric Table 1. Quantitative evaluation and comparison on the Linemod dataset in terms of the ADD(-S) metric. Symmetric objects are marked with * and approaches marked with + are using an additional refinement method. \u03c6 = 3 100 Ours Table 2. Quantitative evaluation in terms of the ADD(-S) metric for the task of multi object 6D pose estimation using a single model on the Occlusion dataset. Symmetric objects are marked with * Ours \u03c6 = 0 Ours \u03c6 = 3 Table 3. Runtime analysis and comparison of our method performing single and multiple object pose estimation while using different scales. For single object 6D pose estimation the Linemod dataset is used while for multi object pose estimation the Occlusion dataset is used which contains usually eight annotated objects per image. We further compare our method's runtime with the vanilla EfficientDet Single Multi Table 4. Ablation study to evaluate the influence of our",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"6D Pose Estimation using RGB\", \"Dataset\": \"LineMOD\", \"Metric\": \"Accuracy (ADD)\", \"Score\": \"97.35%\"}}, {\"LEADERBOARD\": {\"Task\": \"6D Pose Estimation using RGB\", \"Dataset\": \"LineMOD\", \"Metric\": \"Mean ADD\", \"Score\": \"97.35\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"6D Pose Estimation using RGB\", \"Dataset\": \"LineMOD\", \"Metric\": \"Mean ADD\", \"Score\": \"47.4\"}}, { \"LEADERBOARD\": { \"Task\": \"6D Pose Estimation using RGB\", \"Dataset\": \"LineMOD\", \"Metric\": \"Accuracy\", \"Score\": \"94.3\"}}, { \"LEADERBOARD\": { \"Task\": \"6D Pose Estimation using RGB\", \"Dataset\": \"LineMOD\", \"Metric\": \"Accuracy (ADD)\", \"Score\": \"94.3\"}} ]"
    },
    {
        "Context": "Neural-Guided RANSAC: Learning Where to Sample Model Hypotheses We present Neural-Guided RANSAC (NG-RANSAC), an extension to the classic RANSAC algorithm from robust optimization. NG-RANSAC uses prior information to improve model hypothesis search, increasing the chance of finding outlier-free minimal sets. Previous works use heuristic side information like hand-crafted descriptor distance to guide hypothesis search. In contrast, we learn hypothesis search in a principled fashion that lets us optimize an arbitrary task loss during training, leading to large improvements on classic computer vision tasks. We present two further extensions to NG-RANSAC. Firstly, using the inlier count itself as training signal allows us to train neural guidance in a self-supervised fashion. Secondly, we combine neural guidance with differentiable RANSAC to build neural networks which focus on certain parts of the input data and make the output predictions as good as possible. We evaluate NG-RANSAC on a wide array of computer vision tasks, namely estimation of epipolar geometry, horizon line estimation and camera re-localization. We achieve superior or competitive results compared to state-of-the-art robust estimators, including very recent, learned ones. We evaluate neural guidance on multiple, classic computer vision tasks",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Horizon Line Estimation\", \"Dataset\": \"Horizon Lines in the Wild\", \"Metric\": \"AUC (horizon error)\", \"Score\": \"75.2\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet (finetuned)\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"71.1%\"}}, { \"LEADERBOARD\": { \"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet (finetuned)\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"90.3%\"}}, { \"LEADERBOARD\": { \"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 1 Accuracy\", \"Score\": \"75.4%\"}}, { \"LEADERBOARD\": { \"Task\": \"Self-Supervised Image Classification\", \"Dataset\": \"ImageNet\", \"Metric\": \"Top 5 Accuracy\", \"Score\": \"90.0%\"}} ]"
    },
    {
        "Context": "Aerial Imagery Pixel-level Segmentation Aerial Imagery Pixel-level Segmentation Aerial imagery can be used for important work on a global scale. Nevertheless, the analysis of this data using neural network architectures lags behind the current state-of-the-art on popular datasets such as PASCAL VOC, CityScapes and Camvid. In this paper we bridge the performance-gap between these popular datasets and aerial imagery data. Little work is done on aerial imagery with state-of-the-art neural network architectures in a multiclass setting. Our experiments concerning data augmentation, normalisation, image size and loss functions give insight into a high performance setup for aerial imagery segmentation datasets. Our work, using the state-of-the-art DeepLabv3+ Xception65 architecture, achieves a mean IOU of 70% on the DroneDeploy validation set. With this result, we clearly outperform the current publicly available state-of-the-art validation set mIOU (65%) performance with 5%. Furthermore, to our knowledge, there is no mIOU benchmark for the test set. Hence, we also propose anew benchmark on the DroneDeploy test set using the best performing DeepLabv3+ Xception65 architecture, with a mIOU score of 52.5%. Since we are dealing with a pixel level segmentation problem, it might seem logical to think of pixel accuracy as a valid evaluation metric For aerial datasets such as the DroneDeploy dataset this problem is more important since the ground (37.7%) and vegetation (10.43%) classes are dominantly present, so this cannot be ignored CCE loss is still widely used in the current state-of-the-art, but given the expected class imbalance issues in our dataset, we also inspect a loss function which deals explicitly with class imbalance The need for this is also discussed in where they discuss weighting their loss \"for each output channel in order to counteract a class imbalance present in the dataset\" In this section we look into how these design choices affect their performance in detail by experimenting on an aerial imagery dataset Geospatial datasets comes in a large number of different file formats, sizes, and schemes There Table 2: Details of the cluster used for training and experimentation A Precision scores recorded using the Weights and Biases package , for all exploratory S : Table 3: Results of DeepLabv3+ and u-net architecture variants trained on the DroneDe- ploy dataset. val: mean IOU on validation set trained on train set only. test: mean IOU on test set trained on train+validation set. Class IOU scores: 1:Building, 2:Clutter, 3:Vegetation, 4:Water, 5:Ground, 6:Car f1: f1 mean, reported for benchmark architecture comparison against DroneDe- ploy public leaderboard. test set . val test aforementioned for training f1 with 1 2 are obtained by training for 40 epochs on the 3 split 4 5 6 of Table 4: Results of DeepLabv3+ architecture variants trained on the DroneDeploy train and validation images. test: mean IOU on test set. Class IOU scores: 1:Building, 2:Clutter, 3:Vegetation, 4:Water, 5:Ground, 6:Car. 1 2 3 4 test 5",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"DroneDeploy\", \"Metric\": \"Mean IoU (test)\", \"Score\": \"52.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"DroneDeploy\", \"Metric\": \"Mean IoU (val)\", \"Score\": \"69.9\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"Cityscapes test\", \"Metric\": \"Mean IoU (class)\", \"Score\": \"78.4%\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL VOC 2012 test\", \"Metric\": \"Mean IoU\", \"Score\": \"68.10%\"}} ]"
    },
    {
        "Context": "FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation Fully convolutional models for dense prediction have proven successful fora wide range of visual tasks. Such models perform well in a supervised setting, but performance can be surprisingly poor under domain shifts that appear mild to a human observer. For example, training on one city and testing on another in a different geographic region and/or weather condition may result in significantly degraded performance due to pixel-level distribution shift. In this paper, we introduce the first domain adaptive semantic segmentation method, proposing an unsupervised adversarial approach to pixel prediction problems. Our method consists of both global and category specific adaptation techniques. Global domain alignment is performed using a novel semantic segmentation network with fully convolutional domain adversarial learning. This initially adapted space then enables category specific adaptation through a generalization of constrained weak learning, with explicit transfer of the spatial layout from the source to the target domains. Our approach outperforms baselines across different settings on multiple large-scale datasets, including adapting across various real city environments, different synthetic sub-domains, from simulated to real environments, and on a novel large-scale dash-cam dataset. In this section, we report our experimental results on three different domain adaptation tasks: cities \u2192 cities, season \u2192 season, and synthetic \u2192 real, studied across four different datasets All code and models are trained and evaluated in the Caffe framework and will be made available before camera-ready For fair comparison, we use the Intersection over Union (IoU) evaluation metric for all experiments For cities \u2192 cities and synthetic \u2192 real tasks, we followed the evaluation protocol of and train our models with 19 semantic labels of Cityscapes The whole dataset is divided into three parts: 2, 975 training samples, 500 validation samples and 1, 525 test samples The split of this dataset is city-level, which covers individual European cities in different geographic and population distribution We take the whole dataset with labels compatible to Cityscapes categories for synthetic \u2192 real adaptation Table 1: Adaptation from synthetic to real. We study the performance using GTA5 and SYNTHIA as source labeled training data adapted and Cityscapes train as an unlabeled target domain, while evaluating our adaptation algorithm on Cityscapes val. Meanwhile, we show an ablation of the components of our method and how each contributes to the overall performance of our approach. Here GA represents global domain alignment and CA indicates category specific adaptation. SYNTHIA \u2192 Cityscapes mIoU bike Table 2: Adaptation across seasons. We study the cross season performance using sub-sequences of SYNTHIA dataset. We report quantitative comparisons of performance before and after adaptation for training on one season and evaluating on another unannotated novel season. (Avg: the average performance of adaptation from one to another.) sky sidewalk bicycle sign vegetation pole building mIoU t road car light pedestrian lanemarking fence Table 3: Adaptation across cities. We study the performance using Cityscapes train",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"SYNTHIA Fall-to-Winter\", \"Metric\": \"mIoU\", \"Score\": \"59.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Image-to-Image Translation\", \"Dataset\": \"SYNTHIA-to-Cityscapes\", \"Metric\": \"mIoU (13 classes)\", \"Score\": \"20.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Synthetic-to-Real Translation\", \"Dataset\": \"GTAV-to-Cityscapes Labels\", \"Metric\": \"mIoU\", \"Score\": \"27.1\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image-to-Image Translation\", \"Dataset\": \"GTAV-to-Cityscapes Labels\", \"Metric\":\"mIoU\", \"Score\": \"43.2\"}} ]"
    },
    {
        "Context": "A Trust-Based Cross-Layer Security Protocol for Mobile Ad hoc Networks In this paper, we develop a trust based security protocol based on a cross-layer approach which attains confidentiality and authentication of packets in both routing and link layers of MANETs. In the first phase of the protocol, we design a trust based packet forwarding scheme for detecting and isolating the malicious nodes using the routing layer information. It uses trust values to favor packet forwarding by maintaining a trust counter for each node. A node is punished or rewarded by decreasing or increasing the trust counter. If the trust counter value falls below a trust threshold, the corresponding intermediate node is marked as malicious. In the next phase of the protocol, we provide link-layer security using the CBC-X mode of authentication and encryption. By simulation results, we show that the proposed cross-layer security protocol achieves high packet delivery ratio while attaining low delay and overhead.The neighbor attack and the black hole attack prevent the data from being delivered to the destination. But the neighbor attacker does not catch and capture the data packets from the source node. It leaves the settings as soon as sending the false messages.Two malicious nodes share a private communication link between them. One node captures the traffic information of the network and sends them directly to other node. Warm hole can eavesdrop the traffic, maliciously drop the packets, and perform man-in-the-middle attacks against the network protocols.[6].When the network bandwidth is hacked by a malicious node [5], then it results to the DoS attack. In order to utilize precious network resources like bandwidth, or to utilize node resources like memory or computation power, the attacker inserts packets into the network. The specific instances of the DoS attack are the routing table overflow attack and energy consumption attack.The information disclosure attack aims at the privacy requirements of network. The confidential information's like A. Rajaram received the B.E. degree in electronics and communication engineering from the Govt., college of Technology, Coimbatore, Anna University, Chennai, India, in 2006, the M.E. degree in electronics and communication engineering (Applied Electronics) from the Govt., college of Technology, Anna University, Chennai, India, in 2008 and he is currently pursuing the full time Ph.D. degree in electronics and communication engineering from the Anna University Coimbatore, Coimbatore, India. His research interests include communication and networks mobile adhoc networks, wireless communication networks (WiFi, WiMax HighSlot GSM), novel VLSI NOC Design approaches to address issues such as low-power, cross-talk, hardware acceleration, Design issues includes OFDM MIMO and noise Suppression in MAI Systems, ASIC design, Control systems, Fuzzy logic and Networks, AI, Sensor Networks.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Algebraic Linear Orderings An algebraic linear ordering is a component of the initial solution of a first-order recursion scheme over the continuous categorical algebra of countable linear orderings equipped with the sum operation and the constant 1. Due to a general Mezei-Wright type result, algebraic linear orderings are exactly those isomorphic to the linear ordering of the leaves of an algebraic tree. Using Courcelle's characterization of algebraic trees, we obtain the fact that a linear ordering is algebraic if and only if it can be represented as the lexicographic ordering of a deterministic contextfree language. When the algebraic linear ordering is a well-ordering, its order type is an algebraic ordinal. We prove that the Hausdorff rank of any scattered algebraic linear ordering is less than \u03c9 \u03c9 . It follows that the algebraic ordinals are exactly those less than \u03c9 \u03c9 \u03c9 .",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "Efficient Image Retrieval via Decoupling Diffusion into Online and Offline Processing Diffusion is commonly used as a ranking or re-ranking method in retrieval tasks to achieve higher retrieval performance, and has attracted lots of attention in recent years. A downside to diffusion is that it performs slowly in comparison to the naive k-NN search, which causes a non-trivial online computational cost on large datasets. To overcome this weakness, we propose a novel diffusion technique in this paper. In our work, instead of applying diffusion to the query, we precompute the diffusion results of each element in the database, making the online search a simple linear combination on top of the k-NN search process. Our proposed method becomes 10\u223c times faster in terms of online search speed. Moreover, we propose to use late truncation instead of early truncation in previous works to achieve better retrieval performance. For the efficiency evaluation, we use a single core of Intel Xeon 2.80GHz CPU Datasets We use the Oxford Buildings ) and Paris) datasets in our experiments The datasets are referred to as Oxford5k and Paris6k respectively in correspondence with the size of each dataset Another set of 100k random images from Flicker) are commonly used as distractors to enlarge the above datasets to Oxford105k and Paris106k We measure the online computational time on the 55 queries of the datasets for Iscen's method) and our proposed method For evaluation, we adopt the standard mean average precision (mAP) as a performance measurement We experiment on both global and regional features provided For the Oxford and Paris datasets, there are 21 regional features per image on average We conduct k-NN search by using the efficient FAISS toolkit 1 , containing a CPU version and a faster GPU version, which allows us to deal Table 1: Performance comparison with the state of the art. We used R-MAC features extracted with VGG Figure 6 : Retrieval performance ( mAP ) vs . the size of truncated graph L using early truncation and late truncation . Iscen , late R - MAC ( VGG ) R - MAC ( ResNet ) Oxf5k Par6k Par106k 10 3 size of truncated graph k - NN 10 2 ( c ) Paris6k ( d ) Paris106k Oxf105k ours , early Iscen , early ours , late",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Image Retrieval\", \"Dataset\": \"Par106k\", \"Metric\": \"mAP\", \"Score\": \"96.2%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Retrieval\", \"Dataset\": \"Oxf105k\", \"Metric\": \"MAP\", \"Score\": \"95.2%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Retrieval\", \"Dataset\": \"Par6k\", \"Metric\": \"mAP\", \"Score\": \"97.8%\"}}, {\"LEADERBOARD\": {\"Task\": \"Image Retrieval\", \"Dataset\": \"Oxf5k\", \"Metric\": \"MAP\", \"Score\": \"96.2%\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Image Retrieval\", \"Dataset\": \"Par106k\", \"Metric\":\"mAP\", \"Score\": \"90.8%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Retrieval\", \"Dataset\": \"Oxf105k\", \"Metric\": \"MAP\", \"Score\": \"87.8%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Retrieval\", \"Dataset\": \"Par6k\", \"Metric\":\"mAP\", \"Score\": \"93.8%\"}}, { \"LEADERBOARD\": { \"Task\": \"Image Retrieval\", \"Dataset\": \"Oxf5k\", \"Metric\": \"MAP\", \"Score\": \"89.9%\"}} ]"
    },
    {
        "Context": "Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation Recent semantic segmentation methods exploit encoderdecoder architectures to produce the desired pixel-wise segmentation prediction. The last layer of the decoders is typically a bilinear upsampling procedure to recover the final pixel-wise prediction. We empirically show that this oversimple and data-independent bilinear upsampling may lead to sub-optimal results.In this work, we propose a data-dependent upsampling (DUpsampling) to replace bilinear, which takes advantages of the redundancy in the label space of semantic segmentation and is able to recover the pixel-wise prediction from low-resolution outputs of CNNs. The main advantage of the new upsampling layer lies in that with a relatively lowerresolution feature map such as 1 16 or 1 32 of the input size, we can achieve even better segmentation accuracy, significantly reducing computation complexity. This is made possible by 1) the new upsampling layer's much improved reconstruction capability; and more importantly 2) the DUpsampling based decoder's flexibility in leveraging almost arbitrary combinations of the CNN encoders' features. Experiments demonstrate that our proposed decoder outperforms the state-of-the-art decoder, with only \u223c20% of computation. Finally, without any post-processing, the framework equipped with our proposed decoder achieves new state-of-the-art performance on two datasets: 88.1% mIOU on PASCAL VOC with 30% computation of the previously best model; and 52.5% mIOU on PASCAL Context. The proposed models are evaluated on the PASCAL VOC 2012 semantic segmentation benchmark and PAS-CAL Context benchmark PASCAL VOC is the dataset widely used for semantic segmentation Table 1: mIOU over the PASCAL VOC val set of DUpsampling vs. bilinear upsampling. \"output stride\" indicates the ratio of in- put image spatial resolution to final output resolution. mIOU* denotes the upper bound. mIOU * ( % ) mIOU ( % ) Table 2: mIOU over PASCAL VOC val set when using differ- ent fusion of features. bxuycz denotes low-level features named block x/unit y/conv z in ResNet. \"FLOPS\" denotes the amount of computation of the decoder including feature aggregation, con- volutional decoder and the final upsampling. FLOPS mIOU ( % ) Table 2. The best one is the combination of conv1 3 + b3u6u3, achieving mIOU 74.20% over val set. Additionally, as shown in is the combination of conv1 3 + b3u6u3 , achieving mIOU Table 2 . Table 3: mIOU over the PASCAL VOC val set when using dif- ferent fusion strategies of features. bxuycz denotes low-level fea-",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL Context\", \"Metric\": \"mIoU\", \"Score\": \"52.5\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"PASCAL Context\", \"Metric\":\"mIoU\", \"Score\": \"51.2\"}} ]"
    },
    {
        "Context": "OmniNet: Omnidirectional Representations from Transformers This paper proposes Omnidirectional Representations from Transformers (OMNINET). In OmniNet, instead of maintaining a strictly horizontal receptive field, each token is allowed to attend to all tokens in the entire network. This process can also be interpreted as a form of extreme or intensive attention mechanism that has the receptive field of the entire width and depth of the network. To this end, the omnidirectional attention is learned via a meta-learner, which is essentially another self-attention based model. In order to mitigate the computationally expensive costs of full receptive field attention, we leverage efficient self-attention models such as kernel-based (Choromanski et al., 2020), low-rank attention ) and/or Big Bird (Zaheer et al., 2020 as the meta-learner. Extensive experiments are conducted on autoregressive language modeling (LM1B, C4), Machine Translation, Long Range Arena (LRA), and Image Recognition. The experiments show that OmniNet achieves considerable improvements across these tasks, including achieving state-of-the-art performance on LM1B, WMT'14 En-De/En-Fr, and Long Range Arena. Moreover, using omnidirectional representation in Vision Transformers leads to significant improvements on image recognition tasks on both few-shot learning and fine-tuning setups. Similar to the ViT setup, we pre-train our OmniNet models on the JFT dataset with 18k classes and 303M images, for 7 epochs We evaluate our models in the transfer setup (few-shot and fine-tuning) on several downstream tasks: ImageNet, CIFAR-10, CIFAR-100), Oxford-IIIT Pets, and Oxford Flowers-102 For WMT'17, we build sentencepiece tokenizers of 32K from the dataset WMT'17 collections are obtained from Tensorflow datasets (TFDS) For autoregressive language modeling, the C4 corpus is similarly found in TFDS Table 1. Experimental results (quality, i.e., perplexity scores at 30K and 100K respectively) on autoregressive language modeling. All models are approximately 50M parameters. LM1B C4 Table 2. Comparison with existing state-of-the-art and published works on One Billion Word Language modeling (Chelba et al., 2013) benchmark. LM1B C4 Table 3. Results on five collections from the WMT'17 machine translation task. En - De Ru - En En - Fi Cs - En En - Fr Table 4. Comparisons with the state-of-the-art on WMT'14 En-De and WMT'14 En-Fr. OmniNet outperforms ADMIN (Liu et al., 2020), the current state-of-the-art deep transformer model for MT. n / a En - De En - Fr Table 5. Results on Long Range Arena (Tay et al., 2020a). Retrieval Avg ListOps Text Table 6. Transfer performance of pre-trained OmniNet and equivalent ViT models in fine-tuning setup on popular image classification benchmarks. All models are pre-trained on the JFT-300M",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-French\", \"Metric\": \"BLEU score\", \"Score\": \"42.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"WMT2017 Russian-English\", \"Metric\": \"BLEU\", \"Score\": \"36.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"WMT2017 English-Finnish\", \"Metric\": \"BLEU\", \"Score\": \"20.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"WMT2017 Chinese-English\", \"Metric\": \"BLEU\", \"Score\": \"23.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"WMT2017 English-French\", \"Metric\": \"BLEU\", \"Score\": \"43.1\"}}, {\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"WMT2014 English-German\", \"Metric\": \"BLEU score\", \"Score\": \"29.8\"}}, {\"LEADERBOARD\": {\"Task\": \"Machine Translation\", \"Dataset\": \"WMT2017 English-German\", \"Metric\": \"BLEU\", \"Score\": \"29.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"One Billion Word\", \"Metric\": \"Number of params\", \"Score\": \"100M\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"One Billion Word\", \"Metric\": \"PPL\", \"Score\": \"21.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"One Billion Word\", \"Metric\": \"PPL\", \"Score\": \"21.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"One Billion Word\", \"Metric\": \"PPL\", \"Score\": \"22\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"Stanford Cars\", \"Metric\": \"Accuracy\", \"Score\": \"96.2%\"}}, { \"LEADERBOARD\": { \"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"CUB-200-2011\", \"Metric\": \"Accuracy\", \"Score\": \"98%\"}}, { \"LEADERBOARD\": { \"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"Oxford 102 Flowers\", \"Metric\": \"Accuracy\", \"Score\": \"94.9%\"}}, { \"LEADERBOARD\": { \"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"Caltech-101\", \"Metric\": \"Accuracy\", \"Score\": \"90.8%\"}}, { \"LEADERBOARD\": { \"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"Caltech-101\", \"Metric\": \"Accuracy\", \"Score\": \"90.8%\"}}, { \"LEADERBOARD\": { \"Task\": \"Fine-Grained Image Classification\", \"Dataset\": \"OMNIGLOT\", \"Metric\": \"Accuracy\", \"Score\": \"95%\"}}]"
    },
    {
        "Context": "Capturing Polynomial Time on Interval Graphs We prove a characterization of all polynomial-time computable queries on the class of interval graphs by sentences of fixed-point logic with counting. More precisely, it is shown that on the class of unordered interval graphs, any query is polynomial-time computable if and only if it is definable in fixed-point logic with counting. This result is one of the first establishing the capturing of polynomial time on a graph class which is defined by forbidden induced subgraphs. For this, we define a canonical form of interval graphs using a type of modular decomposition, which is different from the method of tree decomposition that is used inmost known capturing results for other graph classes, specifically those defined by forbidden minors. The method might also be of independent interest for its conceptual simplicity. Furthermore, it is shown that fixed-point logic with counting is not expressive enough to capture polynomial time on the classes of chordal graphs or incomparability graphs.",
        "PWC Annotation": "unanswerable",
        "LLM prediction": "unanswerable"
    },
    {
        "Context": "EFFICIENT LONG-DISTANCE RELATION EXTRACTION WITH DG-SPANBERT In natural language processing, relation extraction seeks to rationally understand unstructured text. Here, we propose a novel SpanBERT-based graph convolutional network (DG-SpanBERT) that extracts semantic features from a raw sentence using the pre-trained language model SpanBERT and a graph convolutional network to pool latent features. Our DG-SpanBERT model inherits the advantage of SpanBERT on learning rich lexical features from large-scale corpus. It also has the ability to capture long-range relations between entities due to the usage of GCN on dependency tree. The experimental results show that our model outperforms other existing dependency-based and sequence-based models and achieves a state-of-the-art performance on the TACRED dataset.Relation extraction aims to discern the semantic relation that exists between two entities within the context of a sentence. For example, in the sentence \"The key was in a chest\", \"key\" is a subject entity and \"chest\" is an object entity. The target for relation extraction is to predict the relation between \"key\" and \"chest\", which is \"Content-Container\". Relation extraction plays a fundamental role in natural language understanding of unstructured text, such as knowledge base population [1], question answering [2] and information extraction [3].The existing solutions for relation extraction can be categorized into dependency-based and sequence-based approaches. Dependency-based models rely on the dependency trees that are able to provide rich structural and syntactic information for classifying relations; see, for example, [4] and [5]. Sequence-based models directly operate on the word sequences and forgo the information of dependency structures. For example, the model described in [6] relies on a multi-level attention mechanism to capture the attentions regarding target entities and relations. Bidirectional Long Short-Term Memory (LSTM) is applied on sentences to capture the semantic features in [7]. Recently, BERT-related models [8,9,10] have shown their ability to improve relation extraction tasks and achieve state-of-the-art results.Although BERT-based models are strong on learning rich semantic features, they may not effectively capture the long-range syntactic relations. For example, in the sentence \"Arcandor said in documents filed Wednesday with a district court in Essen, where it is based, that the 15 companies include Corporate Service Group GmbH\", with \"Arcandor\" as the subject and \"Corporate Service Grroup GmbH\" as the object, it is difficult for sequence-based models to extract features between such long-distance entities. Therefore, we propose DG-SpanBERT model, which is the first to combine BERT-related model with Graph Convolutional Network (GCN) on relation extraction. Specifically, our model groups BERT sentence-embedding in a dependency tree structure and then uses a GCN network to extract features from the tree. The TACRED dataset contains over 106K sentences, and it covers 41 relation types (e.g., per:schools_attended and org:members) and one \"no relation\" label to describe the relation between the subject and the object in the sentences To make an appropriate comparison, all evaluated models in this paper are trained with the same setting Table 1: Compare DG-SpanBERT with other models on TACRED dataset. Bold indicates the best performance among all. P R F1 Table 2: Compare average F1 performance for those token distances \u2265 11 tokens between the subject and object. Bold indicates the best performance among all. F1",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Relation Extraction\", \"Dataset\": \"TACRED\", \"Metric\": \"F1\", \"Score\": \"71.5\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"TACRED\", \"Metric\": \"F1\", \"Score\": \"60.6\"}}, { \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"SemEval-2010 Task 8\", \"Metric\": \"F1\", \"Score\": \"71.3\"}} ]"
    },
    {
        "Context": "Fast-Slow Recurrent Neural Networks Processing sequential data of variable length is a major challenge in a wide range of applications, such as speech recognition, language modeling, generative image modeling and machine translation. Here, we address this challenge by proposing a novel recurrent neural network (RNN) architecture, the Fast-Slow RNN (FS-RNN). The FS-RNN incorporates the strengths of both multiscale RNNs and deep transition RNNs as it processes sequential data on different timescales and learns complex transition functions from onetime step to the next. We evaluate the FS-RNN on two character level language modeling data sets, Penn Treebank and Hutter Prize Wikipedia, where we improve state of the art results to 1.19 and 1.25 bits-per-character (BPC), respectively. In addition, an ensemble of two FS-RNNs achieves 1.20 BPC on Hutter Prize Wikipedia outperforming the best known compression algorithm with respect to the BPC measure. We also present an empirical investigation of the learning and network dynamics of the FS-RNN, which explains the improved performance compared to other RNN architectures. Our approach is general as any kind of RNN cell is a possible building block for the FS-RNN architecture, and thus can be flexibly applied to different tasks. The FS-LSTM is evaluated on two character level language modeling data sets, namely Penn Treebank and Hutter Prize Wikipedia, which will be referred to as enwik8 in this section Table 1: BPC on Penn Treebank - Param Count Table 1 : BPC on Penn Treebank BPC Table 2: BPC on enwik8 2 \u00d7 47M 18M 27M 14M 47M 35M 46M 23M 21M 64M Table 2 : BPC on enwik8 BPC Table 3: Hyperparameters for the character-level language model experiments. 1200 FS - LSTM - 4 1500 enwik8 Penn Treebank FS - LSTM - 4 FS - LSTM - 2",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"enwik8\", \"Metric\": \"Bit per Character (BPC)\", \"Score\": \"1.25\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"enwik8\", \"Metric\": \"Number of params\", \"Score\": \"47M\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"Hutter Prize\", \"Metric\": \"Bit per Character (BPC)\", \"Score\": \"1.245\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"Hutter Prize\", \"Metric\": \"Number of params\", \"Score\": \"47M\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"Hutter Prize\", \"Metric\": \"Bit per Character (BPC)\", \"Score\": \"1.277\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"Hutter Prize\", \"Metric\": \"Number of params\", \"Score\": \"27M\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"Penn Treebank (Character Level)\", \"Metric\": \"Bit per Character (BPC)\", \"Score\": \"1.190\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"Penn Treebank (Character Level)\", \"Metric\": \"Number of params\", \"Score\": \"27M\"}}, {\"LEADERBOARD\": {\"Task\": \"Language Modelling\", \"Dataset\": \"Penn Treebank (Character Level)\", \"Metric\": \"Bit per Character (BPC)\", \"Score\": \"1.193\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Language Modelling\", \"Dataset\": \"enwik8\", \"Metric\": \"Bit per Character (BPC)\", \"Score\": \"1.22\"}} ]"
    },
    {
        "Context": "ScaleNAS: One-Shot Learning of Scale-Aware Representations for Visual Recognition Scale variance among different sizes of body parts and objects is a challenging problem for visual recognition tasks. Existing works usually design a dedicated backbone or apply Neural architecture Search(NAS) for each task to tackle this challenge. However, existing works impose significant limitations on the design or search space. To solve these problems, we present ScaleNAS, a one-shot learning method for exploring scale-aware representations. Scale-NAS solves multiple tasks at a time by searching multi-scale feature aggregation. ScaleNAS adopts a flexible search space that allows an arbitrary number of blocks and crossscale feature fusions. To cope with the high search cost incurred by the flexible space, ScaleNAS employs one-shot learning for multi-scale supernet driven by grouped sampling and evolutionary search. Without further retraining, ScaleNet can be directly deployed for different visual recognition tasks with superior performance. We use Scale-NAS to create high-resolution models for two different tasks, ScaleNet-P for human pose estimation and ScaleNet-S for semantic segmentation. ScaleNet-P and ScaleNet-S outperform existing manually crafted and NAS-based methods in both tasks. When applying ScaleNet-P to bottom-up human pose estimation, it surpasses the state-of-the-art High-erHRNet. In particular, ScaleNet-P4 achieves 71.6% AP on COCO test-dev, achieving new state-of-the-art result. In this section, we evaluate ScaleNAS by searching neural architectures for semantic segmentation and human pose estimation First we train SuperScaleNet on semantic segmentation with Ctiyscapes dataset and derive ScaleNet-S using ScaleNAS Then we apply the same searching routine on top-down human pose estimation framework with COCO dataset to derive ScaleNet-P In order to evaluate the generalizability of ScaleNet, we apply ScaleNet-P to HigherHRNet framework for bottom-up human pose estimation To stabilize training, we first train the teacher model with full depths and fusions on ImageNet-1k dataset Did you use tabular or surrogate benchmarks for indepth evaluations? -No, existing surrogate benchmarks such as NASBench-101, NAS-Bench-201, NAS-Bench-1Shot1 are not application to our search space Table 1. Semantic segmentation results on Cityscapes val (single scale and no flipping). The GFLOPs is calculated on the input size 1024 \u00d7 2048. 'D-X' equals to 'Dilated-X'. For existing segmentation NAS works, the total cost grows linear to the number of deployment scenarios N , while the cost of our ScaleNAS remains constant. spectively . Without additional retraining , ScaleNet - S1 out - improves the mIoU to 82 . 0% , surpassing HRNet and Auto - #Params Backbone GFLOPs we use ( GPU hours ) Method Total Cost ( N =40 ) - mIoU 200 300 ( % ) 600 More training details can be found in the sup - After the teacher model is trained , Table 2. Top-down human pose estimation results. 256\u00d7192 Comparison on MPII val . The GFLOPs is calculated on the input size 256 \u00d7 256 . We reuse the searched ScaleNet - P",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Multi-Person Pose Estimation\", \"Dataset\": \"CrowdPose\", \"Metric\": \"mAP @0.5:0.95\", \"Score\": \"71.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Multi-Person Pose Estimation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP\", \"Score\": \"71.6\"}}, {\"LEADERBOARD\": {\"Task\": \"Multi-Person Pose Estimation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP50\", \"Score\": \"90.3\"}}, {\"LEADERBOARD\": {\"Task\": \"Multi-Person Pose Estimation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP75\", \"Score\": \"78.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Multi-Person Pose Estimation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APL\", \"Score\": \"77.2\"}}, {\"LEADERBOARD\": {\"Task\": \"Multi-Person Pose Estimation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APM\", \"Score\": \"67.5\"}}, {\"LEADERBOARD\": {\"Task\": \"Multi-Person Pose Estimation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AR\", \"Score\": \"76.0\"}}, {\"LEADERBOARD\": {\"Task\": \"Multi-Person Pose Estimation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AR50\", \"Score\": \"92.3\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP50\", \"Score\": \"71.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP75\", \"Score\": \"41.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APL\", \"Score\": \"72.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APM\", \"Score\": \"40.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"APS\", \"Score\": \"22.1\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"box AP\", \"Score\": \"41.3\"}}, { \"LEADERBOARD\": { \"Task\": \"Semantic Segmentation\", \"Dataset\": \"COCO test-dev\", \"Metric\": \"AP50\", \"Score\": "
    },
    {
        "Context": "Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention Satellite image time series, bolstered by their growing availability, are at the forefront of an extensive effort towards automated Earth monitoring by international institutions. In particular, large-scale control of agricultural parcels is an issue of major political and economic importance. In this regard, hybrid convolutional-recurrent neural architectures have shown promising results for the automated classification of satellite image time series.We propose an alternative approach in which the convolutional layers are advantageously replaced with encoders operating on unordered sets of pixels to exploit the typically coarse resolution of publicly available satellite images. We also propose to extract temporal features using a bespoke neural architecture based on self-attention instead of recurrent networks. We demonstrate experimentally that our method not only outperforms previous state-of-the-art approaches in terms of precision, but also significantly decreases processing time and memory requirements. Lastly, we release a large openaccess annotated dataset as a benchmark for future work on satellite image time series. We evaluate our models using Sentinel-2 multi-spectral image sequences in top-of-canopy reflectance Data Preparation: In order to evaluate both ours and convolution-based methods, we organize the parcels into two different formats: patches and pixel sets Note that the geometric features f must be computed and saved before preparing the dataset, as all spatial structure is henceforth lost The dataset is highly imbalanced as is often the casein such real word applications and this motivated the use of the focal loss to train our models Both datasets will be released upon publication To the best of our knowledge, no benchmark dataset currently exists for object-based agricultural parcel classification Our datasets area first step towards more reproducible and comparable methodological work in this field Table 1: Configuration of our model chosen for the numer- ical experiments. The dimension of each successive feature space is given for MLPs and fully connected layers. We show the corresponding number of trainable parameters on the last column. parameters 128 , 32 , 4 Hyperparameters Number of 32 \u2192 32 10 \u2192 32 \u2192 64 64 Table 2: Classification metrics and time benchmark of the different architectures. The inter-fold standard deviation of the OA and mIoU is given in smaller font. Additionally, the total time for one epoch of training, and for inference on the complete dataset are given on the third and fourth columns. 1 disk space required for training and pure inference, 2 time for the entire training Training ( s / epoch ) mIoU OA Disk Size Gb Table 3: Ablation study of our different design choices, sorted by decreasing mIoU. minor drop in precision . mIoU",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Time Series Classification\", \"Dataset\": \"s2-agri\", \"Metric\": \"mIoU\", \"Score\": \"50.9\"}}, {\"LEADERBOARD\": {\"Task\": \"Time Series Classification\", \"Dataset\": \"s2-agri\", \"Metric\": \"oAcc\", \"Score\": \"94.2\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Satellite Image Classification\", \"Dataset\": \"Satellite Image Classification Challenge\", \"Metric\": \"Accuracy\", \"Score\": \"94.9\"}}, { \"LEADERBOARD\": { \"Task\": \"Satellite Image Classification\", \"Dataset\": \"Satellite Image Classification Challenge\", \"Metric\": \"Accuracy\", \"Score\": \"92.8\"}}, { \"LEADERBOARD\": { \"Task\": \"Satellite Image Classification\", \"Dataset\": \"Satellite Image Classification Challenge\", \"Metric\": \"Accuracy\", \"Score\": \"80.6\"}}, { \"LEADERBOARD\": { \"Task\": \"Satellite Image Classification\", \"Dataset\": \"Satellite Image Classification Challenge\", \"Metric\": \"Accuracy\", \"Score\": \"9Nu\"}} ]"
    },
    {
        "Context": "Reasoning with Latent Structure Refinement for Document-Level Relation Extraction Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F 1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.  Table 1: Hyper-parameters of LSR. Adam 2 20 120 Table 2: Main results on the development and the test set of DocRED: Models with p are adapted to DocRED based on their open implementations. Results with * are computed based on re-trained models as we need to evaluate F 1 for both intra-and inter-sentence setting, which are not given in original papers. Ign F 1 Dev Test F 1 Inter - F 1 Intra - F 1 Table 3: Results on the test set of the CDR dataset. The methods below the double line take advantage of addi- tional training data and/or incorporate external tools. - F 1 Inter - F 1 Intra - F 1 - Table 4: Results on the test set of the GDA dataset. F 1 Inter - F 1 Intra - F 1 Table 5: Ablation study of LSR on DocRED. F 1 Inter - F",
        "PWC Annotation": "[{\"LEADERBOARD\": {\"Task\": \"Relation Extraction\", \"Dataset\": \"DocRED\", \"Metric\": \"F1\", \"Score\": \"59.05\"}}, {\"LEADERBOARD\": {\"Task\": \"Relation Extraction\", \"Dataset\": \"DocRED\", \"Metric\": \"Ign F1\", \"Score\": \"56.97\"}}, {\"LEADERBOARD\": {\"Task\": \"Relation Extraction\", \"Dataset\": \"DocRED\", \"Metric\": \"F1\", \"Score\": \"54.18\"}}, {\"LEADERBOARD\": {\"Task\": \"Relation Extraction\", \"Dataset\": \"DocRED\", \"Metric\": \"Ign F1\", \"Score\": \"52.15\"}}]",
        "LLM prediction": "[{ \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"DocRED\", \"Metric\": \"F1\", \"Score\": \"59.82\"}}, { \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"DocRED\", \"Metric\": \"Ign F1\", \"Score\": \"57.89\"}}, { \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"DocRED\", \"Metric\": \"F1\", \"Score\": \"55.75\"}}, { \"LEADERBOARD\": { \"Task\": \"Relation Extraction\", \"Dataset\": \"DocRED\", \"Metric\": \"Ign F1\", \"Score\": \"53.43\"}} ]"
    }
]